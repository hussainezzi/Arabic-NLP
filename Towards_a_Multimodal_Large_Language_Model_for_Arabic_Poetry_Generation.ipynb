{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUl12uprb/8Lgl/aYqevLX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hussainezzi/Arabic-NLP/blob/main/Towards_a_Multimodal_Large_Language_Model_for_Arabic_Poetry_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multimodal Arabic Poetry Generation\n",
        "\n",
        "This notebook implements a multimodal LLM for generating Arabic poetry from text, image, or audio inputs.\n",
        "\n",
        "## 1. Setup and Installation\n",
        "\n",
        "First, install required libraries:"
      ],
      "metadata": {
        "id": "V5WcoDYUhRLn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets torch torchvision torchaudio\n",
        "!pip install speechbrain audiomentations\n",
        "!pip install arabic-reshaper python-bidi pytesseract\n",
        "!pip install gradio  # For eventual demo"
      ],
      "metadata": {
        "id": "_BoM9KhGhSkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data Loading and Preparation\n",
        "\n",
        "We'll use the following datasets from Hugging Face:\n",
        "- Arabic Poetry Corpus\n",
        "- Classical Arabic Poetry Dataset\n",
        "- Common Voice Arabic (audio)"
      ],
      "metadata": {
        "id": "gVdqruTmhUEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load text datasets\n",
        "poetry_dataset = load_dataset(\"arbml/ClassicalArabicPoetryDataset\", split=\"train\")\n",
        "calligraphy_dataset = load_dataset(\"arbml/ArabicCalligraphy\", split=\"train\")\n",
        "\n",
        "# Load audio dataset\n",
        "audio_dataset = load_dataset(\"common_voice\", \"ar\", split=\"train[:100]\")\n",
        "\n",
        "print(f\"Loaded {len(poetry_dataset)} poetic verses\")\n",
        "print(f\"Loaded {len(calligraphy_dataset)} calligraphy images\")\n",
        "print(f\"Loaded {len(audio_dataset)} audio samples\")"
      ],
      "metadata": {
        "id": "m2H6W-W8hVZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Model Architecture\n",
        "\n",
        "We'll create a multimodal transformer integrating:\n",
        "- AraGPT2 for text generation\n",
        "- CLIP for image understanding\n",
        "- SpeechT5 for audio processing"
      ],
      "metadata": {
        "id": "ZBZqbJF9hW1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForCausalLM,\n",
        "    CLIPProcessor, CLIPModel,\n",
        "    SpeechT5Processor, SpeechT5ForTextToSpeech\n",
        ")\n",
        "\n",
        "# Text components\n",
        "text_tokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/aragpt2-base\")\n",
        "text_model = AutoModelForCausalLM.from_pretrained(\"aubmindlab/aragpt2-base\")\n",
        "\n",
        "# Image components\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# Audio components\n",
        "speech_processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
        "speech_model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\")"
      ],
      "metadata": {
        "id": "6UB8zjK_hZB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Multimodal Fusion Layer\n",
        "\n",
        "This layer combines features from different modalities:"
      ],
      "metadata": {
        "id": "sVaKrQNthcTr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zcmAfPcKhX9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MultimodalFusion(nn.Module):\n",
        "    def __init__(self, text_dim, image_dim, audio_dim):\n",
        "        super().__init__()\n",
        "        self.text_proj = nn.Linear(text_dim, 512)\n",
        "        self.image_proj = nn.Linear(image_dim, 512)\n",
        "        self.audio_proj = nn.Linear(audio_dim, 512)\n",
        "        self.attention = nn.MultiheadAttention(512, 8)\n",
        "\n",
        "    def forward(self, text_feats, image_feats, audio_feats):\n",
        "        # Project all features to same dimension\n",
        "        text = self.text_proj(text_feats)\n",
        "        image = self.image_proj(image_feats)\n",
        "        audio = self.audio_proj(audio_feats)\n",
        "\n",
        "        # Concatenate and apply attention\n",
        "        combined = torch.cat([text, image, audio], dim=1)\n",
        "        attn_output, _ = self.attention(combined, combined, combined)\n",
        "        return attn_output.mean(dim=1)"
      ],
      "metadata": {
        "id": "1py78EN9hdp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Training Pipeline\n",
        "\n",
        "Custom training loop for multimodal inputs:"
      ],
      "metadata": {
        "id": "VTAZz_Z6hfFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_multimodal(batch):\n",
        "    # Process different modalities\n",
        "    text_inputs = text_tokenizer(batch[\"text\"], return_tensors=\"pt\", padding=True)\n",
        "    image_inputs = clip_processor(images=batch[\"image\"], return_tensors=\"pt\")\n",
        "    audio_inputs = speech_processor(audio=batch[\"audio\"], return_tensors=\"pt\")\n",
        "\n",
        "    # Get features from each modality\n",
        "    text_feats = text_model(**text_inputs).last_hidden_state\n",
        "    image_feats = clip_model.get_image_features(**image_inputs)\n",
        "    audio_feats = speech_model.get_audio_features(**audio_inputs)\n",
        "\n",
        "    # Fuse features\n",
        "    fused_feats = fusion_layer(text_feats, image_feats, audio_feats)\n",
        "\n",
        "    # Generate poetry\n",
        "    outputs = text_model(inputs_embeds=fused_feats)\n",
        "    return outputs.loss\n",
        "\n",
        "# Initialize fusion layer\n",
        "fusion_layer = MultimodalFusion(\n",
        "    text_dim=768,\n",
        "    image_dim=512,\n",
        "    audio_dim=512\n",
        ")\n",
        "\n",
        "# Training loop (simplified)\n",
        "optimizer = torch.optim.AdamW([\n",
        "    {'params': text_model.parameters()},\n",
        "    {'params': fusion_layer.parameters()}\n",
        "], lr=5e-5)\n",
        "\n",
        "for epoch in range(3):\n",
        "    for batch in train_loader:\n",
        "        loss = train_multimodal(batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "    print(f\"Epoch {epoch} Loss: {loss.item()}\")"
      ],
      "metadata": {
        "id": "n6ohjvpkhfxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWSniHMUhLQm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Poetry Generation\n",
        "\n",
        "Function to generate poetry from any input modality:"
      ],
      "metadata": {
        "id": "0_lLaOh5hhPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_poetry(input_text=None, input_image=None, input_audio=None):\n",
        "    # Process input modality\n",
        "    if input_text:\n",
        "        inputs = text_tokenizer(input_text, return_tensors=\"pt\")\n",
        "    elif input_image:\n",
        "        inputs = clip_processor(images=input_image, return_tensors=\"pt\")\n",
        "    elif input_audio:\n",
        "        inputs = speech_processor(audio=input_audio, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate features\n",
        "    if input_text:\n",
        "        feats = text_model(**inputs).last_hidden_state\n",
        "    elif input_image:\n",
        "        feats = clip_model.get_image_features(**inputs)\n",
        "    elif input_audio:\n",
        "        feats = speech_model.get_audio_features(**inputs)\n",
        "\n",
        "    # Generate text\n",
        "    poetry_ids = text_model.generate(\n",
        "        inputs_embeds=feats,\n",
        "        max_length=100,\n",
        "        num_beams=5,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    return text_tokenizer.decode(poetry_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Example usage\n",
        "print(generate_poetry(input_text=\"الغروب الجميل\"))"
      ],
      "metadata": {
        "id": "d_3qZFw4hiMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Output Rendering\n",
        "\n",
        "Convert generated text to multimedia formats:"
      ],
      "metadata": {
        "id": "DRq2zztxhjXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gtts import gTTS\n",
        "from IPython.display import Audio, Image\n",
        "\n",
        "def text_to_speech(text, lang=\"ar\"):\n",
        "    tts = gTTS(text=text, lang=lang, slow=True)\n",
        "    tts.save(\"poem.mp3\")\n",
        "    return Audio(\"poem.mp3\")\n",
        "\n",
        "def create_video(text, image_path):\n",
        "    # Add text to image using Arabic reshaper\n",
        "    from arabic_reshaper import reshape\n",
        "    from bidi.algorithm import get_display\n",
        "\n",
        "    reshaped_text = reshape(text)\n",
        "    bidi_text = get_display(reshaped_text)\n",
        "\n",
        "    # Add text to image and save as video\n",
        "    # (Implementation using OpenCV would go here)\n",
        "    return \"output.mp4\"\n",
        "\n",
        "# Generate multimedia output\n",
        "poem = generate_poetry(input_text=\"الحب\")\n",
        "audio = text_to_speech(poem)\n",
        "video = create_video(poem, \"background.jpg\")"
      ],
      "metadata": {
        "id": "8U71MFY6hkiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Evaluation Metrics\n",
        "\n",
        "Implement automatic evaluation metrics:"
      ],
      "metadata": {
        "id": "FVE4PqRahl5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "def evaluate_poetry(reference, generated):\n",
        "    # BLEU Score\n",
        "    reference = [ref.split()]\n",
        "    generated = generated.split()\n",
        "    bleu = sentence_bleu(reference, generated)\n",
        "\n",
        "    # Rhyme detection (Arabic-specific)\n",
        "    last_words = [line.split()[-1] for line in generated.split('\\n')]\n",
        "    rhyme_score = len(set(last_words)) / len(last_words)\n",
        "\n",
        "    return {\"bleu\": bleu, \"rhyme\": rhyme_score}\n",
        "\n",
        "# Example evaluation\n",
        "poem = \"يا طائر الغرد في الأيك\\nقد كنت أهواك من قديم\"\n",
        "print(evaluate_poetry(reference_poem, poem))"
      ],
      "metadata": {
        "id": "Q8Y5BzB0hmrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next Steps\n",
        "\n",
        "1. Implement reinforcement learning for 'arūḍ meter alignment\n",
        "2. Add diacritization support using Mishkal\n",
        "3. Scale up training using distributed computing\n",
        "4. Develop Gradio demo interface\n",
        "\n",
        "Note: This implementation requires significant GPU resources. For full training, use:\n",
        "- 4x NVIDIA A100 GPUs\n",
        "- Mixed precision training\n",
        "- Dataset sharding"
      ],
      "metadata": {
        "id": "YUV7HaMWhn23"
      }
    }
  ]
}