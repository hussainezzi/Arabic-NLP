{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMySts1CD0Y1vWKGHOUPjW/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8bc615d310214c8a944f1fb7df49b68e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6ce1ad96a8634f5f971fc6dccab7a647",
              "IPY_MODEL_f6a73c7ff4bd4bf0ab733f6cca53bb18",
              "IPY_MODEL_519632a3de3d44d29ec109e3905c06a0"
            ],
            "layout": "IPY_MODEL_05d0c2cd90884217b648e0fc23f51b4d"
          }
        },
        "6ce1ad96a8634f5f971fc6dccab7a647": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13b4bac3c220453e8644ec6e86a4c904",
            "placeholder": "​",
            "style": "IPY_MODEL_3b1a8eef0a48455e945c5834c3a55a5c",
            "value": "config.json: 100%"
          }
        },
        "f6a73c7ff4bd4bf0ab733f6cca53bb18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5cc61f3d3c149f99af2341c8a508fdc",
            "max": 843,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f42249dd47f643e78da479060dbb9afd",
            "value": 843
          }
        },
        "519632a3de3d44d29ec109e3905c06a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90da8120abee4f85befc420bcb605032",
            "placeholder": "​",
            "style": "IPY_MODEL_08ecc8814f98474c84701bf45eda8fbd",
            "value": " 843/843 [00:00&lt;00:00, 64.9kB/s]"
          }
        },
        "05d0c2cd90884217b648e0fc23f51b4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13b4bac3c220453e8644ec6e86a4c904": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b1a8eef0a48455e945c5834c3a55a5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f5cc61f3d3c149f99af2341c8a508fdc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f42249dd47f643e78da479060dbb9afd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "90da8120abee4f85befc420bcb605032": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08ecc8814f98474c84701bf45eda8fbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c01cca8fad67493aa625207662dbd2bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5de7e9082f2e46e0b4b122f328474d20",
              "IPY_MODEL_b9088a15391540f5ae742bcd271f6940",
              "IPY_MODEL_4818d9349327475987423f0f88da86d9"
            ],
            "layout": "IPY_MODEL_3878fc32d0374430ac05f7b575110b83"
          }
        },
        "5de7e9082f2e46e0b4b122f328474d20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44b635616f1d42ad89d8487604115a97",
            "placeholder": "​",
            "style": "IPY_MODEL_b05983b0f7ea4094a3084d5ebfd27c76",
            "value": "model.safetensors: 100%"
          }
        },
        "b9088a15391540f5ae742bcd271f6940": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_743899884cbd40508f7d141f5289ef8b",
            "max": 552576030,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_24243ebe89414dd0819a032b57016542",
            "value": 552576030
          }
        },
        "4818d9349327475987423f0f88da86d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ad621f59f874f4ab185e7f2d62674c6",
            "placeholder": "​",
            "style": "IPY_MODEL_01f800dc1ff447d7b5a9eb716ff2f0b4",
            "value": " 553M/553M [00:03&lt;00:00, 147MB/s]"
          }
        },
        "3878fc32d0374430ac05f7b575110b83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44b635616f1d42ad89d8487604115a97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b05983b0f7ea4094a3084d5ebfd27c76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "743899884cbd40508f7d141f5289ef8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24243ebe89414dd0819a032b57016542": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6ad621f59f874f4ab185e7f2d62674c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01f800dc1ff447d7b5a9eb716ff2f0b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f24bedc34ec42ef8dc971feb32f5da8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4e40103084ac4befa03139aeb57809c3",
              "IPY_MODEL_795cf628e9c945a7be2d07eb739aa8ab",
              "IPY_MODEL_65d18175a53544ba98dc750f8cdb9807"
            ],
            "layout": "IPY_MODEL_548425e527004878bd3f6a980a5db58e"
          }
        },
        "4e40103084ac4befa03139aeb57809c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_177523836dc442bea7ca2f4de7299dc6",
            "placeholder": "​",
            "style": "IPY_MODEL_91a2168afd3e4391b1f7a662d4083ff7",
            "value": "vocab.json: 100%"
          }
        },
        "795cf628e9c945a7be2d07eb739aa8ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43180c03ea06423a9468d5b745502d1e",
            "max": 1935314,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a90740a2a7aa4c32aa233cedaed28620",
            "value": 1935314
          }
        },
        "65d18175a53544ba98dc750f8cdb9807": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0a2932046da451eb2ed2f048e929ee4",
            "placeholder": "​",
            "style": "IPY_MODEL_446e6290354a4de7a4fd2965dc8fd06b",
            "value": " 1.94M/1.94M [00:00&lt;00:00, 21.7MB/s]"
          }
        },
        "548425e527004878bd3f6a980a5db58e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "177523836dc442bea7ca2f4de7299dc6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91a2168afd3e4391b1f7a662d4083ff7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "43180c03ea06423a9468d5b745502d1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a90740a2a7aa4c32aa233cedaed28620": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f0a2932046da451eb2ed2f048e929ee4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "446e6290354a4de7a4fd2965dc8fd06b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0db972d644d04461908f40e2262b6ddf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f0c1d7e8fe2f474f9937a90a13fbf5f2",
              "IPY_MODEL_29a47d7c5c7b4b4383dd649e4fb3c6c2",
              "IPY_MODEL_228c0326935f4d7085042e63c4163e5a"
            ],
            "layout": "IPY_MODEL_6dbe62c7e1cc4f0f9d54963e6a6592ff"
          }
        },
        "f0c1d7e8fe2f474f9937a90a13fbf5f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7a3cd5a05c243109f9b6e03cda990e1",
            "placeholder": "​",
            "style": "IPY_MODEL_f3d4d7aa5d2149829eb4dbb5c9b9830b",
            "value": "merges.txt: 100%"
          }
        },
        "29a47d7c5c7b4b4383dd649e4fb3c6c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0de5a4c8bea74e879379fe4ae3389255",
            "max": 1497508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d8e61caefa45423181985c473a8c6ed1",
            "value": 1497508
          }
        },
        "228c0326935f4d7085042e63c4163e5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9438feb031a746c5bddd5242f9cfe0eb",
            "placeholder": "​",
            "style": "IPY_MODEL_73f65d08b7b0400e8537376d29cc540b",
            "value": " 1.50M/1.50M [00:00&lt;00:00, 13.2MB/s]"
          }
        },
        "6dbe62c7e1cc4f0f9d54963e6a6592ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7a3cd5a05c243109f9b6e03cda990e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3d4d7aa5d2149829eb4dbb5c9b9830b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0de5a4c8bea74e879379fe4ae3389255": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8e61caefa45423181985c473a8c6ed1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9438feb031a746c5bddd5242f9cfe0eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73f65d08b7b0400e8537376d29cc540b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "70d824753efe4048a972047c06d5aa29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_424e5a47895044a79bb7c7571e18bb9b",
              "IPY_MODEL_a41fb655b72744d7817fbfa270330e0f",
              "IPY_MODEL_e9931c684ee147dc86cfecb9c91a0759"
            ],
            "layout": "IPY_MODEL_350a9ce5405646d1b6e31eb8830666cb"
          }
        },
        "424e5a47895044a79bb7c7571e18bb9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65a02f42a6c247e0b42e26d0e2159345",
            "placeholder": "​",
            "style": "IPY_MODEL_9dadccb43ae04304993f5f3adef3a4b7",
            "value": "tokenizer.json: 100%"
          }
        },
        "a41fb655b72744d7817fbfa270330e0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b13f9c6345774c5891e321a6d6aa9579",
            "max": 4519765,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_256d1f297f4c46359bf150a8c94940cc",
            "value": 4519765
          }
        },
        "e9931c684ee147dc86cfecb9c91a0759": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3b86f8874d94584b6513fdd746d8893",
            "placeholder": "​",
            "style": "IPY_MODEL_46c4cbc275424ad281ba9992afd8414a",
            "value": " 4.52M/4.52M [00:00&lt;00:00, 12.9MB/s]"
          }
        },
        "350a9ce5405646d1b6e31eb8830666cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65a02f42a6c247e0b42e26d0e2159345": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9dadccb43ae04304993f5f3adef3a4b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b13f9c6345774c5891e321a6d6aa9579": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "256d1f297f4c46359bf150a8c94940cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d3b86f8874d94584b6513fdd746d8893": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46c4cbc275424ad281ba9992afd8414a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "da586c70f1cc4092ba50878389c7da7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_59ba1b5be73c4615a374a22752feb1a1",
              "IPY_MODEL_8bf530ea1876409085e9aec05b5c4e11",
              "IPY_MODEL_d495510bed73456fb411c6b199345191"
            ],
            "layout": "IPY_MODEL_7f65862ef2cd4a14bed87c26f3b18d05"
          }
        },
        "59ba1b5be73c4615a374a22752feb1a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50b489e946ce4d63a79d9fc208a61bde",
            "placeholder": "​",
            "style": "IPY_MODEL_7813e4f4152641c1bb76c128e13a6330",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "8bf530ea1876409085e9aec05b5c4e11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5119c21e93c404690d4497f0c3a0b63",
            "max": 611,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f96f1b4e67304d41b4011cdfbff7e50b",
            "value": 611
          }
        },
        "d495510bed73456fb411c6b199345191": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_939806e41ce241c79bd0f2c4fba168f7",
            "placeholder": "​",
            "style": "IPY_MODEL_e5b8474c6ab248a3a9e8f5a742c860e9",
            "value": " 611/611 [00:00&lt;00:00, 24.9kB/s]"
          }
        },
        "7f65862ef2cd4a14bed87c26f3b18d05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50b489e946ce4d63a79d9fc208a61bde": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7813e4f4152641c1bb76c128e13a6330": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b5119c21e93c404690d4497f0c3a0b63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f96f1b4e67304d41b4011cdfbff7e50b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "939806e41ce241c79bd0f2c4fba168f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5b8474c6ab248a3a9e8f5a742c860e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f53e77eec3a742d78281befc7b85743c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_093ff339452842ff83b1712288f3c783",
              "IPY_MODEL_efde7d9ef9a04bb2b31491bf329fc5b4",
              "IPY_MODEL_3f2c0ba1c46746b2b19d97c026d04dae"
            ],
            "layout": "IPY_MODEL_ea4ba858e6de48a2b2453ae11b880669"
          }
        },
        "093ff339452842ff83b1712288f3c783": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d2c479a75a54cd2b9e84c55bc50e063",
            "placeholder": "​",
            "style": "IPY_MODEL_a5c012ccd2334397812d3b2a77c21b25",
            "value": "config.json: 100%"
          }
        },
        "efde7d9ef9a04bb2b31491bf329fc5b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_449158312c5f4b34beba933100c2607a",
            "max": 384,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_59a44963787b4397bccc51a297a525b3",
            "value": 384
          }
        },
        "3f2c0ba1c46746b2b19d97c026d04dae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d64f12a81cc34041810d9aaf34848d6d",
            "placeholder": "​",
            "style": "IPY_MODEL_ba04c83922bf497182fe60bfa75f7a90",
            "value": " 384/384 [00:00&lt;00:00, 23.6kB/s]"
          }
        },
        "ea4ba858e6de48a2b2453ae11b880669": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d2c479a75a54cd2b9e84c55bc50e063": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5c012ccd2334397812d3b2a77c21b25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "449158312c5f4b34beba933100c2607a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59a44963787b4397bccc51a297a525b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d64f12a81cc34041810d9aaf34848d6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba04c83922bf497182fe60bfa75f7a90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "80edec71a73f41279af408376f8cec75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0533d6036ed5461c955f06553f6f6ad1",
              "IPY_MODEL_cfebfb9b3f98430495340ff0a4402c93",
              "IPY_MODEL_00d1c11808634935ab3143adb7876952"
            ],
            "layout": "IPY_MODEL_e7c58bb86dcd43d19ce0a21fc3a3e604"
          }
        },
        "0533d6036ed5461c955f06553f6f6ad1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_433847186eae42f8bf9362ace0545a8c",
            "placeholder": "​",
            "style": "IPY_MODEL_0e03af65c6ee49ff941399ee3d7e8227",
            "value": "vocab.txt: 100%"
          }
        },
        "cfebfb9b3f98430495340ff0a4402c93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14545f98fe7a4e1aaa7665de2c02d445",
            "max": 719993,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_58eede6254294ced972f865a68d8e4f9",
            "value": 719993
          }
        },
        "00d1c11808634935ab3143adb7876952": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_656d9e539bef4a7e98db4fd5d86bbd71",
            "placeholder": "​",
            "style": "IPY_MODEL_7810eebbded542e4b539bbfa0109d7cc",
            "value": " 720k/720k [00:00&lt;00:00, 4.11MB/s]"
          }
        },
        "e7c58bb86dcd43d19ce0a21fc3a3e604": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "433847186eae42f8bf9362ace0545a8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e03af65c6ee49ff941399ee3d7e8227": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "14545f98fe7a4e1aaa7665de2c02d445": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58eede6254294ced972f865a68d8e4f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "656d9e539bef4a7e98db4fd5d86bbd71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7810eebbded542e4b539bbfa0109d7cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e26ec8c294a4a4aa766f535258180fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3df1b5b3cb404c148d5ec3bcb840f612",
              "IPY_MODEL_f81900fa496c460b9edcf0a2cdf6a71d",
              "IPY_MODEL_b4b479dabfec4b9eb9b7fc981802c3bc"
            ],
            "layout": "IPY_MODEL_bb8d7703c5fd4a39977d27a8f3dbb177"
          }
        },
        "3df1b5b3cb404c148d5ec3bcb840f612": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_acebe47ada944e999f43550b4e33146c",
            "placeholder": "​",
            "style": "IPY_MODEL_81eacdc6e2ea4d79b14f69376698ae29",
            "value": "tokenizer.json: 100%"
          }
        },
        "f81900fa496c460b9edcf0a2cdf6a71d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8dbc4d87eda48fc96a8b13a6df0bebd",
            "max": 2306039,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c9ffa4b37f1141c9a5b134bb6e7fb5a8",
            "value": 2306039
          }
        },
        "b4b479dabfec4b9eb9b7fc981802c3bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec8c1053f57c4f07b7169f1a3c8c4649",
            "placeholder": "​",
            "style": "IPY_MODEL_5926994a751c4e2c8cc4e132a58ca49d",
            "value": " 2.31M/2.31M [00:00&lt;00:00, 18.8MB/s]"
          }
        },
        "bb8d7703c5fd4a39977d27a8f3dbb177": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "acebe47ada944e999f43550b4e33146c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81eacdc6e2ea4d79b14f69376698ae29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e8dbc4d87eda48fc96a8b13a6df0bebd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9ffa4b37f1141c9a5b134bb6e7fb5a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ec8c1053f57c4f07b7169f1a3c8c4649": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5926994a751c4e2c8cc4e132a58ca49d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2851ddefde1c4c9698721a8f8e55e776": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3ed720300c9e4dbf9c1c73bd097c4196",
              "IPY_MODEL_4c0b9430d1fb47c6b22bba2c8d289b1e",
              "IPY_MODEL_cbd9eb0fdc9e44a4a8ebff17ce88eac6"
            ],
            "layout": "IPY_MODEL_3d3dd752d51649e49546e512906ee8fc"
          }
        },
        "3ed720300c9e4dbf9c1c73bd097c4196": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_329b4dbfe28f4649849ad875b87ace9f",
            "placeholder": "​",
            "style": "IPY_MODEL_2f6a8cd48237433eb86d08a79ecb07f2",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "4c0b9430d1fb47c6b22bba2c8d289b1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6841e45fbe2e40be817611479ef0e476",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8f8857b5ef844a4cb1a1738f34b87073",
            "value": 112
          }
        },
        "cbd9eb0fdc9e44a4a8ebff17ce88eac6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42a167313b794281834dfab7e51439ed",
            "placeholder": "​",
            "style": "IPY_MODEL_ecf665d8cb9e47a4924032e4df460829",
            "value": " 112/112 [00:00&lt;00:00, 3.67kB/s]"
          }
        },
        "3d3dd752d51649e49546e512906ee8fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "329b4dbfe28f4649849ad875b87ace9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f6a8cd48237433eb86d08a79ecb07f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6841e45fbe2e40be817611479ef0e476": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f8857b5ef844a4cb1a1738f34b87073": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "42a167313b794281834dfab7e51439ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ecf665d8cb9e47a4924032e4df460829": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6818829bbb8846b3804c1ed222651a78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_19b3a4ce540c4b489be4d900296de908",
              "IPY_MODEL_ded55a64dd4e4888ae9b2db75a2f0c0f",
              "IPY_MODEL_6f1d4a17e50a412ba321c9bac2d95417"
            ],
            "layout": "IPY_MODEL_575c94668abc4aeda0fa556a0d33886b"
          }
        },
        "19b3a4ce540c4b489be4d900296de908": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5c4e5e2ee114a6890f1bdfcb02530c6",
            "placeholder": "​",
            "style": "IPY_MODEL_69b13ba54a394e9ca6a605e7fca6e130",
            "value": "model.safetensors: 100%"
          }
        },
        "ded55a64dd4e4888ae9b2db75a2f0c0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b15b9c7fdfa4c77b8ef9d0e70765e0b",
            "max": 543432324,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_79be6e7efab64db6bc8dbee753efe129",
            "value": 543432324
          }
        },
        "6f1d4a17e50a412ba321c9bac2d95417": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af5ebc99b537478796f279f36faca1ac",
            "placeholder": "​",
            "style": "IPY_MODEL_de28de86a18c44a78dcc6aa72ac2c339",
            "value": " 543M/543M [00:11&lt;00:00, 24.5MB/s]"
          }
        },
        "575c94668abc4aeda0fa556a0d33886b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5c4e5e2ee114a6890f1bdfcb02530c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69b13ba54a394e9ca6a605e7fca6e130": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b15b9c7fdfa4c77b8ef9d0e70765e0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79be6e7efab64db6bc8dbee753efe129": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "af5ebc99b537478796f279f36faca1ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de28de86a18c44a78dcc6aa72ac2c339": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hussainezzi/Arabic-NLP/blob/main/Character_base_%2C_token_free_arabic_poetry_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RokIneEn_4z",
        "outputId": "767b1342-36db-43a0-d6cd-cd7723774307"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Collecting pyarabic\n",
            "  Downloading PyArabic-0.6.15-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from pyarabic) (1.17.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyArabic-0.6.15-py3-none-any.whl (126 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.4/126.4 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, pyarabic, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed datasets-3.3.2 dill-0.3.8 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pyarabic-0.6.15 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "!pip install transformers torch pyarabic datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyarabic.araby as araby\n",
        "\n",
        "# Preprocess Arabic text by removing diacritics and normalizing\n",
        "def preprocess_text(text):\n",
        "    # Remove diacritics\n",
        "    text = araby.strip_tashkeel(text)\n",
        "    # Normalize text (convert variations of Arabic letters to a standard form)\n",
        "    text = araby.normalize_ligature(text)\n",
        "    # Remove unwanted characters\n",
        "    text = araby.strip_punctuation(text)\n",
        "    return text\n",
        "\n",
        "# Example: Apply preprocessing to the dataset\n",
        "def preprocess_dataset(dataset):\n",
        "    return [preprocess_text(verse) for verse in dataset]\n",
        "\n",
        "# Example datasets (replace with actual dataset loading code)\n",
        "dataset = [\"الحب أسمى من كل شيء\", \"في السماء طيور\"]\n",
        "preprocessed_data = preprocess_dataset(dataset)\n",
        "\n",
        "print(preprocessed_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "_40zAxySoC4F",
        "outputId": "40b1f8c7-62c8-49f5-8920-67de10c63ded"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'pyarabic.araby' has no attribute 'strip_punctuation'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-0c878adcd33f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Example datasets (replace with actual dataset loading code)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"الحب أسمى من كل شيء\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"في السماء طيور\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mpreprocessed_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-0c878adcd33f>\u001b[0m in \u001b[0;36mpreprocess_dataset\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Example: Apply preprocessing to the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpreprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mverse\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Example datasets (replace with actual dataset loading code)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-0c878adcd33f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Example: Apply preprocessing to the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpreprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mverse\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Example datasets (replace with actual dataset loading code)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-0c878adcd33f>\u001b[0m in \u001b[0;36mpreprocess_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maraby\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_ligature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Remove unwanted characters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maraby\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip_punctuation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'pyarabic.araby' has no attribute 'strip_punctuation'"
          ]
        }
      ]
    },
    {
      "source": [
        "# Install required libraries\n",
        "!pip install transformers torch pyarabic datasets\n",
        "\n",
        "import pyarabic.araby as araby\n",
        "import string\n",
        "\n",
        "# Helper function to remove punctuation\n",
        "def remove_punctuation(text):\n",
        "    # Create a translation table to remove punctuation\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    # Remove punctuation from the text\n",
        "    return text.translate(translator)\n",
        "\n",
        "# Preprocess Arabic text by removing diacritics and normalizing\n",
        "def preprocess_text(text):\n",
        "    # Remove diacritics\n",
        "    text = araby.strip_tashkeel(text)\n",
        "    # Normalize text (convert variations of Arabic letters to a standard form)\n",
        "    text = araby.normalize_ligature(text)\n",
        "    # Remove unwanted characters (punctuation in this case)\n",
        "    text = remove_punctuation(text) # call remove_punctuation instead of araby.strip_punctuation\n",
        "    return text\n",
        "\n",
        "# Example: Apply preprocessing to the dataset\n",
        "def preprocess_dataset(dataset):\n",
        "    return [preprocess_text(verse) for verse in dataset]\n",
        "\n",
        "# Example datasets (replace with actual dataset loading code)\n",
        "dataset = [\"الحب أسمى من كل شيء\", \"في السماء طيور\"]\n",
        "preprocessed_data = preprocess_dataset(dataset)\n",
        "\n",
        "print(preprocessed_data)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoApOcvspB8i",
        "outputId": "e6584681-5501-4cd0-e37a-cb0d6c03bedf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: pyarabic in /usr/local/lib/python3.11/dist-packages (0.6.15)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from pyarabic) (1.17.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "['الحب أسمى من كل شيء', 'في السماء طيور']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load pre-trained AraGPT-2 model and tokenizer\n",
        "model_name = 'aubmindlab/aragpt2-base'  # Example pre-trained Arabic GPT-2 model\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Prune tokenizer to use character-level tokenization\n",
        "tokenizer.add_tokens(['[PAD]'])  # Add padding token if needed\n",
        "tokenizer.pad_token = '[PAD]'\n",
        "\n",
        "# Replace the tokenizer's standard vocab with one that treats each character as a token\n",
        "# Here we manually re-tokenize at the character level\n",
        "def char_tokenize(text):\n",
        "    return tokenizer(list(text), is_split_into_words=True, padding=True, truncation=True)\n",
        "\n",
        "# Example of tokenizing at character level\n",
        "example_text = \"الحب أسمى من كل شيء\"\n",
        "char_tokens = char_tokenize(example_text)\n",
        "print(char_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336,
          "referenced_widgets": [
            "8bc615d310214c8a944f1fb7df49b68e",
            "6ce1ad96a8634f5f971fc6dccab7a647",
            "f6a73c7ff4bd4bf0ab733f6cca53bb18",
            "519632a3de3d44d29ec109e3905c06a0",
            "05d0c2cd90884217b648e0fc23f51b4d",
            "13b4bac3c220453e8644ec6e86a4c904",
            "3b1a8eef0a48455e945c5834c3a55a5c",
            "f5cc61f3d3c149f99af2341c8a508fdc",
            "f42249dd47f643e78da479060dbb9afd",
            "90da8120abee4f85befc420bcb605032",
            "08ecc8814f98474c84701bf45eda8fbd",
            "c01cca8fad67493aa625207662dbd2bf",
            "5de7e9082f2e46e0b4b122f328474d20",
            "b9088a15391540f5ae742bcd271f6940",
            "4818d9349327475987423f0f88da86d9",
            "3878fc32d0374430ac05f7b575110b83",
            "44b635616f1d42ad89d8487604115a97",
            "b05983b0f7ea4094a3084d5ebfd27c76",
            "743899884cbd40508f7d141f5289ef8b",
            "24243ebe89414dd0819a032b57016542",
            "6ad621f59f874f4ab185e7f2d62674c6",
            "01f800dc1ff447d7b5a9eb716ff2f0b4",
            "9f24bedc34ec42ef8dc971feb32f5da8",
            "4e40103084ac4befa03139aeb57809c3",
            "795cf628e9c945a7be2d07eb739aa8ab",
            "65d18175a53544ba98dc750f8cdb9807",
            "548425e527004878bd3f6a980a5db58e",
            "177523836dc442bea7ca2f4de7299dc6",
            "91a2168afd3e4391b1f7a662d4083ff7",
            "43180c03ea06423a9468d5b745502d1e",
            "a90740a2a7aa4c32aa233cedaed28620",
            "f0a2932046da451eb2ed2f048e929ee4",
            "446e6290354a4de7a4fd2965dc8fd06b",
            "0db972d644d04461908f40e2262b6ddf",
            "f0c1d7e8fe2f474f9937a90a13fbf5f2",
            "29a47d7c5c7b4b4383dd649e4fb3c6c2",
            "228c0326935f4d7085042e63c4163e5a",
            "6dbe62c7e1cc4f0f9d54963e6a6592ff",
            "d7a3cd5a05c243109f9b6e03cda990e1",
            "f3d4d7aa5d2149829eb4dbb5c9b9830b",
            "0de5a4c8bea74e879379fe4ae3389255",
            "d8e61caefa45423181985c473a8c6ed1",
            "9438feb031a746c5bddd5242f9cfe0eb",
            "73f65d08b7b0400e8537376d29cc540b",
            "70d824753efe4048a972047c06d5aa29",
            "424e5a47895044a79bb7c7571e18bb9b",
            "a41fb655b72744d7817fbfa270330e0f",
            "e9931c684ee147dc86cfecb9c91a0759",
            "350a9ce5405646d1b6e31eb8830666cb",
            "65a02f42a6c247e0b42e26d0e2159345",
            "9dadccb43ae04304993f5f3adef3a4b7",
            "b13f9c6345774c5891e321a6d6aa9579",
            "256d1f297f4c46359bf150a8c94940cc",
            "d3b86f8874d94584b6513fdd746d8893",
            "46c4cbc275424ad281ba9992afd8414a"
          ]
        },
        "id": "_BmL8x6Wo0FF",
        "outputId": "c6446a69-c48a-43df-bd38-d59e6175bb6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8bc615d310214c8a944f1fb7df49b68e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/553M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c01cca8fad67493aa625207662dbd2bf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.94M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9f24bedc34ec42ef8dc971feb32f5da8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/1.50M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0db972d644d04461908f40e2262b6ddf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/4.52M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "70d824753efe4048a972047c06d5aa29"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [278, 224, 153, 230, 331, 287, 224, 224, 291, 334, 224, 153, 231, 31849, 224, 224, 224, 153, 231, 346, 224, 224, 324, 224, 153, 230, 224, 224, 371, 224, 153, 236, 23803], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load pre-trained AraGPT-2 model and tokenizer\n",
        "model_name = 'aubmindlab/aragpt2-base'  # Example pre-trained Arabic GPT-2 model\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Prune tokenizer to use character-level tokenization\n",
        "tokenizer.add_tokens(['[PAD]'])  # Add padding token if needed\n",
        "tokenizer.pad_token = '[PAD]'\n",
        "\n",
        "# Replace the tokenizer's standard vocab with one that treats each character as a token\n",
        "# Here we manually re-tokenize at the character level\n",
        "def char_tokenize(text):\n",
        "    return tokenizer(list(text), is_split_into_words=True, padding=True, truncation=True)\n",
        "\n",
        "# Example of tokenizing at character level\n",
        "example_text = \"الحب أسمى من كل شيء\"\n",
        "char_tokens = char_tokenize(example_text)\n",
        "print(char_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336,
          "referenced_widgets": [
            "8bc615d310214c8a944f1fb7df49b68e",
            "6ce1ad96a8634f5f971fc6dccab7a647",
            "f6a73c7ff4bd4bf0ab733f6cca53bb18",
            "519632a3de3d44d29ec109e3905c06a0",
            "05d0c2cd90884217b648e0fc23f51b4d",
            "13b4bac3c220453e8644ec6e86a4c904",
            "3b1a8eef0a48455e945c5834c3a55a5c",
            "f5cc61f3d3c149f99af2341c8a508fdc",
            "f42249dd47f643e78da479060dbb9afd",
            "90da8120abee4f85befc420bcb605032",
            "08ecc8814f98474c84701bf45eda8fbd",
            "c01cca8fad67493aa625207662dbd2bf",
            "5de7e9082f2e46e0b4b122f328474d20",
            "b9088a15391540f5ae742bcd271f6940",
            "4818d9349327475987423f0f88da86d9",
            "3878fc32d0374430ac05f7b575110b83",
            "44b635616f1d42ad89d8487604115a97",
            "b05983b0f7ea4094a3084d5ebfd27c76",
            "743899884cbd40508f7d141f5289ef8b",
            "24243ebe89414dd0819a032b57016542",
            "6ad621f59f874f4ab185e7f2d62674c6",
            "01f800dc1ff447d7b5a9eb716ff2f0b4",
            "9f24bedc34ec42ef8dc971feb32f5da8",
            "4e40103084ac4befa03139aeb57809c3",
            "795cf628e9c945a7be2d07eb739aa8ab",
            "65d18175a53544ba98dc750f8cdb9807",
            "548425e527004878bd3f6a980a5db58e",
            "177523836dc442bea7ca2f4de7299dc6",
            "91a2168afd3e4391b1f7a662d4083ff7",
            "43180c03ea06423a9468d5b745502d1e",
            "a90740a2a7aa4c32aa233cedaed28620",
            "f0a2932046da451eb2ed2f048e929ee4",
            "446e6290354a4de7a4fd2965dc8fd06b",
            "0db972d644d04461908f40e2262b6ddf",
            "f0c1d7e8fe2f474f9937a90a13fbf5f2",
            "29a47d7c5c7b4b4383dd649e4fb3c6c2",
            "228c0326935f4d7085042e63c4163e5a",
            "6dbe62c7e1cc4f0f9d54963e6a6592ff",
            "d7a3cd5a05c243109f9b6e03cda990e1",
            "f3d4d7aa5d2149829eb4dbb5c9b9830b",
            "0de5a4c8bea74e879379fe4ae3389255",
            "d8e61caefa45423181985c473a8c6ed1",
            "9438feb031a746c5bddd5242f9cfe0eb",
            "73f65d08b7b0400e8537376d29cc540b",
            "70d824753efe4048a972047c06d5aa29",
            "424e5a47895044a79bb7c7571e18bb9b",
            "a41fb655b72744d7817fbfa270330e0f",
            "e9931c684ee147dc86cfecb9c91a0759",
            "350a9ce5405646d1b6e31eb8830666cb",
            "65a02f42a6c247e0b42e26d0e2159345",
            "9dadccb43ae04304993f5f3adef3a4b7",
            "b13f9c6345774c5891e321a6d6aa9579",
            "256d1f297f4c46359bf150a8c94940cc",
            "d3b86f8874d94584b6513fdd746d8893",
            "46c4cbc275424ad281ba9992afd8414a"
          ]
        },
        "outputId": "c6446a69-c48a-43df-bd38-d59e6175bb6c",
        "id": "o_lzDW2Czwqm"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8bc615d310214c8a944f1fb7df49b68e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/553M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c01cca8fad67493aa625207662dbd2bf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.94M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9f24bedc34ec42ef8dc971feb32f5da8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/1.50M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0db972d644d04461908f40e2262b6ddf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/4.52M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "70d824753efe4048a972047c06d5aa29"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [278, 224, 153, 230, 331, 287, 224, 224, 291, 334, 224, 153, 231, 31849, 224, 224, 224, 153, 231, 346, 224, 224, 324, 224, 153, 230, 224, 224, 371, 224, 153, 236, 23803], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load pre-trained AraGPT-2 model and tokenizer\n",
        "model_name = 'aubmindlab/aragpt2-base'  # Example pre-trained Arabic GPT-2 model\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Prune tokenizer to use character-level tokenization\n",
        "tokenizer.add_tokens(['[PAD]'])  # Add padding token if needed\n",
        "tokenizer.pad_token = '[PAD]'\n",
        "\n",
        "# Replace the tokenizer's standard vocab with one that treats each character as a token\n",
        "# Here we manually re-tokenize at the character level\n",
        "def char_tokenize(text):\n",
        "    return tokenizer(list(text), is_split_into_words=True, padding=True, truncation=True)\n",
        "\n",
        "# Example of tokenizing at character level\n",
        "example_text = \"الحب أسمى من كل شيء\"\n",
        "char_tokens = char_tokenize(example_text)\n",
        "print(char_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336,
          "referenced_widgets": [
            "8bc615d310214c8a944f1fb7df49b68e",
            "6ce1ad96a8634f5f971fc6dccab7a647",
            "f6a73c7ff4bd4bf0ab733f6cca53bb18",
            "519632a3de3d44d29ec109e3905c06a0",
            "05d0c2cd90884217b648e0fc23f51b4d",
            "13b4bac3c220453e8644ec6e86a4c904",
            "3b1a8eef0a48455e945c5834c3a55a5c",
            "f5cc61f3d3c149f99af2341c8a508fdc",
            "f42249dd47f643e78da479060dbb9afd",
            "90da8120abee4f85befc420bcb605032",
            "08ecc8814f98474c84701bf45eda8fbd",
            "c01cca8fad67493aa625207662dbd2bf",
            "5de7e9082f2e46e0b4b122f328474d20",
            "b9088a15391540f5ae742bcd271f6940",
            "4818d9349327475987423f0f88da86d9",
            "3878fc32d0374430ac05f7b575110b83",
            "44b635616f1d42ad89d8487604115a97",
            "b05983b0f7ea4094a3084d5ebfd27c76",
            "743899884cbd40508f7d141f5289ef8b",
            "24243ebe89414dd0819a032b57016542",
            "6ad621f59f874f4ab185e7f2d62674c6",
            "01f800dc1ff447d7b5a9eb716ff2f0b4",
            "9f24bedc34ec42ef8dc971feb32f5da8",
            "4e40103084ac4befa03139aeb57809c3",
            "795cf628e9c945a7be2d07eb739aa8ab",
            "65d18175a53544ba98dc750f8cdb9807",
            "548425e527004878bd3f6a980a5db58e",
            "177523836dc442bea7ca2f4de7299dc6",
            "91a2168afd3e4391b1f7a662d4083ff7",
            "43180c03ea06423a9468d5b745502d1e",
            "a90740a2a7aa4c32aa233cedaed28620",
            "f0a2932046da451eb2ed2f048e929ee4",
            "446e6290354a4de7a4fd2965dc8fd06b",
            "0db972d644d04461908f40e2262b6ddf",
            "f0c1d7e8fe2f474f9937a90a13fbf5f2",
            "29a47d7c5c7b4b4383dd649e4fb3c6c2",
            "228c0326935f4d7085042e63c4163e5a",
            "6dbe62c7e1cc4f0f9d54963e6a6592ff",
            "d7a3cd5a05c243109f9b6e03cda990e1",
            "f3d4d7aa5d2149829eb4dbb5c9b9830b",
            "0de5a4c8bea74e879379fe4ae3389255",
            "d8e61caefa45423181985c473a8c6ed1",
            "9438feb031a746c5bddd5242f9cfe0eb",
            "73f65d08b7b0400e8537376d29cc540b",
            "70d824753efe4048a972047c06d5aa29",
            "424e5a47895044a79bb7c7571e18bb9b",
            "a41fb655b72744d7817fbfa270330e0f",
            "e9931c684ee147dc86cfecb9c91a0759",
            "350a9ce5405646d1b6e31eb8830666cb",
            "65a02f42a6c247e0b42e26d0e2159345",
            "9dadccb43ae04304993f5f3adef3a4b7",
            "b13f9c6345774c5891e321a6d6aa9579",
            "256d1f297f4c46359bf150a8c94940cc",
            "d3b86f8874d94584b6513fdd746d8893",
            "46c4cbc275424ad281ba9992afd8414a"
          ]
        },
        "outputId": "c6446a69-c48a-43df-bd38-d59e6175bb6c",
        "id": "jdBjOfgDzxJj"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8bc615d310214c8a944f1fb7df49b68e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/553M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c01cca8fad67493aa625207662dbd2bf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.94M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9f24bedc34ec42ef8dc971feb32f5da8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/1.50M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0db972d644d04461908f40e2262b6ddf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/4.52M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "70d824753efe4048a972047c06d5aa29"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [278, 224, 153, 230, 331, 287, 224, 224, 291, 334, 224, 153, 231, 31849, 224, 224, 224, 153, 231, 346, 224, 224, 324, 224, 153, 230, 224, 224, 371, 224, 153, 236, 23803], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load pre-trained AraGPT-2 model and tokenizer\n",
        "model_name = 'aubmindlab/aragpt2-base'  # Example pre-trained Arabic GPT-2 model\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Prune tokenizer to use character-level tokenization\n",
        "tokenizer.add_tokens(['[PAD]'])  # Add padding token if needed\n",
        "tokenizer.pad_token = '[PAD]'\n",
        "\n",
        "# Replace the tokenizer's standard vocab with one that treats each character as a token\n",
        "# Here we manually re-tokenize at the character level\n",
        "def char_tokenize(text):\n",
        "    return tokenizer(list(text), is_split_into_words=True, padding=True, truncation=True)\n",
        "\n",
        "# Example of tokenizing at character level\n",
        "example_text = \"الحب أسمى من كل شيء\"\n",
        "char_tokens = char_tokenize(example_text)\n",
        "print(char_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336,
          "referenced_widgets": [
            "8bc615d310214c8a944f1fb7df49b68e",
            "6ce1ad96a8634f5f971fc6dccab7a647",
            "f6a73c7ff4bd4bf0ab733f6cca53bb18",
            "519632a3de3d44d29ec109e3905c06a0",
            "05d0c2cd90884217b648e0fc23f51b4d",
            "13b4bac3c220453e8644ec6e86a4c904",
            "3b1a8eef0a48455e945c5834c3a55a5c",
            "f5cc61f3d3c149f99af2341c8a508fdc",
            "f42249dd47f643e78da479060dbb9afd",
            "90da8120abee4f85befc420bcb605032",
            "08ecc8814f98474c84701bf45eda8fbd",
            "c01cca8fad67493aa625207662dbd2bf",
            "5de7e9082f2e46e0b4b122f328474d20",
            "b9088a15391540f5ae742bcd271f6940",
            "4818d9349327475987423f0f88da86d9",
            "3878fc32d0374430ac05f7b575110b83",
            "44b635616f1d42ad89d8487604115a97",
            "b05983b0f7ea4094a3084d5ebfd27c76",
            "743899884cbd40508f7d141f5289ef8b",
            "24243ebe89414dd0819a032b57016542",
            "6ad621f59f874f4ab185e7f2d62674c6",
            "01f800dc1ff447d7b5a9eb716ff2f0b4",
            "9f24bedc34ec42ef8dc971feb32f5da8",
            "4e40103084ac4befa03139aeb57809c3",
            "795cf628e9c945a7be2d07eb739aa8ab",
            "65d18175a53544ba98dc750f8cdb9807",
            "548425e527004878bd3f6a980a5db58e",
            "177523836dc442bea7ca2f4de7299dc6",
            "91a2168afd3e4391b1f7a662d4083ff7",
            "43180c03ea06423a9468d5b745502d1e",
            "a90740a2a7aa4c32aa233cedaed28620",
            "f0a2932046da451eb2ed2f048e929ee4",
            "446e6290354a4de7a4fd2965dc8fd06b",
            "0db972d644d04461908f40e2262b6ddf",
            "f0c1d7e8fe2f474f9937a90a13fbf5f2",
            "29a47d7c5c7b4b4383dd649e4fb3c6c2",
            "228c0326935f4d7085042e63c4163e5a",
            "6dbe62c7e1cc4f0f9d54963e6a6592ff",
            "d7a3cd5a05c243109f9b6e03cda990e1",
            "f3d4d7aa5d2149829eb4dbb5c9b9830b",
            "0de5a4c8bea74e879379fe4ae3389255",
            "d8e61caefa45423181985c473a8c6ed1",
            "9438feb031a746c5bddd5242f9cfe0eb",
            "73f65d08b7b0400e8537376d29cc540b",
            "70d824753efe4048a972047c06d5aa29",
            "424e5a47895044a79bb7c7571e18bb9b",
            "a41fb655b72744d7817fbfa270330e0f",
            "e9931c684ee147dc86cfecb9c91a0759",
            "350a9ce5405646d1b6e31eb8830666cb",
            "65a02f42a6c247e0b42e26d0e2159345",
            "9dadccb43ae04304993f5f3adef3a4b7",
            "b13f9c6345774c5891e321a6d6aa9579",
            "256d1f297f4c46359bf150a8c94940cc",
            "d3b86f8874d94584b6513fdd746d8893",
            "46c4cbc275424ad281ba9992afd8414a"
          ]
        },
        "outputId": "c6446a69-c48a-43df-bd38-d59e6175bb6c",
        "id": "P58WM_GMz_RO"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8bc615d310214c8a944f1fb7df49b68e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/553M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c01cca8fad67493aa625207662dbd2bf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.94M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9f24bedc34ec42ef8dc971feb32f5da8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/1.50M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0db972d644d04461908f40e2262b6ddf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/4.52M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "70d824753efe4048a972047c06d5aa29"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [278, 224, 153, 230, 331, 287, 224, 224, 291, 334, 224, 153, 231, 31849, 224, 224, 224, 153, 231, 346, 224, 224, 324, 224, 153, 230, 224, 224, 371, 224, 153, 236, 23803], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load pre-trained AraGPT-2 model and tokenizer\n",
        "model_name = 'aubmindlab/aragpt2-base'  # Example pre-trained Arabic GPT-2 model\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Prune tokenizer to use character-level tokenization\n",
        "tokenizer.add_tokens(['[PAD]'])  # Add padding token if needed\n",
        "tokenizer.pad_token = '[PAD]'\n",
        "\n",
        "# Replace the tokenizer's standard vocab with one that treats each character as a token\n",
        "# Here we manually re-tokenize at the character level\n",
        "def char_tokenize(text):\n",
        "    return tokenizer(list(text), is_split_into_words=True, padding=True, truncation=True)\n",
        "\n",
        "# Example of tokenizing at character level\n",
        "example_text = \"الحب أسمى من كل شيء\"\n",
        "char_tokens = char_tokenize(example_text)\n",
        "print(char_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336,
          "referenced_widgets": [
            "8bc615d310214c8a944f1fb7df49b68e",
            "6ce1ad96a8634f5f971fc6dccab7a647",
            "f6a73c7ff4bd4bf0ab733f6cca53bb18",
            "519632a3de3d44d29ec109e3905c06a0",
            "05d0c2cd90884217b648e0fc23f51b4d",
            "13b4bac3c220453e8644ec6e86a4c904",
            "3b1a8eef0a48455e945c5834c3a55a5c",
            "f5cc61f3d3c149f99af2341c8a508fdc",
            "f42249dd47f643e78da479060dbb9afd",
            "90da8120abee4f85befc420bcb605032",
            "08ecc8814f98474c84701bf45eda8fbd",
            "c01cca8fad67493aa625207662dbd2bf",
            "5de7e9082f2e46e0b4b122f328474d20",
            "b9088a15391540f5ae742bcd271f6940",
            "4818d9349327475987423f0f88da86d9",
            "3878fc32d0374430ac05f7b575110b83",
            "44b635616f1d42ad89d8487604115a97",
            "b05983b0f7ea4094a3084d5ebfd27c76",
            "743899884cbd40508f7d141f5289ef8b",
            "24243ebe89414dd0819a032b57016542",
            "6ad621f59f874f4ab185e7f2d62674c6",
            "01f800dc1ff447d7b5a9eb716ff2f0b4",
            "9f24bedc34ec42ef8dc971feb32f5da8",
            "4e40103084ac4befa03139aeb57809c3",
            "795cf628e9c945a7be2d07eb739aa8ab",
            "65d18175a53544ba98dc750f8cdb9807",
            "548425e527004878bd3f6a980a5db58e",
            "177523836dc442bea7ca2f4de7299dc6",
            "91a2168afd3e4391b1f7a662d4083ff7",
            "43180c03ea06423a9468d5b745502d1e",
            "a90740a2a7aa4c32aa233cedaed28620",
            "f0a2932046da451eb2ed2f048e929ee4",
            "446e6290354a4de7a4fd2965dc8fd06b",
            "0db972d644d04461908f40e2262b6ddf",
            "f0c1d7e8fe2f474f9937a90a13fbf5f2",
            "29a47d7c5c7b4b4383dd649e4fb3c6c2",
            "228c0326935f4d7085042e63c4163e5a",
            "6dbe62c7e1cc4f0f9d54963e6a6592ff",
            "d7a3cd5a05c243109f9b6e03cda990e1",
            "f3d4d7aa5d2149829eb4dbb5c9b9830b",
            "0de5a4c8bea74e879379fe4ae3389255",
            "d8e61caefa45423181985c473a8c6ed1",
            "9438feb031a746c5bddd5242f9cfe0eb",
            "73f65d08b7b0400e8537376d29cc540b",
            "70d824753efe4048a972047c06d5aa29",
            "424e5a47895044a79bb7c7571e18bb9b",
            "a41fb655b72744d7817fbfa270330e0f",
            "e9931c684ee147dc86cfecb9c91a0759",
            "350a9ce5405646d1b6e31eb8830666cb",
            "65a02f42a6c247e0b42e26d0e2159345",
            "9dadccb43ae04304993f5f3adef3a4b7",
            "b13f9c6345774c5891e321a6d6aa9579",
            "256d1f297f4c46359bf150a8c94940cc",
            "d3b86f8874d94584b6513fdd746d8893",
            "46c4cbc275424ad281ba9992afd8414a"
          ]
        },
        "outputId": "c6446a69-c48a-43df-bd38-d59e6175bb6c",
        "id": "6WWixhy9z_lN"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8bc615d310214c8a944f1fb7df49b68e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/553M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c01cca8fad67493aa625207662dbd2bf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.94M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9f24bedc34ec42ef8dc971feb32f5da8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/1.50M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0db972d644d04461908f40e2262b6ddf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/4.52M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "70d824753efe4048a972047c06d5aa29"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [278, 224, 153, 230, 331, 287, 224, 224, 291, 334, 224, 153, 231, 31849, 224, 224, 224, 153, 231, 346, 224, 224, 324, 224, 153, 230, 224, 224, 371, 224, 153, 236, 23803], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load pre-trained AraGPT-2 model and tokenizer\n",
        "model_name = 'aubmindlab/aragpt2-base'  # Example pre-trained Arabic GPT-2 model\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Prune tokenizer to use character-level tokenization\n",
        "tokenizer.add_tokens(['[PAD]'])  # Add padding token if needed\n",
        "tokenizer.pad_token = '[PAD]'\n",
        "\n",
        "# Replace the tokenizer's standard vocab with one that treats each character as a token\n",
        "# Here we manually re-tokenize at the character level\n",
        "def char_tokenize(text):\n",
        "    return tokenizer(list(text), is_split_into_words=True, padding=True, truncation=True)\n",
        "\n",
        "# Example of tokenizing at character level\n",
        "example_text = \"الحب أسمى من كل شيء\"\n",
        "char_tokens = char_tokenize(example_text)\n",
        "print(char_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336,
          "referenced_widgets": [
            "8bc615d310214c8a944f1fb7df49b68e",
            "6ce1ad96a8634f5f971fc6dccab7a647",
            "f6a73c7ff4bd4bf0ab733f6cca53bb18",
            "519632a3de3d44d29ec109e3905c06a0",
            "05d0c2cd90884217b648e0fc23f51b4d",
            "13b4bac3c220453e8644ec6e86a4c904",
            "3b1a8eef0a48455e945c5834c3a55a5c",
            "f5cc61f3d3c149f99af2341c8a508fdc",
            "f42249dd47f643e78da479060dbb9afd",
            "90da8120abee4f85befc420bcb605032",
            "08ecc8814f98474c84701bf45eda8fbd",
            "c01cca8fad67493aa625207662dbd2bf",
            "5de7e9082f2e46e0b4b122f328474d20",
            "b9088a15391540f5ae742bcd271f6940",
            "4818d9349327475987423f0f88da86d9",
            "3878fc32d0374430ac05f7b575110b83",
            "44b635616f1d42ad89d8487604115a97",
            "b05983b0f7ea4094a3084d5ebfd27c76",
            "743899884cbd40508f7d141f5289ef8b",
            "24243ebe89414dd0819a032b57016542",
            "6ad621f59f874f4ab185e7f2d62674c6",
            "01f800dc1ff447d7b5a9eb716ff2f0b4",
            "9f24bedc34ec42ef8dc971feb32f5da8",
            "4e40103084ac4befa03139aeb57809c3",
            "795cf628e9c945a7be2d07eb739aa8ab",
            "65d18175a53544ba98dc750f8cdb9807",
            "548425e527004878bd3f6a980a5db58e",
            "177523836dc442bea7ca2f4de7299dc6",
            "91a2168afd3e4391b1f7a662d4083ff7",
            "43180c03ea06423a9468d5b745502d1e",
            "a90740a2a7aa4c32aa233cedaed28620",
            "f0a2932046da451eb2ed2f048e929ee4",
            "446e6290354a4de7a4fd2965dc8fd06b",
            "0db972d644d04461908f40e2262b6ddf",
            "f0c1d7e8fe2f474f9937a90a13fbf5f2",
            "29a47d7c5c7b4b4383dd649e4fb3c6c2",
            "228c0326935f4d7085042e63c4163e5a",
            "6dbe62c7e1cc4f0f9d54963e6a6592ff",
            "d7a3cd5a05c243109f9b6e03cda990e1",
            "f3d4d7aa5d2149829eb4dbb5c9b9830b",
            "0de5a4c8bea74e879379fe4ae3389255",
            "d8e61caefa45423181985c473a8c6ed1",
            "9438feb031a746c5bddd5242f9cfe0eb",
            "73f65d08b7b0400e8537376d29cc540b",
            "70d824753efe4048a972047c06d5aa29",
            "424e5a47895044a79bb7c7571e18bb9b",
            "a41fb655b72744d7817fbfa270330e0f",
            "e9931c684ee147dc86cfecb9c91a0759",
            "350a9ce5405646d1b6e31eb8830666cb",
            "65a02f42a6c247e0b42e26d0e2159345",
            "9dadccb43ae04304993f5f3adef3a4b7",
            "b13f9c6345774c5891e321a6d6aa9579",
            "256d1f297f4c46359bf150a8c94940cc",
            "d3b86f8874d94584b6513fdd746d8893",
            "46c4cbc275424ad281ba9992afd8414a"
          ]
        },
        "outputId": "c6446a69-c48a-43df-bd38-d59e6175bb6c",
        "id": "iKS-T6UZz_oA"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8bc615d310214c8a944f1fb7df49b68e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/553M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c01cca8fad67493aa625207662dbd2bf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.94M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9f24bedc34ec42ef8dc971feb32f5da8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/1.50M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0db972d644d04461908f40e2262b6ddf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/4.52M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "70d824753efe4048a972047c06d5aa29"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [278, 224, 153, 230, 331, 287, 224, 224, 291, 334, 224, 153, 231, 31849, 224, 224, 224, 153, 231, 346, 224, 224, 324, 224, 153, 230, 224, 224, 371, 224, 153, 236, 23803], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load pre-trained AraGPT-2 model and tokenizer\n",
        "model_name = 'aubmindlab/aragpt2-base'  # Example pre-trained Arabic GPT-2 model\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Prune tokenizer to use character-level tokenization\n",
        "tokenizer.add_tokens(['[PAD]'])  # Add padding token if needed\n",
        "tokenizer.pad_token = '[PAD]'\n",
        "\n",
        "# Replace the tokenizer's standard vocab with one that treats each character as a token\n",
        "# Here we manually re-tokenize at the character level\n",
        "def char_tokenize(text):\n",
        "    return tokenizer(list(text), is_split_into_words=True, padding=True, truncation=True)\n",
        "\n",
        "# Example of tokenizing at character level\n",
        "example_text = \"الحب أسمى من كل شيء\"\n",
        "char_tokens = char_tokenize(example_text)\n",
        "print(char_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336,
          "referenced_widgets": [
            "8bc615d310214c8a944f1fb7df49b68e",
            "6ce1ad96a8634f5f971fc6dccab7a647",
            "f6a73c7ff4bd4bf0ab733f6cca53bb18",
            "519632a3de3d44d29ec109e3905c06a0",
            "05d0c2cd90884217b648e0fc23f51b4d",
            "13b4bac3c220453e8644ec6e86a4c904",
            "3b1a8eef0a48455e945c5834c3a55a5c",
            "f5cc61f3d3c149f99af2341c8a508fdc",
            "f42249dd47f643e78da479060dbb9afd",
            "90da8120abee4f85befc420bcb605032",
            "08ecc8814f98474c84701bf45eda8fbd",
            "c01cca8fad67493aa625207662dbd2bf",
            "5de7e9082f2e46e0b4b122f328474d20",
            "b9088a15391540f5ae742bcd271f6940",
            "4818d9349327475987423f0f88da86d9",
            "3878fc32d0374430ac05f7b575110b83",
            "44b635616f1d42ad89d8487604115a97",
            "b05983b0f7ea4094a3084d5ebfd27c76",
            "743899884cbd40508f7d141f5289ef8b",
            "24243ebe89414dd0819a032b57016542",
            "6ad621f59f874f4ab185e7f2d62674c6",
            "01f800dc1ff447d7b5a9eb716ff2f0b4",
            "9f24bedc34ec42ef8dc971feb32f5da8",
            "4e40103084ac4befa03139aeb57809c3",
            "795cf628e9c945a7be2d07eb739aa8ab",
            "65d18175a53544ba98dc750f8cdb9807",
            "548425e527004878bd3f6a980a5db58e",
            "177523836dc442bea7ca2f4de7299dc6",
            "91a2168afd3e4391b1f7a662d4083ff7",
            "43180c03ea06423a9468d5b745502d1e",
            "a90740a2a7aa4c32aa233cedaed28620",
            "f0a2932046da451eb2ed2f048e929ee4",
            "446e6290354a4de7a4fd2965dc8fd06b",
            "0db972d644d04461908f40e2262b6ddf",
            "f0c1d7e8fe2f474f9937a90a13fbf5f2",
            "29a47d7c5c7b4b4383dd649e4fb3c6c2",
            "228c0326935f4d7085042e63c4163e5a",
            "6dbe62c7e1cc4f0f9d54963e6a6592ff",
            "d7a3cd5a05c243109f9b6e03cda990e1",
            "f3d4d7aa5d2149829eb4dbb5c9b9830b",
            "0de5a4c8bea74e879379fe4ae3389255",
            "d8e61caefa45423181985c473a8c6ed1",
            "9438feb031a746c5bddd5242f9cfe0eb",
            "73f65d08b7b0400e8537376d29cc540b",
            "70d824753efe4048a972047c06d5aa29",
            "424e5a47895044a79bb7c7571e18bb9b",
            "a41fb655b72744d7817fbfa270330e0f",
            "e9931c684ee147dc86cfecb9c91a0759",
            "350a9ce5405646d1b6e31eb8830666cb",
            "65a02f42a6c247e0b42e26d0e2159345",
            "9dadccb43ae04304993f5f3adef3a4b7",
            "b13f9c6345774c5891e321a6d6aa9579",
            "256d1f297f4c46359bf150a8c94940cc",
            "d3b86f8874d94584b6513fdd746d8893",
            "46c4cbc275424ad281ba9992afd8414a"
          ]
        },
        "outputId": "c6446a69-c48a-43df-bd38-d59e6175bb6c",
        "id": "l4V6w6E2z_xI"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8bc615d310214c8a944f1fb7df49b68e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/553M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c01cca8fad67493aa625207662dbd2bf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.94M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9f24bedc34ec42ef8dc971feb32f5da8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/1.50M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0db972d644d04461908f40e2262b6ddf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/4.52M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "70d824753efe4048a972047c06d5aa29"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [278, 224, 153, 230, 331, 287, 224, 224, 291, 334, 224, 153, 231, 31849, 224, 224, 224, 153, 231, 346, 224, 224, 324, 224, 153, 230, 224, 224, 371, 224, 153, 236, 23803], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load pre-trained AraGPT-2 model and tokenizer\n",
        "model_name = 'aubmindlab/aragpt2-base'  # Example pre-trained Arabic GPT-2 model\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Prune tokenizer to use character-level tokenization\n",
        "tokenizer.add_tokens(['[PAD]'])  # Add padding token if needed\n",
        "tokenizer.pad_token = '[PAD]'\n",
        "\n",
        "# Replace the tokenizer's standard vocab with one that treats each character as a token\n",
        "# Here we manually re-tokenize at the character level\n",
        "def char_tokenize(text):\n",
        "    return tokenizer(list(text), is_split_into_words=True, padding=True, truncation=True)\n",
        "\n",
        "# Example of tokenizing at character level\n",
        "example_text = \"الحب أسمى من كل شيء\"\n",
        "char_tokens = char_tokenize(example_text)\n",
        "print(char_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336,
          "referenced_widgets": [
            "8bc615d310214c8a944f1fb7df49b68e",
            "6ce1ad96a8634f5f971fc6dccab7a647",
            "f6a73c7ff4bd4bf0ab733f6cca53bb18",
            "519632a3de3d44d29ec109e3905c06a0",
            "05d0c2cd90884217b648e0fc23f51b4d",
            "13b4bac3c220453e8644ec6e86a4c904",
            "3b1a8eef0a48455e945c5834c3a55a5c",
            "f5cc61f3d3c149f99af2341c8a508fdc",
            "f42249dd47f643e78da479060dbb9afd",
            "90da8120abee4f85befc420bcb605032",
            "08ecc8814f98474c84701bf45eda8fbd",
            "c01cca8fad67493aa625207662dbd2bf",
            "5de7e9082f2e46e0b4b122f328474d20",
            "b9088a15391540f5ae742bcd271f6940",
            "4818d9349327475987423f0f88da86d9",
            "3878fc32d0374430ac05f7b575110b83",
            "44b635616f1d42ad89d8487604115a97",
            "b05983b0f7ea4094a3084d5ebfd27c76",
            "743899884cbd40508f7d141f5289ef8b",
            "24243ebe89414dd0819a032b57016542",
            "6ad621f59f874f4ab185e7f2d62674c6",
            "01f800dc1ff447d7b5a9eb716ff2f0b4",
            "9f24bedc34ec42ef8dc971feb32f5da8",
            "4e40103084ac4befa03139aeb57809c3",
            "795cf628e9c945a7be2d07eb739aa8ab",
            "65d18175a53544ba98dc750f8cdb9807",
            "548425e527004878bd3f6a980a5db58e",
            "177523836dc442bea7ca2f4de7299dc6",
            "91a2168afd3e4391b1f7a662d4083ff7",
            "43180c03ea06423a9468d5b745502d1e",
            "a90740a2a7aa4c32aa233cedaed28620",
            "f0a2932046da451eb2ed2f048e929ee4",
            "446e6290354a4de7a4fd2965dc8fd06b",
            "0db972d644d04461908f40e2262b6ddf",
            "f0c1d7e8fe2f474f9937a90a13fbf5f2",
            "29a47d7c5c7b4b4383dd649e4fb3c6c2",
            "228c0326935f4d7085042e63c4163e5a",
            "6dbe62c7e1cc4f0f9d54963e6a6592ff",
            "d7a3cd5a05c243109f9b6e03cda990e1",
            "f3d4d7aa5d2149829eb4dbb5c9b9830b",
            "0de5a4c8bea74e879379fe4ae3389255",
            "d8e61caefa45423181985c473a8c6ed1",
            "9438feb031a746c5bddd5242f9cfe0eb",
            "73f65d08b7b0400e8537376d29cc540b",
            "70d824753efe4048a972047c06d5aa29",
            "424e5a47895044a79bb7c7571e18bb9b",
            "a41fb655b72744d7817fbfa270330e0f",
            "e9931c684ee147dc86cfecb9c91a0759",
            "350a9ce5405646d1b6e31eb8830666cb",
            "65a02f42a6c247e0b42e26d0e2159345",
            "9dadccb43ae04304993f5f3adef3a4b7",
            "b13f9c6345774c5891e321a6d6aa9579",
            "256d1f297f4c46359bf150a8c94940cc",
            "d3b86f8874d94584b6513fdd746d8893",
            "46c4cbc275424ad281ba9992afd8414a"
          ]
        },
        "outputId": "c6446a69-c48a-43df-bd38-d59e6175bb6c",
        "id": "cK78AmPBz_5m"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8bc615d310214c8a944f1fb7df49b68e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/553M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c01cca8fad67493aa625207662dbd2bf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.94M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9f24bedc34ec42ef8dc971feb32f5da8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/1.50M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0db972d644d04461908f40e2262b6ddf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/4.52M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "70d824753efe4048a972047c06d5aa29"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [278, 224, 153, 230, 331, 287, 224, 224, 291, 334, 224, 153, 231, 31849, 224, 224, 224, 153, 231, 346, 224, 224, 324, 224, 153, 230, 224, 224, 371, 224, 153, 236, 23803], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# Prepare dataset (assuming dataset is preprocessed and ready)\n",
        "train_dataset = [{\"input_ids\": tokenizer.encode(text)} for text in preprocessed_data]  # Example format\n",
        "\n",
        "# Define the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory for model predictions\n",
        "    evaluation_strategy=\"epoch\",     # Evaluation strategy\n",
        "    per_device_train_batch_size=8,   # Batch size for training\n",
        "    num_train_epochs=3,              # Number of epochs\n",
        "    logging_dir='./logs',            # Directory for logging\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the model being trained\n",
        "    args=training_args,                  # training arguments\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        ")\n",
        "\n",
        "# Start fine-tuning\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "KZzsvpf34DvA",
        "outputId": "6c58106f-dabd-4988-81cb-9579d44266a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "You have set `args.eval_strategy` to IntervalStrategy.EPOCH but you didn't pass an `eval_dataset` to `Trainer`. Either set `args.eval_strategy` to `no` or pass an `eval_dataset`. ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-761e245360d9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Initialize the Trainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m trainer = Trainer(\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m                         \u001b[0;31m# the model being trained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m                  \u001b[0;31m# training arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    439\u001b[0m                 )\n\u001b[1;32m    440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_strategy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_strategy\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"no\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0meval_dataset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    442\u001b[0m                 \u001b[0;34mf\"You have set `args.eval_strategy` to {args.eval_strategy} but you didn't pass an `eval_dataset` to `Trainer`. Either set `args.eval_strategy` to `no` or pass an `eval_dataset`. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: You have set `args.eval_strategy` to IntervalStrategy.EPOCH but you didn't pass an `eval_dataset` to `Trainer`. Either set `args.eval_strategy` to `no` or pass an `eval_dataset`. "
          ]
        }
      ]
    },
    {
      "source": [
        "# Import the necessary modules.\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
        "import pyarabic.araby as araby\n",
        "import string\n",
        "\n",
        "# Helper function to remove punctuation\n",
        "def remove_punctuation(text):\n",
        "    # Create a translation table to remove punctuation\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    # Remove punctuation from the text\n",
        "    return text.translate(translator)\n",
        "\n",
        "# Preprocess Arabic text by removing diacritics and normalizing\n",
        "def preprocess_text(text):\n",
        "    # Remove diacritics\n",
        "    text = araby.strip_tashkeel(text)\n",
        "    # Normalize text (convert variations of Arabic letters to a standard form)\n",
        "    text = araby.normalize_ligature(text)\n",
        "    # Remove unwanted characters (punctuation in this case)\n",
        "    text = remove_punctuation(text) # call remove_punctuation instead of araby.strip_punctuation\n",
        "    return text\n",
        "\n",
        "# Example: Apply preprocessing to the dataset\n",
        "def preprocess_dataset(dataset):\n",
        "    return [preprocess_text(verse) for verse in dataset]\n",
        "\n",
        "# Example datasets (replace with actual dataset loading code)\n",
        "dataset = [\"الحب أسمى من كل شيء\", \"في السماء طيور\"]\n",
        "preprocessed_data = preprocess_dataset(dataset)\n",
        "\n",
        "# Load pre-trained AraGPT-2 model and tokenizer\n",
        "model_name = 'aubmindlab/aragpt2-base'  # Example pre-trained Arabic GPT-2 model\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Prune tokenizer to use character-level tokenization\n",
        "tokenizer.add_tokens(['[PAD]'])  # Add padding token if needed\n",
        "tokenizer.pad_token = '[PAD]'\n",
        "\n",
        "# Replace the tokenizer's standard vocab with one that treats each character as a token\n",
        "# Here we manually re-tokenize at the character level\n",
        "def char_tokenize(text):\n",
        "    return tokenizer(list(text), is_split_into_words=True, padding=True, truncation=True)\n",
        "\n",
        "# Example of tokenizing at character level\n",
        "example_text = \"الحب أسمى من كل شيء\"\n",
        "char_tokens = char_tokenize(example_text)\n",
        "print(char_tokens)\n",
        "\n",
        "# Prepare dataset (assuming dataset is preprocessed and ready)\n",
        "train_dataset = [{\"input_ids\": tokenizer.encode(text)} for text in preprocessed_data]  # Example format\n",
        "\n",
        "# Create dummy eval_dataset\n",
        "eval_dataset = [{\"input_ids\": tokenizer.encode(text)} for text in [\"نص للتقييم\"]]\n",
        "\n",
        "# Define the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory for model predictions\n",
        "    evaluation_strategy=\"epoch\",     # Evaluation strategy\n",
        "    per_device_train_batch_size=8,   # Batch size for training\n",
        "    num_train_epochs=3,              # Number of epochs\n",
        "    logging_dir='./logs',            # Directory for logging\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the model being trained\n",
        "    args=training_args,                  # training arguments\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=eval_dataset, # add the eval_dataset\n",
        ")\n",
        "\n",
        "# Start fine-tuning\n",
        "trainer.train()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 688
        },
        "id": "Rc9GLiCW6rIt",
        "outputId": "c776ea02-816a-4f84-b69d-b5384cd5aab2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [278, 224, 153, 230, 331, 287, 224, 224, 291, 334, 224, 153, 231, 31849, 224, 224, 224, 153, 231, 346, 224, 224, 324, 224, 153, 230, 224, 224, 371, 224, 153, 236, 23803], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhussainezzi03\u001b[0m (\u001b[33mhussainezzi03-aljamea-tus-saifiyah\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250227_103230-f4mb5cmo</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/hussainezzi03-aljamea-tus-saifiyah/huggingface/runs/f4mb5cmo' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/hussainezzi03-aljamea-tus-saifiyah/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/hussainezzi03-aljamea-tus-saifiyah/huggingface' target=\"_blank\">https://wandb.ai/hussainezzi03-aljamea-tus-saifiyah/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/hussainezzi03-aljamea-tus-saifiyah/huggingface/runs/f4mb5cmo' target=\"_blank\">https://wandb.ai/hussainezzi03-aljamea-tus-saifiyah/huggingface/runs/f4mb5cmo</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "expected sequence of length 6 at dim 1 (got 3)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-5a6386b4e762>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m# Start fine-tuning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2169\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2170\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2171\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2172\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2173\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2478\u001b[0m                 \u001b[0mupdate_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2479\u001b[0m                 \u001b[0mnum_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mupdate_step\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_updates\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mremainder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2480\u001b[0;31m                 \u001b[0mbatch_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2481\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2482\u001b[0m                     \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mget_batch_samples\u001b[0;34m(self, epoch_iterator, num_batches)\u001b[0m\n\u001b[1;32m   5151\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5152\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5153\u001b[0;31m                 \u001b[0mbatch_samples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5154\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5155\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/data_loader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;31m# We iterate one batch ahead to check when we are at the end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0mcurrent_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m             \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    846\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_remove_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_collator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/data/data_collator.py\u001b[0m in \u001b[0;36mdefault_data_collator\u001b[0;34m(features, return_tensors)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_tensors\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch_default_data_collator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mreturn_tensors\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tf\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf_default_data_collator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/data/data_collator.py\u001b[0m in \u001b[0;36mtorch_default_data_collator\u001b[0;34m(features)\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m                 \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: expected sequence of length 6 at dim 1 (got 3)"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip install transformers torch pyarabic datasets"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "t1XSAE4xBe8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Import the necessary modules.\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments, DataCollatorWithPadding\n",
        "import pyarabic.araby as araby\n",
        "import string\n",
        "import torch\n",
        "\n",
        "# Helper function to remove punctuation\n",
        "def remove_punctuation(text):\n",
        "    # Create a translation table to remove punctuation\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    # Remove punctuation from the text\n",
        "    return text.translate(translator)\n",
        "\n",
        "# Preprocess Arabic text by removing diacritics and normalizing\n",
        "def preprocess_text(text):\n",
        "    # Remove diacritics\n",
        "    text = araby.strip_tashkeel(text)\n",
        "    # Normalize text (convert variations of Arabic letters to a standard form)\n",
        "    text = araby.normalize_ligature(text)\n",
        "    # Remove unwanted characters (punctuation in this case)\n",
        "    text = remove_punctuation(text) # call remove_punctuation instead of araby.strip_punctuation\n",
        "    return text\n",
        "\n",
        "# Example: Apply preprocessing to the dataset\n",
        "def preprocess_dataset(dataset):\n",
        "    return [preprocess_text(verse) for verse in dataset]\n",
        "\n",
        "# Example datasets (replace with actual dataset loading code)\n",
        "dataset = [\"الحب أسمى من كل شيء\", \"في السماء طيور\"]\n",
        "preprocessed_data = preprocess_dataset(dataset)\n",
        "\n",
        "# Load pre-trained AraGPT-2 model and tokenizer\n",
        "model_name = 'aubmindlab/aragpt2-base'  # Example pre-trained Arabic GPT-2 model\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Prune tokenizer to use character-level tokenization\n",
        "tokenizer.add_tokens(['[PAD]'])  # Add padding token if needed\n",
        "tokenizer.pad_token = '[PAD]'\n",
        "\n",
        "# Replace the tokenizer's standard vocab with one that treats each character as a token\n",
        "# Here we manually re-tokenize at the character level\n",
        "def char_tokenize(text):\n",
        "    return tokenizer(list(text), is_split_into_words=True, padding=True, truncation=True, return_tensors=\"pt\") # tokenize character by character and return a pytorch tensor.\n",
        "\n",
        "# Example of tokenizing at character level\n",
        "example_text = \"الحب أسمى من كل شيء\"\n",
        "char_tokens = char_tokenize(example_text)\n",
        "print(char_tokens)\n",
        "\n",
        "# Correctly prepare the dataset: tokenize each character, then pad.\n",
        "train_dataset = []\n",
        "max_length = 0\n",
        "for text in preprocessed_data:\n",
        "    tokenized_chars = char_tokenize(text)\n",
        "    max_length = max(max_length, tokenized_chars[\"input_ids\"].shape[1]) # find the max length of each of the character by character sequences.\n",
        "    train_dataset.append(tokenized_chars)\n",
        "\n",
        "# padding\n",
        "for item in train_dataset:\n",
        "    current_length = item[\"input_ids\"].shape[1]\n",
        "    if current_length < max_length:\n",
        "      pad_length = max_length - current_length\n",
        "      padding_tensor = torch.full((1, pad_length), tokenizer.pad_token_id)\n",
        "      item[\"input_ids\"] = torch.cat((item[\"input_ids\"], padding_tensor), dim=1)\n",
        "      item[\"attention_mask\"] = torch.cat((item[\"attention_mask\"], torch.zeros((1,pad_length))), dim=1)\n",
        "\n",
        "# Create dummy eval_dataset\n",
        "eval_dataset = []\n",
        "for text in [\"نص للتقييم\"]:\n",
        "  tokenized_chars = char_tokenize(text)\n",
        "  eval_dataset.append(tokenized_chars)\n",
        "\n",
        "# padding\n",
        "for item in eval_dataset:\n",
        "    current_length = item[\"input_ids\"].shape[1]\n",
        "    if current_length < max_length:\n",
        "      pad_length = max_length - current_length\n",
        "      padding_tensor = torch.full((1, pad_length), tokenizer.pad_token_id)\n",
        "      item[\"input_ids\"] = torch.cat((item[\"input_ids\"], padding_tensor), dim=1)\n",
        "      item[\"attention_mask\"] = torch.cat((item[\"attention_mask\"], torch.zeros((1,pad_length))), dim=1)\n",
        "\n",
        "# Define the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory for model predictions\n",
        "    evaluation_strategy=\"epoch\",     # Evaluation strategy\n",
        "    per_device_train_batch_size=1,   # Batch size for training # set to 1 since they are different sizes\n",
        "    num_train_epochs=3,              # Number of epochs\n",
        "    logging_dir='./logs',            # Directory for logging\n",
        ")\n",
        "# Create a custom DataCollatorWithPadding\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the model being trained\n",
        "    args=training_args,                  # training arguments\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=eval_dataset, # add the eval_dataset\n",
        "    data_collator=data_collator # add the data_collator\n",
        ")\n",
        "\n",
        "# Start fine-tuning\n",
        "trainer.train()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "L7dJLo6LBflP",
        "outputId": "069ee844-5ec2-412b-b739-d3c246850527"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  278,   224,   153,   230,   331,   287,   224,   224,   291,   334,\n",
            "           224,   153,   231, 31849,   224,   224,   224,   153,   231,   346,\n",
            "           224,   224,   324,   224,   153,   230,   224,   224,   371,   224,\n",
            "           153,   236, 23803]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "The model did not return a loss from the inputs, only the following keys: logits,past_key_values. For reference, the inputs it received are input_ids,attention_mask.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-6ac7b8911060>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;31m# Start fine-tuning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2169\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2170\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2171\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2172\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2173\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2529\u001b[0m                     )\n\u001b[1;32m   2530\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2531\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2533\u001b[0m                     if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3674\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3675\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3677\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3750\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3751\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"loss\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3752\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m   3753\u001b[0m                     \u001b[0;34m\"The model did not return a loss from the inputs, only the following keys: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3754\u001b[0m                     \u001b[0;34mf\"{','.join(outputs.keys())}. For reference, the inputs it received are {','.join(inputs.keys())}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The model did not return a loss from the inputs, only the following keys: logits,past_key_values. For reference, the inputs it received are input_ids,attention_mask."
          ]
        }
      ]
    },
    {
      "source": [
        "# Import the necessary modules.\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "import pyarabic.araby as araby\n",
        "import string\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "\n",
        "# Helper function to remove punctuation\n",
        "def remove_punctuation(text):\n",
        "    # Create a translation table to remove punctuation\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    # Remove punctuation from the text\n",
        "    return text.translate(translator)\n",
        "\n",
        "# Preprocess Arabic text by removing diacritics and normalizing\n",
        "def preprocess_text(text):\n",
        "    # Remove diacritics\n",
        "    text = araby.strip_tashkeel(text)\n",
        "    # Normalize text (convert variations of Arabic letters to a standard form)\n",
        "    text = araby.normalize_ligature(text)\n",
        "    # Remove unwanted characters (punctuation in this case)\n",
        "    text = remove_punctuation(text) # call remove_punctuation instead of araby.strip_punctuation\n",
        "    return text\n",
        "\n",
        "# Example: Apply preprocessing to the dataset\n",
        "def preprocess_dataset(dataset):\n",
        "    return [preprocess_text(verse) for verse in dataset]\n",
        "\n",
        "# Example datasets (replace with actual dataset loading code)\n",
        "dataset = [\"الحب أسمى من كل شيء\", \"في السماء طيور\"]\n",
        "preprocessed_data = preprocess_dataset(dataset)\n",
        "\n",
        "# Load pre-trained AraGPT-2 model and tokenizer\n",
        "model_name = 'aubmindlab/aragpt2-base'  # Example pre-trained Arabic GPT-2 model\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Prune tokenizer to use character-level tokenization\n",
        "tokenizer.add_tokens(['[PAD]'])  # Add padding token if needed\n",
        "tokenizer.pad_token = '[PAD]'\n",
        "\n",
        "# Replace the tokenizer's standard vocab with one that treats each character as a token\n",
        "# Here we manually re-tokenize at the character level\n",
        "def char_tokenize(text):\n",
        "    return tokenizer(list(text), is_split_into_words=True, padding=True, truncation=True, return_tensors=\"pt\") # tokenize character by character and return a pytorch tensor.\n",
        "\n",
        "# Example of tokenizing at character level\n",
        "example_text = \"الحب أسمى من كل شيء\"\n",
        "char_tokens = char_tokenize(example_text)\n",
        "print(char_tokens)\n",
        "\n",
        "# Tokenize all texts in preprocessed_data and make a dataset.\n",
        "tokenized_data = [tokenizer(list(text), is_split_into_words=True, padding='max_length', truncation=True, max_length=33, return_tensors='pt') for text in preprocessed_data]\n",
        "\n",
        "# Add labels to the tokenized_data.\n",
        "for item in tokenized_data:\n",
        "  item[\"labels\"] = item[\"input_ids\"].clone()\n",
        "\n",
        "# Create dummy eval_dataset\n",
        "tokenized_eval_data = [tokenizer(list(text), is_split_into_words=True, padding='max_length', truncation=True, max_length=33, return_tensors='pt') for text in [\"نص للتقييم\"]]\n",
        "for item in tokenized_eval_data:\n",
        "    item[\"labels\"] = item[\"input_ids\"].clone()\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = Dataset.from_list([item for item in tokenized_data])\n",
        "eval_dataset = Dataset.from_list([item for item in tokenized_eval_data])\n",
        "\n",
        "# Define the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory for model predictions\n",
        "    evaluation_strategy=\"epoch\",     # Evaluation strategy\n",
        "    per_device_train_batch_size=1,   # Batch size for training # set to 1 since they are different sizes\n",
        "    num_train_epochs=3,              # Number of epochs\n",
        "    logging_dir='./logs',            # Directory for logging\n",
        ")\n",
        "\n",
        "# Use DataCollatorForLanguageModeling\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=False\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the model being trained\n",
        "    args=training_args,                  # training arguments\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=eval_dataset, # add the eval_dataset\n",
        "    data_collator=data_collator # add the data_collator\n",
        ")\n",
        "\n",
        "# Start fine-tuning\n",
        "trainer.train()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "k__ZjXMxCn4A",
        "outputId": "d6d7faf6-f786-4b17-b2c8-f19d9efd90da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  278,   224,   153,   230,   331,   287,   224,   224,   291,   334,\n",
            "           224,   153,   231, 31849,   224,   224,   224,   153,   231,   346,\n",
            "           224,   224,   324,   224,   153,   230,   224,   224,   371,   224,\n",
            "           153,   236, 23803]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/6 : < :, Epoch 0.50/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "index out of range in self",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-4ce3e695126e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;31m# Start fine-tuning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2169\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2170\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2171\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2172\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2173\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2529\u001b[0m                     )\n\u001b[1;32m   2530\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2531\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2533\u001b[0m                     if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3674\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3675\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3677\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3729\u001b[0m                 \u001b[0mloss_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_items_in_batch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3730\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mloss_kwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3731\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3732\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3733\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1060\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1062\u001b[0;31m         transformer_outputs = self.transformer(\n\u001b[0m\u001b[1;32m   1063\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m             \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwte\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m         \u001b[0mposition_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwpe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mposition_embeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2549\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
          ]
        }
      ]
    },
    {
      "source": [
        "# Import the necessary modules.\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "import pyarabic.araby as araby\n",
        "import string\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "\n",
        "# Helper function to remove punctuation\n",
        "def remove_punctuation(text):\n",
        "    # Create a translation table to remove punctuation\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    # Remove punctuation from the text\n",
        "    return text.translate(translator)\n",
        "\n",
        "# Preprocess Arabic text by removing diacritics and normalizing\n",
        "def preprocess_text(text):\n",
        "    # Remove diacritics\n",
        "    text = araby.strip_tashkeel(text)\n",
        "    # Normalize text (convert variations of Arabic letters to a standard form)\n",
        "    text = araby.normalize_ligature(text)\n",
        "    # Remove unwanted characters (punctuation in this case)\n",
        "    text = remove_punctuation(text) # call remove_punctuation instead of araby.strip_punctuation\n",
        "    return text\n",
        "\n",
        "# Example: Apply preprocessing to the dataset\n",
        "def preprocess_dataset(dataset):\n",
        "    return [preprocess_text(verse) for verse in dataset]\n",
        "\n",
        "# Example datasets (replace with actual dataset loading code)\n",
        "dataset = [\"الحب أسمى من كل شيء\", \"في السماء طيور\"]\n",
        "preprocessed_data = preprocess_dataset(dataset)\n",
        "\n",
        "# Load pre-trained AraGPT-2 model and tokenizer\n",
        "model_name = 'aubmindlab/aragpt2-base'  # Example pre-trained Arabic GPT-2 model\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Add padding token if needed\n",
        "tokenizer.add_tokens(['[PAD]'])\n",
        "tokenizer.pad_token = '[PAD]'\n",
        "\n",
        "# Tokenize all texts in preprocessed_data and make a dataset.\n",
        "# This time the tokenizer will be used correctly.\n",
        "tokenized_data = [tokenizer(text, padding='max_length', truncation=True, max_length=33, return_tensors='pt') for text in preprocessed_data]\n",
        "\n",
        "# Add labels to the tokenized_data.\n",
        "# The labels will be the same as the input_ids.\n",
        "for item in tokenized_data:\n",
        "    item[\"labels\"] = item[\"input_ids\"].clone()\n",
        "\n",
        "# Create dummy eval_dataset\n",
        "tokenized_eval_data = [tokenizer(text, padding='max_length', truncation=True, max_length=33, return_tensors='pt') for text in [\"نص للتقييم\"]]\n",
        "for item in tokenized_eval_data:\n",
        "    item[\"labels\"] = item[\"input_ids\"].clone()\n",
        "\n",
        "# Create datasets\n",
        "# this time the datasets will be correct.\n",
        "train_dataset = Dataset.from_list([item for item in tokenized_data])\n",
        "eval_dataset = Dataset.from_list([item for item in tokenized_eval_data])\n",
        "\n",
        "# Define the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory for model predictions\n",
        "    evaluation_strategy=\"epoch\",     # Evaluation strategy\n",
        "    per_device_train_batch_size=1,   # Batch size for training # set to 1 since they are different sizes\n",
        "    num_train_epochs=3,              # Number of epochs\n",
        "    logging_dir='./logs',            # Directory for logging\n",
        ")\n",
        "\n",
        "# Use DataCollatorForLanguageModeling\n",
        "# This will take care of the padding and creating the correct inputs for the model.\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=False\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the model being trained\n",
        "    args=training_args,                  # training arguments\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=eval_dataset, # add the eval_dataset\n",
        "    data_collator=data_collator # add the data_collator\n",
        ")\n",
        "\n",
        "# Start fine-tuning\n",
        "trainer.train()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "mlyilgiEDMFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Import the necessary modules\n",
        "from transformers import AutoModel, AutoTokenizer, GPT2LMHeadModel, GPT2Config, Trainer, TrainingArguments\n",
        "import pyarabic.araby as araby\n",
        "import string\n",
        "import torch\n",
        "from torch.nn import Linear, Embedding, Module\n",
        "from datasets import Dataset\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Helper function to remove punctuation\n",
        "def remove_punctuation(text):\n",
        "    # Create a translation table to remove punctuation\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    # Remove punctuation from the text\n",
        "    return text.translate(translator)\n",
        "\n",
        "# Preprocess Arabic text by removing diacritics and normalizing\n",
        "def preprocess_text(text):\n",
        "    # Remove diacritics\n",
        "    text = araby.strip_tashkeel(text)\n",
        "    # Normalize text (convert variations of Arabic letters to a standard form)\n",
        "    text = araby.normalize_ligature(text)\n",
        "    # Remove unwanted characters (punctuation in this case)\n",
        "    text = remove_punctuation(text)\n",
        "    return text\n",
        "\n",
        "# Example: Apply preprocessing to the dataset\n",
        "def preprocess_dataset(dataset):\n",
        "    return [preprocess_text(verse) for verse in dataset]\n",
        "\n",
        "# Example datasets (replace with actual dataset loading code)\n",
        "dataset = [\"الحب أسمى من كل شيء\", \"في السماء طيور\"]\n",
        "preprocessed_data = preprocess_dataset(dataset)\n",
        "\n",
        "# 1. Load AraBERT Tokenizer and Model\n",
        "arabert_tokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/bert-base-arabertv2\")\n",
        "arabert_model = AutoModel.from_pretrained(\"aubmindlab/bert-base-arabertv2\")\n",
        "\n",
        "# 2. Character-Level Tokenization and Subword Mapping\n",
        "def get_character_subword_mappings(text, arabert_tokenizer):\n",
        "    characters = list(text)\n",
        "    subword_tokens = []\n",
        "    subword_indices = []\n",
        "    for i, char in enumerate(characters):\n",
        "        char_tokens = arabert_tokenizer.tokenize(char)\n",
        "        subword_tokens.extend(char_tokens)\n",
        "        subword_indices.extend([i] * len(char_tokens))\n",
        "    return characters, subword_tokens, subword_indices\n",
        "\n",
        "# 3. Get Subword Embeddings\n",
        "def get_subword_embeddings(subword_tokens, arabert_tokenizer, arabert_model):\n",
        "    inputs = arabert_tokenizer(subword_tokens, is_split_into_words=True, return_tensors=\"pt\", padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = arabert_model(**inputs)\n",
        "    subword_embeddings = outputs.last_hidden_state\n",
        "    return subword_embeddings, inputs.attention_mask\n",
        "\n",
        "# 4. Aggregate Embeddings for Characters\n",
        "def aggregate_embeddings(subword_embeddings, subword_indices, attention_mask):\n",
        "    aggregated_embeddings = []\n",
        "    mask = []\n",
        "    unique_indices = sorted(list(set(subword_indices)))\n",
        "    for unique_idx in unique_indices:\n",
        "      idx_mask = [i for i, x in enumerate(subword_indices) if x == unique_idx]\n",
        "      char_embeddings = subword_embeddings[0][idx_mask] #get all the embeddings for that character.\n",
        "      char_mask = attention_mask[0][idx_mask] # get all the mask values for that character\n",
        "      mask_value = torch.max(char_mask) # if any of the mask values are 1 then set the mask value to 1.\n",
        "\n",
        "      if len(char_embeddings)>0:\n",
        "          aggregated_embeddings.append(torch.mean(char_embeddings, dim=0)) #get the mean of the characters embeddings.\n",
        "          mask.append(mask_value)\n",
        "    return torch.stack(aggregated_embeddings), mask\n",
        "\n",
        "# 5. Prepare Dataset\n",
        "def prepare_dataset(preprocessed_data, arabert_tokenizer, arabert_model):\n",
        "    data = []\n",
        "    max_len = 0\n",
        "    for text in preprocessed_data:\n",
        "        characters, subword_tokens, subword_indices = get_character_subword_mappings(text, arabert_tokenizer)\n",
        "        subword_embeddings, attention_mask = get_subword_embeddings(subword_tokens, arabert_tokenizer, arabert_model)\n",
        "        aggregated_embeddings, mask = aggregate_embeddings(subword_embeddings, subword_indices, attention_mask)\n",
        "        data.append({\"character_embeddings\": aggregated_embeddings, \"mask\": mask, \"text\":text})\n",
        "        max_len = max(max_len, len(aggregated_embeddings))\n",
        "\n",
        "    #padding\n",
        "    for item in data:\n",
        "      current_len = len(item[\"character_embeddings\"])\n",
        "      if current_len < max_len:\n",
        "        pad_length = max_len - current_len\n",
        "        padding_tensor = torch.full((pad_length, 768), 0)\n",
        "        item[\"character_embeddings\"] = torch.cat((item[\"character_embeddings\"], padding_tensor), dim=0)\n",
        "        item[\"mask\"] = item[\"mask\"] + [0]*pad_length #pad the mask with 0s.\n",
        "    return data\n",
        "\n",
        "prepared_data = prepare_dataset(preprocessed_data, arabert_tokenizer, arabert_model)\n",
        "\n",
        "# 6. Create a language model\n",
        "class CharacterLanguageModel(Module):\n",
        "    def __init__(self, hidden_size=256, num_layers=2, output_size=30, embedding_size=768): #output size is a random value.\n",
        "        super(CharacterLanguageModel, self).__init__()\n",
        "        self.lstm = torch.nn.LSTM(embedding_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.linear = Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        logits = self.linear(lstm_out)\n",
        "        return logits\n",
        "\n",
        "# Create a model, a dataloader and an optimizer.\n",
        "model = CharacterLanguageModel()\n",
        "data_loader = DataLoader(prepared_data, batch_size=1, shuffle=True) #set shuffle to True to shuffle the data.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "#training loop\n",
        "num_epochs = 10 #increase the number of epochs.\n",
        "for epoch in range(num_epochs):\n",
        "  epoch_loss = 0\n",
        "  for batch in data_loader:\n",
        "      input_embeddings = batch[\"character_embeddings\"]\n",
        "      input_masks = torch.tensor(batch[\"mask\"])\n",
        "      targets = torch.zeros_like(input_masks) #since we dont have real labels, we are using dummy labels.\n",
        "      targets = targets.type(torch.LongTensor)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      logits = model(input_embeddings)\n",
        "      loss = loss_function(logits.view(-1, logits.shape[-1]), targets.view(-1))\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      epoch_loss = epoch_loss+loss.item()\n",
        "\n",
        "  print(f\"loss for epoch: {epoch}, is : {epoch_loss}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383,
          "referenced_widgets": [
            "da586c70f1cc4092ba50878389c7da7a",
            "59ba1b5be73c4615a374a22752feb1a1",
            "8bf530ea1876409085e9aec05b5c4e11",
            "d495510bed73456fb411c6b199345191",
            "7f65862ef2cd4a14bed87c26f3b18d05",
            "50b489e946ce4d63a79d9fc208a61bde",
            "7813e4f4152641c1bb76c128e13a6330",
            "b5119c21e93c404690d4497f0c3a0b63",
            "f96f1b4e67304d41b4011cdfbff7e50b",
            "939806e41ce241c79bd0f2c4fba168f7",
            "e5b8474c6ab248a3a9e8f5a742c860e9",
            "f53e77eec3a742d78281befc7b85743c",
            "093ff339452842ff83b1712288f3c783",
            "efde7d9ef9a04bb2b31491bf329fc5b4",
            "3f2c0ba1c46746b2b19d97c026d04dae",
            "ea4ba858e6de48a2b2453ae11b880669",
            "1d2c479a75a54cd2b9e84c55bc50e063",
            "a5c012ccd2334397812d3b2a77c21b25",
            "449158312c5f4b34beba933100c2607a",
            "59a44963787b4397bccc51a297a525b3",
            "d64f12a81cc34041810d9aaf34848d6d",
            "ba04c83922bf497182fe60bfa75f7a90",
            "80edec71a73f41279af408376f8cec75",
            "0533d6036ed5461c955f06553f6f6ad1",
            "cfebfb9b3f98430495340ff0a4402c93",
            "00d1c11808634935ab3143adb7876952",
            "e7c58bb86dcd43d19ce0a21fc3a3e604",
            "433847186eae42f8bf9362ace0545a8c",
            "0e03af65c6ee49ff941399ee3d7e8227",
            "14545f98fe7a4e1aaa7665de2c02d445",
            "58eede6254294ced972f865a68d8e4f9",
            "656d9e539bef4a7e98db4fd5d86bbd71",
            "7810eebbded542e4b539bbfa0109d7cc",
            "3e26ec8c294a4a4aa766f535258180fc",
            "3df1b5b3cb404c148d5ec3bcb840f612",
            "f81900fa496c460b9edcf0a2cdf6a71d",
            "b4b479dabfec4b9eb9b7fc981802c3bc",
            "bb8d7703c5fd4a39977d27a8f3dbb177",
            "acebe47ada944e999f43550b4e33146c",
            "81eacdc6e2ea4d79b14f69376698ae29",
            "e8dbc4d87eda48fc96a8b13a6df0bebd",
            "c9ffa4b37f1141c9a5b134bb6e7fb5a8",
            "ec8c1053f57c4f07b7169f1a3c8c4649",
            "5926994a751c4e2c8cc4e132a58ca49d",
            "2851ddefde1c4c9698721a8f8e55e776",
            "3ed720300c9e4dbf9c1c73bd097c4196",
            "4c0b9430d1fb47c6b22bba2c8d289b1e",
            "cbd9eb0fdc9e44a4a8ebff17ce88eac6",
            "3d3dd752d51649e49546e512906ee8fc",
            "329b4dbfe28f4649849ad875b87ace9f",
            "2f6a8cd48237433eb86d08a79ecb07f2",
            "6841e45fbe2e40be817611479ef0e476",
            "8f8857b5ef844a4cb1a1738f34b87073",
            "42a167313b794281834dfab7e51439ed",
            "ecf665d8cb9e47a4924032e4df460829",
            "6818829bbb8846b3804c1ed222651a78",
            "19b3a4ce540c4b489be4d900296de908",
            "ded55a64dd4e4888ae9b2db75a2f0c0f",
            "6f1d4a17e50a412ba321c9bac2d95417",
            "575c94668abc4aeda0fa556a0d33886b",
            "a5c4e5e2ee114a6890f1bdfcb02530c6",
            "69b13ba54a394e9ca6a605e7fca6e130",
            "2b15b9c7fdfa4c77b8ef9d0e70765e0b",
            "79be6e7efab64db6bc8dbee753efe129",
            "af5ebc99b537478796f279f36faca1ac",
            "de28de86a18c44a78dcc6aa72ac2c339"
          ]
        },
        "id": "wmyesCMwFQUv",
        "outputId": "3e572eb1-dd30-4de2-8480-3d15d8007310"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/611 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "da586c70f1cc4092ba50878389c7da7a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/384 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f53e77eec3a742d78281befc7b85743c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/720k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "80edec71a73f41279af408376f8cec75"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.31M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3e26ec8c294a4a4aa766f535258180fc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2851ddefde1c4c9698721a8f8e55e776"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/543M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6818829bbb8846b3804c1ed222651a78"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss for epoch: 0, is : 6.487481117248535\n",
            "loss for epoch: 1, is : 4.3730340003967285\n",
            "loss for epoch: 2, is : 1.704083800315857\n",
            "loss for epoch: 3, is : 0.5469769984483719\n",
            "loss for epoch: 4, is : 0.24685414880514145\n",
            "loss for epoch: 5, is : 0.12779831513762474\n",
            "loss for epoch: 6, is : 0.06492139771580696\n",
            "loss for epoch: 7, is : 0.03208552487194538\n",
            "loss for epoch: 8, is : 0.016611756291240454\n",
            "loss for epoch: 9, is : 0.009362410753965378\n"
          ]
        }
      ]
    },
    {
      "source": [
        "# Import the necessary modules\n",
        "from transformers import AutoModel, AutoTokenizer, GPT2LMHeadModel, GPT2Config, Trainer, TrainingArguments\n",
        "import pyarabic.araby as araby\n",
        "import string\n",
        "import torch\n",
        "from torch.nn import Linear, Embedding, Module\n",
        "from datasets import Dataset\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Helper function to remove punctuation\n",
        "def remove_punctuation(text):\n",
        "    # Create a translation table to remove punctuation\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    # Remove punctuation from the text\n",
        "    return text.translate(translator)\n",
        "\n",
        "# Preprocess Arabic text by removing diacritics and normalizing\n",
        "def preprocess_text(text):\n",
        "    # Remove diacritics\n",
        "    text = araby.strip_tashkeel(text)\n",
        "    # Normalize text (convert variations of Arabic letters to a standard form)\n",
        "    text = araby.normalize_ligature(text)\n",
        "    # Remove unwanted characters (punctuation in this case)\n",
        "    text = remove_punctuation(text)\n",
        "    return text\n",
        "\n",
        "# Example: Apply preprocessing to the dataset\n",
        "def preprocess_dataset(dataset):\n",
        "    return [preprocess_text(verse) for verse in dataset]\n",
        "\n",
        "# Example datasets (replace with actual dataset loading code)\n",
        "dataset = [\"الحب أسمى من كل شيء\", \"في السماء طيور\"]\n",
        "preprocessed_data = preprocess_dataset(dataset)\n",
        "\n",
        "# 1. Load AraBERT Tokenizer and Model\n",
        "arabert_tokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/bert-base-arabertv2\")\n",
        "arabert_model = AutoModel.from_pretrained(\"aubmindlab/bert-base-arabertv2\")\n",
        "\n",
        "# 2. Character-Level Tokenization and Subword Mapping\n",
        "def get_character_subword_mappings(text, arabert_tokenizer):\n",
        "    characters = list(text)\n",
        "    subword_tokens = []\n",
        "    subword_indices = []\n",
        "    for i, char in enumerate(characters):\n",
        "        char_tokens = arabert_tokenizer.tokenize(char)\n",
        "        subword_tokens.extend(char_tokens)\n",
        "        subword_indices.extend([i] * len(char_tokens))\n",
        "    return characters, subword_tokens, subword_indices\n",
        "\n",
        "# 3. Get Subword Embeddings\n",
        "def get_subword_embeddings(subword_tokens, arabert_tokenizer, arabert_model):\n",
        "    inputs = arabert_tokenizer(subword_tokens, is_split_into_words=True, return_tensors=\"pt\", padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = arabert_model(**inputs)\n",
        "    subword_embeddings = outputs.last_hidden_state\n",
        "    return subword_embeddings, inputs.attention_mask\n",
        "\n",
        "# 4. Aggregate Embeddings for Characters\n",
        "def aggregate_embeddings(subword_embeddings, subword_indices, attention_mask):\n",
        "    aggregated_embeddings = []\n",
        "    mask = []\n",
        "    unique_indices = sorted(list(set(subword_indices)))\n",
        "    for unique_idx in unique_indices:\n",
        "      idx_mask = [i for i, x in enumerate(subword_indices) if x == unique_idx]\n",
        "      char_embeddings = subword_embeddings[0][idx_mask] #get all the embeddings for that character.\n",
        "      char_mask = attention_mask[0][idx_mask] # get all the mask values for that character\n",
        "      mask_value = torch.max(char_mask) # if any of the mask values are 1 then set the mask value to 1.\n",
        "\n",
        "      if len(char_embeddings)>0:\n",
        "          aggregated_embeddings.append(torch.mean(char_embeddings, dim=0)) #get the mean of the characters embeddings.\n",
        "          mask.append(mask_value)\n",
        "    return torch.stack(aggregated_embeddings), mask\n",
        "\n",
        "# 5. Prepare Dataset\n",
        "def prepare_dataset(preprocessed_data, arabert_tokenizer, arabert_model):\n",
        "    data = []\n",
        "    max_len = 0\n",
        "    for text in preprocessed_data:\n",
        "        characters, subword_tokens, subword_indices = get_character_subword_mappings(text, arabert_tokenizer)\n",
        "        subword_embeddings, attention_mask = get_subword_embeddings(subword_tokens, arabert_tokenizer, arabert_model)\n",
        "        aggregated_embeddings, mask = aggregate_embeddings(subword_embeddings, subword_indices, attention_mask)\n",
        "        data.append({\"character_embeddings\": aggregated_embeddings, \"mask\": mask, \"text\":text})\n",
        "        max_len = max(max_len, len(aggregated_embeddings))\n",
        "\n",
        "    #padding\n",
        "    for item in data:\n",
        "      current_len = len(item[\"character_embeddings\"])\n",
        "      if current_len < max_len:\n",
        "        pad_length = max_len - current_len\n",
        "        padding_tensor = torch.full((pad_length, 768), 0)\n",
        "        item[\"character_embeddings\"] = torch.cat((item[\"character_embeddings\"], padding_tensor), dim=0)\n",
        "        item[\"mask\"] = item[\"mask\"] + [0]*pad_length #pad the mask with 0s.\n",
        "    return data\n",
        "\n",
        "prepared_data = prepare_dataset(preprocessed_data, arabert_tokenizer, arabert_model)\n",
        "\n",
        "# 6. Create a language model\n",
        "class CharacterLanguageModel(Module):\n",
        "    def __init__(self, hidden_size=256, num_layers=2, output_size=30, embedding_size=768): #output size is a random value.\n",
        "        super(CharacterLanguageModel, self).__init__()\n",
        "        self.lstm = torch.nn.LSTM(embedding_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.linear = Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        logits = self.linear(lstm_out)\n",
        "        return logits\n",
        "\n",
        "# Create a model, a dataloader and an optimizer.\n",
        "model = CharacterLanguageModel()\n",
        "data_loader = DataLoader(prepared_data, batch_size=1, shuffle=True) #set shuffle to True to shuffle the data.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "#training loop\n",
        "num_epochs = 10 #increase the number of epochs.\n",
        "for epoch in range(num_epochs):\n",
        "  epoch_loss = 0\n",
        "  for batch in data_loader:\n",
        "      input_embeddings = batch[\"character_embeddings\"]\n",
        "      input_masks = torch.tensor(batch[\"mask\"])\n",
        "      targets = torch.zeros_like(input_masks) #since we dont have real labels, we are using dummy labels.\n",
        "      targets = targets.type(torch.LongTensor)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      logits = model(input_embeddings)\n",
        "      loss = loss_function(logits.view(-1, logits.shape[-1]), targets.view(-1))\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      epoch_loss = epoch_loss+loss.item()\n",
        "\n",
        "  print(f\"loss for epoch: {epoch}, is : {epoch_loss}\")\n",
        "\n",
        "# Function to prepare input text for the model\n",
        "def prepare_input_text(text, arabert_tokenizer, arabert_model):\n",
        "    characters, subword_tokens, subword_indices = get_character_subword_mappings(text, arabert_tokenizer)\n",
        "    subword_embeddings, attention_mask = get_subword_embeddings(subword_tokens, arabert_tokenizer, arabert_model)\n",
        "    aggregated_embeddings, mask = aggregate_embeddings(subword_embeddings, subword_indices, attention_mask)\n",
        "\n",
        "    # Padding the input\n",
        "    max_len = 0\n",
        "    for item in prepared_data: #use the data from the training set to find the max length\n",
        "        max_len = max(max_len, len(item[\"character_embeddings\"]))\n",
        "\n",
        "    current_len = len(aggregated_embeddings)\n",
        "    if current_len < max_len:\n",
        "      pad_length = max_len - current_len\n",
        "      padding_tensor = torch.full((pad_length, 768), 0)\n",
        "      aggregated_embeddings = torch.cat((aggregated_embeddings, padding_tensor), dim=0)\n",
        "\n",
        "    return aggregated_embeddings.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "# Function to generate text (or get predictions)\n",
        "def generate_text(model, input_text, arabert_tokenizer, arabert_model):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    input_tensor = prepare_input_text(input_text, arabert_tokenizer, arabert_model)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_tensor)\n",
        "\n",
        "    # Process logits\n",
        "    # here we get the sequence of numbers that the model has generated.\n",
        "    # these numbers have no meaning since we used dummy labels.\n",
        "    predicted_sequence = torch.argmax(logits, dim=-1).squeeze().tolist()\n",
        "\n",
        "    return predicted_sequence\n",
        "\n",
        "# Example usage:\n",
        "input_text = \"  الحب حيث المعشر الاعداء\"  # Example input text\n",
        "predicted_sequence = generate_text(model, input_text, arabert_tokenizer, arabert_model)\n",
        "print(f\"Input Text: {input_text}\")\n",
        "print(f\"Predicted Sequence: {predicted_sequence}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yn20ZjAHGDju",
        "outputId": "02d4c71d-b36b-4a24-9866-9c0b2f284df0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss for epoch: 0, is : 6.530747890472412\n",
            "loss for epoch: 1, is : 4.378942608833313\n",
            "loss for epoch: 2, is : 1.6696369647979736\n",
            "loss for epoch: 3, is : 0.5276787430047989\n",
            "loss for epoch: 4, is : 0.23746713995933533\n",
            "loss for epoch: 5, is : 0.1205633357167244\n",
            "loss for epoch: 6, is : 0.0584384948015213\n",
            "loss for epoch: 7, is : 0.027519521303474903\n",
            "loss for epoch: 8, is : 0.013590925838798285\n",
            "loss for epoch: 9, is : 0.007339492440223694\n",
            "Input Text: الحب\n",
            "Predicted Sequence: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"  الحب حيث المعشر الاعداء\"  # Example input text\n",
        "predicted_sequence = generate_text(model, input_text, arabert_tokenizer, arabert_model)\n",
        "print(f\"Input Text: {input_text}\")\n",
        "print(f\"Predicted Sequence: {predicted_sequence}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5xzOF2zGMFE",
        "outputId": "091f4c7c-3780-4d51-96a7-d625f439cec6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Text:   الحب حيث المعشر الاعداء\n",
            "Predicted Sequence: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m2Dhh6fEGMfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Install necessary libraries\n",
        "!pip install transformers torch pyarabic datasets pandas"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5J7N6SRLd6t",
        "outputId": "30931596-ecec-4eb4-bee5-a9d45a0a715e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: pyarabic in /usr/local/lib/python3.11/dist-packages (0.6.15)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from pyarabic) (1.17.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xy9gxUnmLezx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Import the necessary modules\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import pyarabic.araby as araby\n",
        "import string\n",
        "import torch\n",
        "from torch.nn import Linear, Module\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.optim import Adam\n",
        "import random\n",
        "from datasets import load_dataset #import load_dataset\n",
        "import pandas as pd #import pandas\n",
        "\n",
        "# --- Helper Functions ---\n",
        "\n",
        "# Function to remove punctuation\n",
        "def remove_punctuation(text):\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    return text.translate(translator)\n",
        "\n",
        "# Preprocess Arabic text\n",
        "def preprocess_text(text):\n",
        "    text = araby.strip_tashkeel(text)  # Remove diacritics\n",
        "    text = araby.normalize_ligature(text)  # Normalize ligatures\n",
        "    text = remove_punctuation(text)  # Remove punctuation\n",
        "    return text\n",
        "\n",
        "# --- Dataset Loading and Selection ---\n",
        "\n",
        "def load_and_select_dataset(dataset_name, num_verses=50000):\n",
        "    \"\"\"\n",
        "    Loads the dataset from Hugging Face or Mendeley, checks for 'sadr' and 'ajz' structure,\n",
        "    and selects a random subset of verses.\n",
        "\n",
        "    Args:\n",
        "        dataset_name (str): Name of the dataset to load ('ashaar' or path to csv file).\n",
        "        num_verses (int): Number of random verses to select.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of (sadr, ajz) pairs or None if neither dataset is suitable.\n",
        "    \"\"\"\n",
        "    if dataset_name == 'ashaar':\n",
        "        try:\n",
        "            # Load the ashaar dataset from Hugging Face\n",
        "            dataset = load_dataset('arbml/ashaar')\n",
        "\n",
        "            #get the list of all the baits.\n",
        "            baits = dataset['train']['peom verses']\n",
        "\n",
        "            #we need to split each bait into sadr and ajz.\n",
        "            verses = []\n",
        "            for bait in baits:\n",
        "                parts = bait.split(\" # \")\n",
        "                if len(parts) == 2: #check that it has both parts.\n",
        "                  verses.append((parts[0], parts[1]))\n",
        "\n",
        "            print(f\"Dataset loaded successfully from {dataset_name} and contains separate sadr and ajz.\")\n",
        "            # Randomly select verses\n",
        "            if len(verses) >= num_verses:\n",
        "                selected_verses = random.sample(verses, num_verses)\n",
        "            else:\n",
        "                selected_verses = verses  # Not enough verses, take all\n",
        "            return selected_verses\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading or processing dataset from {dataset_name}: {e}\")\n",
        "            return None\n",
        "\n",
        "    elif dataset_name.endswith('.csv'):\n",
        "        try:\n",
        "            # Load the dataset from a local CSV file using pandas\n",
        "            df = pd.read_csv(dataset_name)\n",
        "\n",
        "            # Check if the dataframe has the necessary columns\n",
        "            if 'sadr' in df.columns and 'ajz' in df.columns:\n",
        "                verses = list(zip(df['sadr'], df['ajz']))\n",
        "                print(f\"Dataset loaded successfully from {dataset_name} and contains separate sadr and ajz.\")\n",
        "\n",
        "                # Randomly select verses\n",
        "                if len(verses) >= num_verses:\n",
        "                    selected_verses = random.sample(verses, num_verses)\n",
        "                else:\n",
        "                    selected_verses = verses  # Not enough verses, take all\n",
        "                return selected_verses\n",
        "            else:\n",
        "                print(f\"Dataset from {dataset_name} does not contain 'sadr' and 'ajz' columns.\")\n",
        "                return None\n",
        "        except FileNotFoundError:\n",
        "            print(f\"File not found: {dataset_name}. Please check the filepath.\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading or processing dataset from {dataset_name}: {e}\")\n",
        "            return None\n",
        "    else:\n",
        "        print(f\"Invalid dataset name or file path: {dataset_name}\")\n",
        "        return None\n",
        "\n",
        "# --- Dataset Loading and Preprocessing ---\n",
        "\n",
        "selected_dataset = load_and_select_dataset('ashaar', num_verses=50000) # try with ashaar first\n",
        "\n",
        "# If the first dataset was not suitable, load and select from the second dataset\n",
        "if selected_dataset is None:\n",
        "    print(\"First dataset not suitable. Attempting to use the second dataset.\")\n",
        "    pcd_dataset_path = \"mcj6vkg6zw_cleaned_fragmented_dataset.csv\"  # Replace with your file path\n",
        "    selected_dataset = load_and_select_dataset(pcd_dataset_path, num_verses=50000)\n",
        "\n",
        "if selected_dataset is None:\n",
        "    print(\"Both datasets were not suitable. Exiting.\")\n",
        "    exit()\n",
        "\n",
        "# --- Dataset Preparation ---\n",
        "\n",
        "def filter_incomplete_verses(dataset):\n",
        "    \"\"\"Filters out incomplete verses (missing lines).\"\"\"\n",
        "    filtered_dataset = [verse for verse in dataset if len(verse) == 2]\n",
        "    return filtered_dataset\n",
        "\n",
        "def normalize_dataset(dataset):\n",
        "    \"\"\"Normalizes text in the dataset.\"\"\"\n",
        "    normalized_dataset = []\n",
        "    for sadr, ajz in dataset:\n",
        "        normalized_sadr = preprocess_text(sadr)\n",
        "        normalized_ajz = preprocess_text(ajz)\n",
        "        normalized_dataset.append((normalized_sadr, normalized_ajz))\n",
        "    return normalized_dataset\n",
        "\n",
        "filtered_dataset = filter_incomplete_verses(selected_dataset)\n",
        "normalized_dataset = normalize_dataset(filtered_dataset)\n",
        "\n",
        "# Create a new list to hold all the lines of the dataset.\n",
        "all_lines = []\n",
        "for sadr,ajz in normalized_dataset:\n",
        "  all_lines.append(sadr)\n",
        "  all_lines.append(ajz)\n",
        "\n",
        "# Create a character vocabulary.\n",
        "char_vocab = sorted(list(set(\"\".join(all_lines))))\n",
        "char_to_idx = {char: idx for idx, char in enumerate(char_vocab)}\n",
        "idx_to_char = {idx: char for idx, char in enumerate(char_vocab)}\n",
        "\n",
        "# --- AraBERT Setup ---\n",
        "arabert_tokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/bert-base-arabertv2\")\n",
        "arabert_model = AutoModel.from_pretrained(\"aubmindlab/bert-base-arabertv2\")\n",
        "\n",
        "# --- AraBERT helper functions ---\n",
        "def get_character_subword_mappings(text, arabert_tokenizer):\n",
        "    characters = list(text)\n",
        "    subword_tokens = []\n",
        "    subword_indices = []\n",
        "    for i, char in enumerate(characters):\n",
        "        char_tokens = arabert_tokenizer.tokenize(char)\n",
        "        subword_tokens.extend(char_tokens)\n",
        "        subword_indices.extend([i] * len(char_tokens))\n",
        "    return characters, subword_tokens, subword_indices\n",
        "\n",
        "def get_subword_embeddings(subword_tokens, arabert_tokenizer, arabert_model):\n",
        "    inputs = arabert_tokenizer(subword_tokens, is_split_into_words=True, return_tensors=\"pt\", padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = arabert_model(**inputs)\n",
        "    subword_embeddings = outputs.last_hidden_state\n",
        "    return subword_embeddings, inputs.attention_mask\n",
        "\n",
        "def aggregate_embeddings(subword_embeddings, subword_indices, attention_mask):\n",
        "    aggregated_embeddings = []\n",
        "    mask = []\n",
        "    unique_indices = sorted(list(set(subword_indices)))\n",
        "    for unique_idx in unique_indices:\n",
        "      idx_mask = [i for i, x in enumerate(subword_indices) if x == unique_idx]\n",
        "      char_embeddings = subword_embeddings[0][idx_mask] #get all the embeddings for that character.\n",
        "      char_mask = attention_mask[0][idx_mask] # get all the mask values for that character\n",
        "      mask_value = torch.max(char_mask) # if any of the mask values are 1 then set the mask value to 1.\n",
        "\n",
        "      if len(char_embeddings)>0:\n",
        "          aggregated_embeddings.append(torch.mean(char_embeddings, dim=0)) #get the mean of the characters embeddings.\n",
        "          mask.append(mask_value)\n",
        "    return torch.stack(aggregated_embeddings), mask\n",
        "\n",
        "# --- Dataset Preparation ---\n",
        "def prepare_dataset(dataset, arabert_tokenizer, arabert_model, char_to_idx):\n",
        "    data = []\n",
        "    max_len = 0\n",
        "    for sadr, ajz in dataset:\n",
        "        #prepare the sadr\n",
        "        characters, subword_tokens, subword_indices = get_character_subword_mappings(sadr, arabert_tokenizer)\n",
        "        subword_embeddings, attention_mask = get_subword_embeddings(subword_tokens, arabert_tokenizer, arabert_model)\n",
        "        aggregated_embeddings, mask = aggregate_embeddings(subword_embeddings, subword_indices, attention_mask)\n",
        "        #convert the characters into indexes.\n",
        "        sadr_labels = torch.tensor([char_to_idx[char] for char in sadr])\n",
        "        data.append({\"character_embeddings\": aggregated_embeddings, \"mask\": mask, \"labels\":sadr_labels, \"text\":sadr})\n",
        "        max_len = max(max_len, len(aggregated_embeddings))\n",
        "\n",
        "        #prepare the ajz\n",
        "        characters, subword_tokens, subword_indices = get_character_subword_mappings(ajz, arabert_tokenizer)\n",
        "        subword_embeddings, attention_mask = get_subword_embeddings(subword_tokens, arabert_tokenizer, arabert_model)\n",
        "        aggregated_embeddings, mask = aggregate_embeddings(subword_embeddings, subword_indices, attention_mask)\n",
        "        #convert the characters into indexes.\n",
        "        ajz_labels = torch.tensor([char_to_idx[char] for char in ajz])\n",
        "        data.append({\"character_embeddings\": aggregated_embeddings, \"mask\": mask, \"labels\":ajz_labels, \"text\":ajz})\n",
        "        max_len = max(max_len, len(aggregated_embeddings))\n",
        "\n",
        "\n",
        "    #padding\n",
        "    for item in data:\n",
        "      current_len = len(item[\"character_embeddings\"])\n",
        "      if current_len < max_len:\n",
        "        pad_length = max_len - current_len\n",
        "        padding_tensor = torch.full((pad_length, 768), 0)\n",
        "        item[\"character_embeddings\"] = torch.cat((item[\"character_embeddings\"], padding_tensor), dim=0)\n",
        "        item[\"mask\"] = item[\"mask\"] + [0]*pad_length #pad the mask with 0s.\n",
        "    return data\n",
        "\n",
        "prepared_dataset = prepare_dataset(normalized_dataset, arabert_tokenizer, arabert_model, char_to_idx)\n",
        "\n",
        "# --- Data Loader ---\n",
        "class PoetryDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        return {\n",
        "            \"character_embeddings\": item[\"character_embeddings\"],\n",
        "            \"mask\": torch.tensor(item[\"mask\"]),\n",
        "            \"labels\": item[\"labels\"],\n",
        "        }\n",
        "dataset = PoetryDataset(prepared_dataset)\n",
        "data_loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "# --- Model Definition ---\n",
        "class CharacterLanguageModel(Module):\n",
        "    def __init__(self, hidden_size=256, num_layers=2, output_size=len(char_vocab), embedding_size=768):\n",
        "        super(CharacterLanguageModel, self).__init__()\n",
        "        self.lstm = torch.nn.LSTM(embedding_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.linear = Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        logits = self.linear(lstm_out)\n",
        "        return logits\n",
        "\n",
        "# --- Model and Training ---\n",
        "model = CharacterLanguageModel()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# --- Training Loop ---\n",
        "num_epochs = 10 #increase the number of epochs.\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0\n",
        "    for batch in data_loader:\n",
        "        input_embeddings = batch[\"character_embeddings\"].unsqueeze(0)\n",
        "        input_masks = batch[\"mask\"].unsqueeze(0)\n",
        "        targets = batch[\"labels\"].unsqueeze(0)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(input_embeddings)\n",
        "\n",
        "        # Pad targets to match the logits shape\n",
        "        targets_padded = torch.nn.functional.pad(targets, (0, logits.size(1) - targets.size(1)), 'constant', 0)\n",
        "\n",
        "        loss = loss_function(logits.view(-1, logits.shape[-1]), targets_padded.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss = epoch_loss + loss.item()\n",
        "\n",
        "    print(f\"loss for epoch: {epoch}, is : {epoch_loss}\")\n",
        "\n",
        "# --- Inference ---\n",
        "# Function to prepare input text for the model\n",
        "def prepare_input_text(text, arabert_tokenizer, arabert_model, char_to_idx):\n",
        "    characters, subword_tokens, subword_indices = get_character_subword_mappings(text, arabert_tokenizer)\n",
        "    subword_embeddings, attention_mask = get_subword_embeddings(subword_tokens, arabert_tokenizer, arabert_model)\n",
        "    aggregated_embeddings, mask = aggregate_embeddings(subword_embeddings, subword_indices, attention_mask)\n",
        "\n",
        "    # Padding the input\n",
        "    max_len = 0\n",
        "    for item in prepared_dataset: #use the data from the training set to find the max length\n",
        "        max_len = max(max_len, len(item[\"character_embeddings\"]))\n",
        "\n",
        "    current_len = len(aggregated_embeddings)\n",
        "    if current_len < max_len:\n",
        "      pad_length = max_len - current_len\n",
        "      padding_tensor = torch.full((pad_length, 768), 0)\n",
        "      aggregated_embeddings = torch.cat((aggregated_embeddings, padding_tensor), dim=0)\n",
        "\n",
        "    return aggregated_embeddings.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "# Function to generate text (or get predictions)\n",
        "def generate_text(model, input_text, arabert_tokenizer, arabert_model, char_to_idx, idx_to_char):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    input_tensor = prepare_input_text(input_text, arabert_tokenizer, arabert_model, char_to_idx)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_tensor)\n",
        "\n",
        "    # Process logits\n",
        "    # here we get the sequence of numbers that the model has generated.\n",
        "    # the numbers will be the indices of the characters in the vocabulary.\n",
        "    predicted_sequence = torch.argmax(logits, dim=-1).squeeze().tolist()\n",
        "    characters = [idx_to_char[idx] for idx in predicted_sequence]\n",
        "    return \"\".join(characters)\n",
        "\n",
        "# Example usage:\n",
        "input_text = \"الحب\"  # Example input text\n",
        "predicted_sequence = generate_text(model, input_text, arabert_tokenizer, arabert_model, char_to_idx, idx_to_char)\n",
        "print(f\"Input Text: {input_text}\")\n",
        "print(f\"Predicted Sequence: {predicted_sequence}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 525
        },
        "id": "BY9gYsIRLwwS",
        "outputId": "85d410ee-b667-4014-d915-070fea57c6f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error loading or processing dataset from ashaar: \"Column peom verses not in the dataset. Current columns in the dataset: ['poem title', 'poem meter', 'poem verses', 'poem theme', 'poem url', 'poet name', 'poet description', 'poet url', 'poet era', 'poet location', 'poem description', 'poem language type']\"\n",
            "First dataset not suitable. Attempting to use the second dataset.\n",
            "File not found: mcj6vkg6zw_cleaned_fragmented_dataset.csv. Please check the filepath.\n",
            "Both datasets were not suitable. Exiting.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'NoneType' object is not iterable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ae9ebb2ac391>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnormalized_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m \u001b[0mfiltered_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_incomplete_verses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0mnormalized_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-ae9ebb2ac391>\u001b[0m in \u001b[0;36mfilter_incomplete_verses\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfilter_incomplete_verses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;34m\"\"\"Filters out incomplete verses (missing lines).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0mfiltered_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mverse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mverse\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfiltered_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
          ]
        }
      ]
    },
    {
      "source": [
        "# Import the necessary modules\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import pyarabic.araby as araby\n",
        "import string\n",
        "import torch\n",
        "from torch.nn import Linear, Module\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.optim import Adam\n",
        "import random\n",
        "from datasets import load_dataset #import load_dataset\n",
        "import pandas as pd #import pandas\n",
        "import requests #import requests\n",
        "\n",
        "# --- Helper Functions ---\n",
        "\n",
        "# Function to remove punctuation\n",
        "def remove_punctuation(text):\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    return text.translate(translator)\n",
        "\n",
        "# Preprocess Arabic text\n",
        "def preprocess_text(text):\n",
        "    text = araby.strip_tashkeel(text)  # Remove diacritics\n",
        "    text = araby.normalize_ligature(text)  # Normalize ligatures\n",
        "    text = remove_punctuation(text)  # Remove punctuation\n",
        "    return text\n",
        "\n",
        "# --- Dataset Loading and Selection ---\n",
        "\n",
        "def load_and_select_dataset(dataset_name, num_verses=50000):\n",
        "    \"\"\"\n",
        "    Loads the dataset from Hugging Face or Mendeley, checks for 'sadr' and 'ajz' structure,\n",
        "    and selects a random subset of verses.\n",
        "\n",
        "    Args:\n",
        "        dataset_name (str): Name of the dataset to load ('ashaar' or path to csv file).\n",
        "        num_verses (int): Number of random verses to select.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of (sadr, ajz) pairs or None if neither dataset is suitable.\n",
        "    \"\"\"\n",
        "    if dataset_name == 'ashaar':\n",
        "        try:\n",
        "            # Load the ashaar dataset from Hugging Face\n",
        "            dataset = load_dataset('arbml/ashaar')\n",
        "\n",
        "            #get the list of all the verses.\n",
        "            verses = dataset['train']['poem verses']\n",
        "\n",
        "            #we need to split each bait into sadr and ajz.\n",
        "            verses_split = []\n",
        "            for verse in verses:\n",
        "                parts = verse.split(\" # \")\n",
        "                if len(parts) == 2: #check that it has both parts.\n",
        "                  verses_split.append((parts[0], parts[1]))\n",
        "                else:\n",
        "                  verses_split.append((parts[0], \"\")) #if it does not have two parts, then ajz will be empty.\n",
        "\n",
        "            print(f\"Dataset loaded successfully from {dataset_name}.\")\n",
        "            # Randomly select verses\n",
        "            if len(verses_split) >= num_verses:\n",
        "                selected_verses = random.sample(verses_split, num_verses)\n",
        "            else:\n",
        "                selected_verses = verses_split  # Not enough verses, take all\n",
        "            return selected_verses\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading or processing dataset from {dataset_name}: {e}\")\n",
        "            return None\n",
        "\n",
        "    elif dataset_name.endswith('.csv'):\n",
        "        try:\n",
        "            # Load the dataset from a local CSV file using pandas\n",
        "            df = pd.read_csv(dataset_name)\n",
        "\n",
        "            # Check if the dataframe has the necessary columns\n",
        "            if 'verse' in df.columns:\n",
        "                verses = df['verse'].tolist()\n",
        "                verses_split = []\n",
        "                for verse in verses:\n",
        "                    parts = verse.split(\" # \")\n",
        "                    if len(parts) == 2: #check that it has both parts.\n",
        "                        verses_split.append((parts[0], parts[1]))\n",
        "                    else:\n",
        "                        verses_split.append((parts[0], \"\")) #if it does not have two parts, then ajz will be empty.\n",
        "\n",
        "                print(f\"Dataset loaded successfully from {dataset_name}.\")\n",
        "\n",
        "                # Randomly select verses\n",
        "                if len(verses_split) >= num_verses:\n",
        "                    selected_verses = random.sample(verses_split, num_verses)\n",
        "                else:\n",
        "                    selected_verses = verses_split  # Not enough verses, take all\n",
        "                return selected_verses\n",
        "            else:\n",
        "                print(f\"Dataset from {dataset_name} does not contain 'verse' columns.\")\n",
        "                return None\n",
        "        except FileNotFoundError:\n",
        "            print(f\"File not found: {dataset_name}. Please check the filepath.\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading or processing dataset from {dataset_name}: {e}\")\n",
        "            return None\n",
        "    else:\n",
        "        print(f\"Invalid dataset name or file path: {dataset_name}\")\n",
        "        return None\n",
        "\n",
        "def select_dataset():\n",
        "    \"\"\"\n",
        "    Selects and loads a dataset, trying 'ashaar' first and then 'pcd_dataset_path' if necessary.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of (sadr, ajz) pairs or None if neither dataset is suitable.\n",
        "    \"\"\"\n",
        "    selected_dataset = load_and_select_dataset('ashaar', num_verses=50000) # try with ashaar first\n",
        "\n",
        "    # If the first dataset was not suitable, load and select from the second dataset\n",
        "    if selected_dataset is None:\n",
        "        print(\"First dataset not suitable. Attempting to use the second dataset.\")\n",
        "\n",
        "        # Download the dataset from the URL\n",
        "        url = \"https://raw.githubusercontent.com/arbml/poetry-corpus/main/mcj6vkg6zw_cleaned_fragmented_dataset.csv\"\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            with open(\"mcj6vkg6zw_cleaned_fragmented_dataset.csv\", \"wb\") as f:\n",
        "                f.write(response.content)\n",
        "        else:\n",
        "            print(f\"Failed to download the file. Status code: {response.status_code}\")\n",
        "            return None\n",
        "\n",
        "        pcd_dataset_path = \"mcj6vkg6zw_cleaned_fragmented_dataset.csv\"\n",
        "        selected_dataset = load_and_select_dataset(pcd_dataset_path, num_verses=50000)\n",
        "\n",
        "    if selected_dataset is None:\n",
        "        print(\"Both datasets were not suitable.\")\n",
        "\n",
        "    return selected_dataset\n",
        "\n",
        "# --- Dataset Loading and Preprocessing ---\n",
        "selected_dataset = select_dataset()\n",
        "\n",
        "if selected_dataset is None:\n",
        "    print(\"Exiting due to dataset loading failure.\")\n",
        "    exit()\n",
        "\n",
        "# --- Dataset Preparation ---\n",
        "\n",
        "def filter_incomplete_verses(dataset):\n",
        "    \"\"\"Filters out incomplete verses (missing lines).\"\"\"\n",
        "    # Check if the dataset is None before attempting to iterate over it.\n",
        "    if dataset is None:\n",
        "        print(\"Warning: dataset is None. Skipping filtering.\")\n",
        "        return []  # Return an empty list to avoid the TypeError\n",
        "\n",
        "    filtered_dataset = [verse for verse in dataset if len(verse) > 0] #at least one verse\n",
        "    return filtered_dataset\n",
        "\n",
        "def normalize_dataset(dataset):\n",
        "    \"\"\"Normalizes text in the dataset.\"\"\"\n",
        "    normalized_dataset = []\n",
        "    for sadr, ajz in dataset:\n",
        "        normalized_sadr = preprocess_text(sadr)\n",
        "        normalized_ajz = preprocess_text(ajz)\n",
        "        normalized_dataset.append((normalized_sadr, normalized_ajz))\n",
        "    return normalized_dataset\n",
        "\n",
        "filtered_dataset = filter_incomplete_verses(selected_dataset)\n",
        "normalized_dataset = normalize_dataset(filtered_dataset)\n",
        "\n",
        "# Create a new list to hold all the lines of the dataset.\n",
        "all_lines = []\n",
        "for sadr,ajz in normalized_dataset:\n",
        "  all_lines.append(sadr)\n",
        "  all_lines.append(ajz)\n",
        "\n",
        "# Create a character vocabulary.\n",
        "char_vocab = sorted(list(set(\"\".join(all_lines))))\n",
        "char_to_idx = {char: idx for idx, char in enumerate(char_vocab)}\n",
        "idx_to_char = {idx: char for idx, char in enumerate(char_vocab)}\n",
        "\n",
        "# --- AraBERT Setup ---\n",
        "arabert_tokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/bert-base-arabertv2\")\n",
        "arabert_model = AutoModel.from_pretrained(\"aubmindlab/bert-base-arabertv2\")\n",
        "\n",
        "# --- AraBERT helper functions ---\n",
        "def get_character_subword_mappings(text, arabert_tokenizer):\n",
        "    characters = list(text)\n",
        "    subword_tokens = []\n",
        "    subword_indices = []\n",
        "    for i, char in enumerate(characters):\n",
        "        char_tokens = arabert_tokenizer.tokenize(char)\n",
        "        subword_tokens.extend(char_tokens)\n",
        "        subword_indices.extend([i] * len(char_tokens))\n",
        "    return characters, subword_tokens, subword_indices\n",
        "\n",
        "def get_subword_embeddings(subword_tokens, arabert_tokenizer, arabert_model):\n",
        "    inputs = arabert_tokenizer(subword_tokens, is_split_into_words=True, return_tensors=\"pt\", padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = arabert_model(**inputs)\n",
        "    subword_embeddings = outputs.last_hidden_state\n",
        "    return subword_embeddings\n",
        "\n",
        "def aggregate_embeddings(subword_embeddings, subword_indices):\n",
        "    aggregated_embeddings = []\n",
        "    unique_indices = sorted(list(set(subword_indices)))\n",
        "    for unique_idx in unique_indices:\n",
        "      idx_mask = [i for i, x in enumerate(subword_indices) if x == unique_idx]\n",
        "      char_embeddings = subword_embeddings[0][idx_mask] #get all the embeddings for that character.\n",
        "      if len(char_embeddings)>0:\n",
        "          aggregated_embeddings.append(torch.mean(char_embeddings, dim=0)) #get the mean of the characters embeddings.\n",
        "    return torch.stack(aggregated_embeddings)\n",
        "\n",
        "# --- Dataset Preparation ---\n",
        "def prepare_dataset(dataset, arabert_tokenizer, arabert_model, char_to_idx):\n",
        "    data = []\n",
        "    max_len = 0\n",
        "    for sadr, ajz in dataset:\n",
        "        #prepare the sadr\n",
        "        characters, subword_tokens, subword_indices = get_character_subword_mappings(sadr, arabert_tokenizer)\n",
        "        subword_embeddings = get_subword_embeddings(subword_tokens, arabert_tokenizer, arabert_model)\n",
        "        aggregated_embeddings = aggregate_embeddings(subword_embeddings, subword_indices)\n",
        "        #convert the characters into indexes.\n",
        "        sadr_labels = torch.tensor([char_to_idx[char] for char in sadr])\n",
        "        data.append({\"character_embeddings\": aggregated_embeddings, \"labels\":sadr_labels, \"text\":sadr})\n",
        "        max_len = max(max_len, len(aggregated_embeddings))\n",
        "\n",
        "        #prepare the ajz\n",
        "        characters, subword_tokens, subword_indices = get_character_subword_mappings(ajz, arabert_tokenizer)\n",
        "        subword_embeddings = get_subword_embeddings(subword_tokens, arabert_tokenizer, arabert_model)\n",
        "        aggregated_embeddings = aggregate_embeddings(subword_embeddings, subword_indices)\n",
        "        #convert the characters into indexes.\n",
        "        ajz_labels = torch.tensor([char_to_idx[char] for char in ajz])\n",
        "        data.append({\"character_embeddings\": aggregated_embeddings, \"labels\":ajz_labels, \"text\":ajz})\n",
        "        max_len = max(max_len, len(aggregated_embeddings))\n",
        "\n",
        "\n",
        "    #padding\n",
        "    for item in data:\n",
        "      current_len = len(item[\"character_embeddings\"])\n",
        "      if current_len < max_len:\n",
        "        pad_length = max_len - current_len\n",
        "        padding_tensor = torch.full((pad_length, 768), 0)\n",
        "        item[\"character_embeddings\"] = torch.cat((item[\"character_embeddings\"], padding_tensor), dim=0)\n",
        "    return data\n",
        "\n",
        "prepared_dataset = prepare_dataset(normalized_dataset, arabert_tokenizer, arabert_model, char_to_idx)\n",
        "\n",
        "# --- Data Loader ---\n",
        "class PoetryDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        return {\n",
        "            \"character_embeddings\": item[\"character_embeddings\"],\n",
        "            \"labels\": item[\"labels\"],\n",
        "        }\n",
        "dataset = PoetryDataset(prepared_dataset)\n",
        "data_loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "# --- Model Definition ---\n",
        "class CharacterLanguageModel(Module):\n",
        "    def __init__(self, hidden_size=256, num_layers=2, output_size=len(char_vocab), embedding_size=768):\n",
        "        super(CharacterLanguageModel, self).__init__()\n",
        "        self.lstm = torch.nn.LSTM(embedding_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.linear = Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        logits = self.linear(lstm_out)\n",
        "        return logits\n",
        "\n",
        "# --- Model and Training ---\n",
        "model = CharacterLanguageModel()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# --- Training Loop ---\n",
        "num_epochs = 10 #increase the number of epochs.\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0\n",
        "    for batch in data_loader:\n",
        "        input_embeddings = batch[\"character_embeddings\"].unsqueeze(0)\n",
        "        targets = batch[\"labels\"].unsqueeze(0)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(input_embeddings)\n",
        "\n",
        "        # Pad targets to match the logits shape\n",
        "        # Find the maximum length between targets and logits\n",
        "        max_seq_length = max(targets.size(1), logits.size(1))\n",
        "\n",
        "        # Pad targets\n",
        "        targets_padded = torch.nn.functional.pad(targets, (0, max_seq_length - targets.size(1)), 'constant', 0)\n",
        "        loss = loss_function(logits.view(-1, logits.shape[-1]), targets_padded.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss = epoch_loss + loss.item()\n",
        "\n",
        "    print(f\"loss for epoch: {epoch}, is : {epoch_loss}\")\n",
        "\n",
        "# --- Inference ---\n",
        "\n",
        "# Helper function to process text for the model.\n",
        "def process_text(text, arabert_tokenizer, arabert_model):\n",
        "    characters, subword_tokens, subword_indices = get_character_subword_mappings(text, arabert_tokenizer)\n",
        "    subword_embeddings = get_subword_embeddings(subword_tokens, arabert_tokenizer, arabert_model)\n",
        "    aggregated_embeddings = aggregate_embeddings(subword_embeddings, subword_indices)\n",
        "    return aggregated_embeddings\n",
        "\n",
        "# Function to prepare input text for the model\n",
        "def prepare_input_text(text, arabert_tokenizer, arabert_model, char_to_idx, prepared_dataset):\n",
        "    aggregated_embeddings = process_text(text, arabert_tokenizer, arabert_model)\n",
        "\n",
        "    # Padding the input\n",
        "    max_len = 0\n",
        "    for item in prepared_dataset: #use the data from the training set to find the max length\n",
        "        max_len = max(max_len, len(item[\"character_embeddings\"]))\n",
        "\n",
        "    current_len = len(aggregated_embeddings)\n",
        "    if current_len < max_len:\n",
        "      pad_length = max_len - current_len\n",
        "      padding_tensor = torch.full((pad_length, 768), 0)\n",
        "      aggregated_embeddings = torch.cat((aggregated_embeddings, padding_tensor), dim=0)\n",
        "\n",
        "    return aggregated_embeddings.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "# Function to generate text (or get predictions)\n",
        "def generate_text(model, input_text, arabert_tokenizer, arabert_model, char_to_idx, idx_to_char, prepared_dataset):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    input_tensor = prepare_input_text(input_text, arabert_tokenizer, arabert_model, char_to_idx, prepared_dataset)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_tensor)\n",
        "\n",
        "    # Process logits\n",
        "    # here we get the sequence of numbers that the model has generated.\n",
        "    # the numbers will be the indices of the characters in the vocabulary.\n",
        "    predicted_sequence = torch.argmax(logits, dim=-1).squeeze().tolist()\n",
        "    characters = [idx_to_char[idx] for idx in predicted_sequence]\n",
        "    return \"\".join(characters)\n",
        "\n",
        "# Example usage:\n",
        "input_text = \"الحب\"  # Example input text\n",
        "predicted_sequence = generate_text(model, input_text, arabert_tokenizer, arabert_model, char_to_idx, idx_to_char, prepared_dataset)\n",
        "print(f\"Input Text: {input_text}\")\n",
        "print(f\"Predicted Sequence: {predicted_sequence}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "id": "4Vcfmu8ENCGH",
        "outputId": "548fa2a9-0de4-4107-a2bb-04911d08c342"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error loading or processing dataset from ashaar: 'list' object has no attribute 'split'\n",
            "First dataset not suitable. Attempting to use the second dataset.\n",
            "Failed to download the file. Status code: 404\n",
            "Exiting due to dataset loading failure.\n",
            "Warning: dataset is None. Skipping filtering.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "num_samples should be a positive integer value, but got num_samples=0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-608a7233b09e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    261\u001b[0m         }\n\u001b[1;32m    262\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPoetryDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepared_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m \u001b[0mdata_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;31m# --- Model Definition ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# map-style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    165\u001b[0m                 \u001b[0;34mf\"num_samples should be a positive integer value, but got num_samples={self.num_samples}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
          ]
        }
      ]
    },
    {
      "source": [
        "# Import the necessary modules\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import pyarabic.araby as araby\n",
        "import string\n",
        "import torch\n",
        "from torch.nn import Linear, Module\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.optim import Adam\n",
        "import random\n",
        "from datasets import load_dataset #import load_dataset\n",
        "import pandas as pd #import pandas\n",
        "import requests #import requests\n",
        "\n",
        "# --- Helper Functions ---\n",
        "\n",
        "# Function to remove punctuation\n",
        "def remove_punctuation(text):\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    return text.translate(translator)\n",
        "\n",
        "# Preprocess Arabic text\n",
        "def preprocess_text(text):\n",
        "    text = araby.strip_tashkeel(text)  # Remove diacritics\n",
        "    text = araby.normalize_ligature(text)  # Normalize ligatures\n",
        "    text = remove_punctuation(text)  # Remove punctuation\n",
        "    return text\n",
        "\n",
        "# --- Dataset Loading and Selection ---\n",
        "\n",
        "def load_and_select_dataset(dataset_name, num_verses=50000):\n",
        "    \"\"\"\n",
        "    Loads the dataset from Hugging Face or Mendeley, checks for 'sadr' and 'ajz' structure,\n",
        "    and selects a random subset of verses.\n",
        "\n",
        "    Args:\n",
        "        dataset_name (str): Name of the dataset to load ('ashaar' or path to csv file).\n",
        "        num_verses (int): Number of random verses to select.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of (sadr, ajz) pairs or None if neither dataset is suitable.\n",
        "    \"\"\"\n",
        "    if dataset_name == 'ashaar':\n",
        "        try:\n",
        "            # Load the ashaar dataset from Hugging Face\n",
        "            dataset = load_dataset('arbml/ashaar')\n",
        "\n",
        "            #get the list of all the verses.\n",
        "            verses = dataset['train']['bait'] # The key 'poem verses' does not exist.\n",
        "\n",
        "            #we need to split each bait into sadr and ajz.\n",
        "            verses_split = []\n",
        "            for verse in verses:\n",
        "                parts = verse.split(\" # \")\n",
        "                if len(parts) == 2: #check that it has both parts.\n",
        "                  verses_split.append((parts[0], parts[1]))\n",
        "                else:\n",
        "                  verses_split.append((parts[0], \"\")) #if it does not have two parts, then ajz will be empty.\n",
        "\n",
        "            print(f\"Dataset loaded successfully from {dataset_name}.\")\n",
        "\n",
        "            # Select up to num_verses\n",
        "            if len(verses_split) > num_verses:\n",
        "              selected_verses = random.sample(verses_split, num_verses)\n",
        "            else:\n",
        "              selected_verses = verses_split\n",
        "\n",
        "            return selected_verses\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading or processing dataset from {dataset_name}: {e}\")\n",
        "            return None\n",
        "\n",
        "    elif dataset_name.endswith('.csv'):\n",
        "        try:\n",
        "            # Load the dataset from a local CSV file using pandas\n",
        "            df = pd.read_csv(dataset_name)\n",
        "\n",
        "            # Check if the dataframe has the necessary columns\n",
        "            if 'verse' in df.columns:\n",
        "                verses = df['verse'].tolist()\n",
        "                verses_split = []\n",
        "                for verse in verses:\n",
        "                    parts = verse.split(\" # \")\n",
        "                    if len(parts) == 2: #check that it has both parts.\n",
        "                        verses_split.append((parts[0], parts[1]))\n",
        "                    else:\n",
        "                        verses_split.append((parts[0], \"\")) #if it does not have two parts, then ajz will be empty.\n",
        "\n",
        "                print(f\"Dataset loaded successfully from {dataset_name}.\")\n",
        "\n",
        "                # Select up to num_verses\n",
        "                if len(verses_split) > num_verses:\n",
        "                  selected_verses = random.sample(verses_split, num_verses)\n",
        "                else:\n",
        "                  selected_verses = verses_split\n",
        "                return selected_verses\n",
        "            else:\n",
        "                print(f\"Dataset from {dataset_name} does not contain 'verse' columns.\")\n",
        "                return None\n",
        "        except FileNotFoundError:\n",
        "            print(f\"File not found: {dataset_name}. Please check the filepath.\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading or processing dataset from {dataset_name}: {e}\")\n",
        "            return None\n",
        "    else:\n",
        "        print(f\"Invalid dataset name or file path: {dataset_name}\")\n",
        "        return None\n",
        "\n",
        "def select_dataset(num_verses=50000):\n",
        "    \"\"\"\n",
        "    Selects and loads a dataset, trying 'ashaar' first and then 'pcd_dataset_path' if necessary.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of (sadr, ajz) pairs or None if neither dataset is suitable.\n",
        "    \"\"\"\n",
        "    selected_dataset = load_and_select_dataset('ashaar', num_verses) # try with ashaar first\n",
        "\n",
        "    # If the first dataset was not suitable, load and select from the second dataset\n",
        "    if selected_dataset is None:\n",
        "        print(\"First dataset not suitable. Attempting to use the second dataset.\")\n",
        "\n",
        "        # Download the dataset from the URL\n",
        "        url = \"https://raw.githubusercontent.com/arbml/poetry-corpus/main/mcj6vkg6zw_cleaned_fragmented_dataset.csv\"\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            with open(\"mcj6vkg6zw_cleaned_fragmented_dataset.csv\", \"wb\") as f:\n",
        "                f.write(response.content)\n",
        "        else:\n",
        "            print(f\"Failed to download the file. Status code: {response.status_code}\")\n",
        "            return None\n",
        "\n",
        "        pcd_dataset_path = \"mcj6vkg6zw_cleaned_fragmented_dataset.csv\"\n",
        "        selected_dataset = load_and_select_dataset(pcd_dataset_path, num_verses)\n",
        "\n",
        "    if selected_dataset is None:\n",
        "        print(\"Both datasets were not suitable.\")\n",
        "\n",
        "    return selected_dataset\n",
        "\n",
        "# --- Dataset Loading and Preprocessing ---\n",
        "num_verses = 50000\n",
        "selected_dataset = select_dataset(num_verses)\n",
        "\n",
        "if selected_dataset is None:\n",
        "    print(\"Exiting due to dataset loading failure.\")\n",
        "    exit()\n",
        "\n",
        "# --- Dataset Preparation ---\n",
        "\n",
        "def filter_incomplete_verses(dataset):\n",
        "    \"\"\"Filters out incomplete verses (missing lines).\"\"\"\n",
        "    # Check if the dataset is None before attempting to iterate over it.\n",
        "    if dataset is None:\n",
        "        print(\"Warning: dataset is None. Skipping filtering.\")\n",
        "        return []  # Return an empty list to avoid the TypeError\n",
        "\n",
        "    filtered_dataset = [verse for verse in dataset if len(verse) > 0] #at least one verse\n",
        "    return filtered_dataset\n",
        "\n",
        "def normalize_dataset(dataset):\n",
        "    \"\"\"Normalizes text in the dataset.\"\"\"\n",
        "    normalized_dataset = []\n",
        "    for sadr, ajz in dataset:\n",
        "        normalized_sadr = preprocess_text(sadr)\n",
        "        normalized_ajz = preprocess_text(ajz)\n",
        "        normalized_dataset.append((normalized_sadr, normalized_ajz))\n",
        "    return normalized_dataset\n",
        "\n",
        "filtered_dataset = filter_incomplete_verses(selected_dataset)\n",
        "normalized_dataset = normalize_dataset(filtered_dataset)\n",
        "\n",
        "# Create a new list to hold all the lines of the dataset.\n",
        "all_lines = []\n",
        "for sadr,ajz in normalized_dataset:\n",
        "  all_lines.append(sadr)\n",
        "  all_lines.append(ajz)\n",
        "\n",
        "# Create a character vocabulary.\n",
        "char_vocab = sorted(list(set(\"\".join(all_lines))))\n",
        "char_to_idx = {char: idx for idx, char in enumerate(char_vocab)}\n",
        "idx_to_char = {idx: char for idx, char in enumerate(char_vocab)}\n",
        "\n",
        "# --- AraBERT Setup ---\n",
        "arabert_tokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/bert-base-arabertv2\")\n",
        "arabert_model = AutoModel.from_pretrained(\"aubmindlab/bert-base-arabertv2\")\n",
        "\n",
        "# --- AraBERT helper functions ---\n",
        "def get_character_subword_mappings(text, arabert_tokenizer):\n",
        "    characters = list(text)\n",
        "    subword_tokens = []\n",
        "    subword_indices = []\n",
        "    for i, char in enumerate(characters):\n",
        "        char_tokens = arabert_tokenizer.tokenize(char)\n",
        "        subword_tokens.extend(char_tokens)\n",
        "        subword_indices.extend([i] * len(char_tokens))\n",
        "    return characters, subword_tokens, subword_indices\n",
        "\n",
        "def get_subword_embeddings(subword_tokens, arabert_tokenizer, arabert_model):\n",
        "    inputs = arabert_tokenizer(subword_tokens, is_split_into_words=True, return_tensors=\"pt\", padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = arabert_model(**inputs)\n",
        "    subword_embeddings = outputs.last_hidden_state\n",
        "    return subword_embeddings\n",
        "\n",
        "def aggregate_embeddings(subword_embeddings, subword_indices):\n",
        "    aggregated_embeddings = []\n",
        "    unique_indices = sorted(list(set(subword_indices)))\n",
        "    for unique_idx in unique_indices:\n",
        "      idx_mask = [i for i, x in enumerate(subword_indices) if x == unique_idx]\n",
        "      char_embeddings = subword_embeddings[0][idx_mask] #get all the embeddings for that character.\n",
        "      if len(char_embeddings)>0:\n",
        "          aggregated_embeddings.append(torch.mean(char_embeddings, dim=0)) #get the mean of the characters embeddings.\n",
        "    return torch.stack(aggregated_embeddings)\n",
        "\n",
        "# --- Dataset Preparation ---\n",
        "def prepare_dataset(dataset, arabert_tokenizer, arabert_model, char_to_idx):\n",
        "    data = []\n",
        "    max_len = 0\n",
        "    for sadr, ajz in dataset:\n",
        "        #prepare the sadr\n",
        "        characters, subword_tokens, subword_indices = get_character_subword_mappings(sadr, arabert_tokenizer)\n",
        "        subword_embeddings = get_subword_embeddings(subword_tokens, arabert_tokenizer, arabert_model)\n",
        "        aggregated_embeddings = aggregate_embeddings(subword_embeddings, subword_indices)\n",
        "        #convert the characters into indexes.\n",
        "        sadr_labels = torch.tensor([char_to_idx[char] for char in sadr])\n",
        "        data.append({\"character_embeddings\": aggregated_embeddings, \"labels\":sadr_labels, \"text\":sadr})\n",
        "        max_len = max(max_len, len(aggregated_embeddings))\n",
        "\n",
        "        #prepare the ajz\n",
        "        characters, subword_tokens, subword_indices = get_character_subword_mappings(ajz, arabert_tokenizer)\n",
        "        subword_embeddings = get_subword_embeddings(subword_tokens, arabert_tokenizer, arabert_model)\n",
        "        aggregated_embeddings = aggregate_embeddings(subword_embeddings, subword_indices)\n",
        "        #convert the characters into indexes.\n",
        "        ajz_labels = torch.tensor([char_to_idx[char] for char in ajz])\n",
        "        data.append({\"character_embeddings\": aggregated_embeddings, \"labels\":ajz_labels, \"text\":ajz})\n",
        "        max_len = max(max_len, len(aggregated_embeddings))\n",
        "\n",
        "\n",
        "    #padding\n",
        "    for item in data:\n",
        "      current_len = len(item[\"character_embeddings\"])\n",
        "      if current_len < max_len:\n",
        "        pad_length = max_len - current_len\n",
        "        padding_tensor = torch.full((pad_length, 768), 0)\n",
        "        item[\"character_embeddings\"] = torch.cat((item[\"character_embeddings\"], padding_tensor), dim=0)\n",
        "    return data\n",
        "\n",
        "prepared_dataset = prepare_dataset(normalized_dataset, arabert_tokenizer, arabert_model, char_to_idx)\n",
        "\n",
        "# --- Data Loader ---\n",
        "class PoetryDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        return {\n",
        "            \"character_embeddings\": item[\"character_embeddings\"],\n",
        "            \"labels\": item[\"labels\"],\n",
        "        }\n",
        "dataset = PoetryDataset(prepared_dataset)\n",
        "data_loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "# --- Model Definition ---\n",
        "class CharacterLanguageModel(Module):\n",
        "    def __init__(self, hidden_size=256, num_layers=2, output_size=len(char_vocab), embedding_size=768):\n",
        "        super(CharacterLanguageModel, self).__init__()\n",
        "        self.lstm = torch.nn.LSTM(embedding_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.linear = Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        logits = self.linear(lstm_out)\n",
        "        return logits\n",
        "\n",
        "# --- Model and Training ---\n",
        "model = CharacterLanguageModel()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# --- Training Loop ---\n",
        "num_epochs = 10 #increase the number of epochs.\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0\n",
        "    for batch in data_loader:\n",
        "        input_embeddings = batch[\"character_embeddings\"].unsqueeze(0)\n",
        "        targets = batch[\"labels\"].unsqueeze(0)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(input_embeddings)\n",
        "\n",
        "        # Pad targets to match the logits shape\n",
        "        # Find the maximum length between targets and logits\n",
        "        max_seq_length = max(targets.size(1), logits.size(1))\n",
        "\n",
        "        # Pad targets\n",
        "        targets_padded = torch.nn.functional.pad(targets, (0, max_seq_length - targets.size(1)), 'constant', 0)\n",
        "        loss = loss_function(logits.view(-1, logits.shape[-1]), targets_padded.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss = epoch_loss + loss.item()\n",
        "\n",
        "    print(f\"loss for epoch: {epoch}, is : {epoch_loss}\")\n",
        "\n",
        "# --- Inference ---\n",
        "\n",
        "# Helper function to process text for the model.\n",
        "def process_text(text, arabert_tokenizer, arabert_model):\n",
        "    characters, subword_tokens, subword_indices = get_character_subword_mappings(text, arabert_tokenizer)\n",
        "    subword_embeddings = get_subword_embeddings(subword_tokens, arabert_tokenizer, arabert_model)\n",
        "    aggregated_embeddings = aggregate_embeddings(subword_embeddings, subword_indices)\n",
        "    return aggregated_embeddings\n",
        "\n",
        "# Function to prepare input text for the model\n",
        "def prepare_input_text(text, arabert_tokenizer, arabert_model, char_to_idx, prepared_dataset):\n",
        "    aggregated_embeddings = process_text(text, arabert_tokenizer, arabert_model)\n",
        "\n",
        "    # Padding the input\n",
        "    max_len = 0\n",
        "    for item in prepared_dataset: #use the data from the training set to find the max length\n",
        "        max_len = max(max_len, len(item[\"character_embeddings\"]))\n",
        "\n",
        "    current_len = len(aggregated_embeddings)\n",
        "    if current_len < max_len:\n",
        "      pad_length = max_len - current_len\n",
        "      padding_tensor = torch.full((pad_length, 768), 0)\n",
        "      aggregated_embeddings = torch.cat((aggregated_embeddings, padding_tensor), dim=0)\n",
        "\n",
        "    return aggregated_embeddings.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "# Function to generate text (or get predictions)\n",
        "def generate_text(model, input_text, arabert_tokenizer, arabert_model, char_to_idx, idx_to_char, prepared_dataset):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    input_tensor = prepare_input_text(input_text, arabert_tokenizer, arabert_model, char_to_idx, prepared_dataset)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_tensor)\n",
        "\n",
        "    # Process logits\n",
        "    # here we get the sequence of numbers that the model has generated.\n",
        "    # the numbers will be the indices of the characters in the vocabulary.\n",
        "    predicted_sequence = torch.argmax(logits, dim=-1).squeeze().tolist()\n",
        "    characters = [idx_to_char[idx] for idx in predicted_sequence]\n",
        "    return \"\".join(characters)\n",
        "\n",
        "# Example usage:\n",
        "input_text = \"الحب\"  # Example input text\n",
        "predicted_sequence = generate_text(model, input_text, arabert_tokenizer, arabert_model, char_to_idx, idx_to_char, prepared_dataset)\n",
        "print(f\"Input Text: {input_text}\")\n",
        "print(f\"Predicted Sequence: {predicted_sequence}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "id": "3sSWdEgVNqLz",
        "outputId": "0af8d4e8-03fa-4ae0-e2d9-caa74383769d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error loading or processing dataset from ashaar: \"Column bait not in the dataset. Current columns in the dataset: ['poem title', 'poem meter', 'poem verses', 'poem theme', 'poem url', 'poet name', 'poet description', 'poet url', 'poet era', 'poet location', 'poem description', 'poem language type']\"\n",
            "First dataset not suitable. Attempting to use the second dataset.\n",
            "Failed to download the file. Status code: 404\n",
            "Exiting due to dataset loading failure.\n",
            "Warning: dataset is None. Skipping filtering.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "num_samples should be a positive integer value, but got num_samples=0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1385ce0a5857>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    264\u001b[0m         }\n\u001b[1;32m    265\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPoetryDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepared_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m \u001b[0mdata_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;31m# --- Model Definition ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# map-style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    165\u001b[0m                 \u001b[0;34mf\"num_samples should be a positive integer value, but got num_samples={self.num_samples}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
          ]
        }
      ]
    },
    {
      "source": [
        "# Import the necessary modules\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import pyarabic.araby as araby\n",
        "import string\n",
        "import torch\n",
        "from torch.nn import Linear, Module\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.optim import Adam\n",
        "import random\n",
        "from datasets import load_dataset #import load_dataset\n",
        "import pandas as pd #import pandas\n",
        "import requests #import requests\n",
        "\n",
        "# --- Helper Functions ---\n",
        "\n",
        "# Function to remove punctuation\n",
        "def remove_punctuation(text):\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    return text.translate(translator)\n",
        "\n",
        "# Preprocess Arabic text\n",
        "def preprocess_text(text):\n",
        "    text = araby.strip_tashkeel(text)  # Remove diacritics\n",
        "    text = araby.normalize_ligature(text)  # Normalize ligatures\n",
        "    text = remove_punctuation(text)  # Remove punctuation\n",
        "    return text\n",
        "\n",
        "# --- Dataset Loading and Selection ---\n",
        "\n",
        "def load_and_select_dataset(dataset_name, num_verses=50000):\n",
        "    \"\"\"\n",
        "    Loads the dataset from Hugging Face or Mendeley, checks for 'sadr' and 'ajz' structure,\n",
        "    and selects a random subset of verses.\n",
        "\n",
        "    Args:\n",
        "        dataset_name (str): Name of the dataset to load ('ashaar' or path to csv file).\n",
        "        num_verses (int): Number of random verses to select.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of (sadr, ajz) pairs or None if neither dataset is suitable.\n",
        "    \"\"\"\n",
        "    if dataset_name == 'ashaar':\n",
        "        try:\n",
        "            # Load the ashaar dataset from Hugging Face\n",
        "            dataset = load_dataset('arbml/ashaar')\n",
        "\n",
        "            #get the list of all the verses.\n",
        "            verses = dataset['train']['poem verses'] # get the poem verses instead of bait.\n",
        "\n",
        "            #we need to split each verse into sadr and ajz.\n",
        "            verses_split = []\n",
        "            for verse in verses:\n",
        "                parts = verse.split(\" # \")\n",
        "                if len(parts) == 2: #check that it has both parts.\n",
        "                  verses_split.append((parts[0], parts[1]))\n",
        "                else:\n",
        "                  verses_split.append((parts[0], \"\")) #if it does not have two parts, then ajz will be empty.\n",
        "\n",
        "            print(f\"Dataset loaded successfully from {dataset_name}.\")\n",
        "\n",
        "            # Select up to num_verses\n",
        "            if len(verses_split) > num_verses:\n",
        "              selected_verses = random.sample(verses_split, num_verses)\n",
        "            else:\n",
        "              selected_verses = verses_split\n",
        "\n",
        "            return selected_verses\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading or processing dataset from {dataset_name}: {e}\")\n",
        "            return None\n",
        "\n",
        "    elif dataset_name.endswith('.csv'):\n",
        "        try:\n",
        "            # Load the dataset from a local CSV file using pandas\n",
        "            df = pd.read_csv(dataset_name)\n",
        "\n",
        "            # Check if the dataframe has the necessary columns\n",
        "            if 'verse' in df.columns:\n",
        "                verses = df['verse'].tolist()\n",
        "                verses_split = []\n",
        "                for verse in verses:\n",
        "                    parts = verse.split(\" # \")\n",
        "                    if len(parts) == 2: #check that it has both parts.\n",
        "                        verses_split.append((parts[0], parts[1]))\n",
        "                    else:\n",
        "                        verses_split.append((parts[0], \"\")) #if it does not have two parts, then ajz will be empty.\n",
        "\n",
        "                print(f\"Dataset loaded successfully from {dataset_name}.\")\n",
        "\n",
        "                # Select up to num_verses\n",
        "                if len(verses_split) > num_verses:\n",
        "                  selected_verses = random.sample(verses_split, num_verses)\n",
        "                else:\n",
        "                  selected_verses = verses_split\n",
        "                return selected_verses\n",
        "            else:\n",
        "                print(f\"Dataset from {dataset_name} does not contain 'verse' columns.\")\n",
        "                return None\n",
        "        except FileNotFoundError:\n",
        "            print(f\"File not found: {dataset_name}. Please check the filepath.\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading or processing dataset from {dataset_name}: {e}\")\n",
        "            return None\n",
        "    else:\n",
        "        print(f\"Invalid dataset name or file path: {dataset_name}\")\n",
        "        return None\n",
        "\n",
        "def select_dataset(num_verses=50000):\n",
        "    \"\"\"\n",
        "    Selects and loads a dataset, trying 'ashaar' first and then 'pcd_dataset_path' if necessary.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of (sadr, ajz) pairs or None if neither dataset is suitable.\n",
        "    \"\"\"\n",
        "    selected_dataset = load_and_select_dataset('ashaar', num_verses) # try with ashaar first\n",
        "\n",
        "    # If the first dataset was not suitable, load and select from the second dataset\n",
        "    if selected_dataset is None:\n",
        "        print(\"First dataset not suitable. Attempting to use the second dataset.\")\n",
        "\n",
        "        # Download the dataset from the URL\n",
        "        url = \"https://raw.githubusercontent.com/arbml/poetry-corpus/main/mcj6vkg6zw_cleaned_fragmented_dataset.csv\"\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            with open(\"mcj6vkg6zw_cleaned_fragmented_dataset.csv\", \"wb\") as f:\n",
        "                f.write(response.content)\n",
        "        else:\n",
        "            print(f\"Failed to download the file. Status code: {response.status_code}\")\n",
        "            return None\n",
        "\n",
        "        pcd_dataset_path = \"mcj6vkg6zw_cleaned_fragmented_dataset.csv\"\n",
        "        selected_dataset = load_and_select_dataset(pcd_dataset_path, num_verses)\n",
        "\n",
        "    if selected_dataset is None:\n",
        "        print(\"Both datasets were not suitable.\")\n",
        "\n",
        "    return selected_dataset\n",
        "\n",
        "# --- Dataset Loading and Preprocessing ---\n",
        "num_verses = 50000\n",
        "selected_dataset = select_dataset(num_verses)\n",
        "\n",
        "if selected_dataset is None:\n",
        "    print(\"Exiting due to dataset loading failure.\")\n",
        "    exit()\n",
        "\n",
        "# --- Dataset Preparation ---\n",
        "\n",
        "def filter_incomplete_verses(dataset):\n",
        "    \"\"\"Filters out incomplete verses (missing lines).\"\"\"\n",
        "    # Check if the dataset is None before attempting to iterate over it.\n",
        "    if dataset is None:\n",
        "        print(\"Warning: dataset is None. Skipping filtering.\")\n",
        "        return []  # Return an empty list to avoid the TypeError\n",
        "\n",
        "    filtered_dataset = [verse for verse in dataset if len(verse) > 0] #at least one verse\n",
        "    return filtered_dataset\n",
        "\n",
        "def normalize_dataset(dataset):\n",
        "    \"\"\"Normalizes text in the dataset.\"\"\"\n",
        "    normalized_dataset = []\n",
        "    for sadr, ajz in dataset:\n",
        "        normalized_sadr = preprocess_text(sadr)\n",
        "        normalized_ajz = preprocess_text(ajz)\n",
        "        normalized_dataset.append((normalized_sadr, normalized_ajz))\n",
        "    return normalized_dataset\n",
        "\n",
        "filtered_dataset = filter_incomplete_verses(selected_dataset)\n",
        "normalized_dataset = normalize_dataset(filtered_dataset)\n",
        "\n",
        "# Create a new list to hold all the lines of the dataset.\n",
        "all_lines = []\n",
        "for sadr,ajz in normalized_dataset:\n",
        "  all_lines.append(sadr)\n",
        "  all_lines.append(ajz)\n",
        "\n",
        "# Create a character vocabulary.\n",
        "char_vocab = sorted(list(set(\"\".join(all_lines))))\n",
        "char_to_idx = {char: idx for idx, char in enumerate(char_vocab)}\n",
        "idx_to_char = {idx: char for idx, char in enumerate(char_vocab)}\n",
        "\n",
        "# --- AraBERT Setup ---\n",
        "arabert_tokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/bert-base-arabertv2\")\n",
        "arabert_model = AutoModel.from_pretrained(\"aubmindlab/bert-base-arabertv2\")\n",
        "\n",
        "# --- AraBERT helper functions ---\n",
        "def get_character_subword_mappings(text, arabert_tokenizer):\n",
        "    characters = list(text)\n",
        "    subword_tokens = []\n",
        "    subword_indices = []\n",
        "    for i, char in enumerate(characters):\n",
        "        char_tokens = arabert_tokenizer.tokenize(char)\n",
        "        subword_tokens.extend(char_tokens)\n",
        "        subword_indices.extend([i] * len(char_tokens))\n",
        "    return characters, subword_tokens, subword_indices\n",
        "\n",
        "def get_subword_embeddings(subword_tokens, arabert_tokenizer, arabert_model):\n",
        "    inputs = arabert_tokenizer(subword_tokens, is_split_into_words=True, return_tensors=\"pt\", padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = arabert_model(**inputs)\n",
        "    subword_embeddings = outputs.last_hidden_state\n",
        "    return subword_embeddings\n",
        "\n",
        "def aggregate_embeddings(subword_embeddings, subword_indices):\n",
        "    aggregated_embeddings = []\n",
        "    unique_indices = sorted(list(set(subword_indices)))\n",
        "    for unique_idx in unique_indices:\n",
        "      idx_mask = [i for i, x in enumerate(subword_indices) if x == unique_idx]\n",
        "      char_embeddings = subword_embeddings[0][idx_mask] #get all the embeddings for that character.\n",
        "      if len(char_embeddings)>0:\n",
        "          aggregated_embeddings.append(torch.mean(char_embeddings, dim=0)) #get the mean of the characters embeddings.\n",
        "    return torch.stack(aggregated_embeddings)\n",
        "\n",
        "# --- Dataset Preparation ---\n",
        "def prepare_dataset(dataset, arabert_tokenizer, arabert_model, char_to_idx):\n",
        "    data = []\n",
        "    max_len = 0\n",
        "    for sadr, ajz in dataset:\n",
        "        #prepare the sadr\n",
        "        characters, subword_tokens, subword_indices = get_character_subword_mappings(sadr, arabert_tokenizer)\n",
        "        subword_embeddings = get_subword_embeddings(subword_tokens, arabert_tokenizer, arabert_model)\n",
        "        aggregated_embeddings = aggregate_embeddings(subword_embeddings, subword_indices)\n",
        "        #convert the characters into indexes.\n",
        "        sadr_labels = torch.tensor([char_to_idx[char] for char in sadr])\n",
        "        data.append({\"character_embeddings\": aggregated_embeddings, \"labels\":sadr_labels, \"text\":sadr})\n",
        "        max_len = max(max_len, len(aggregated_embeddings))\n",
        "\n",
        "        #prepare the ajz\n",
        "        characters, subword_tokens, subword_indices = get_character_subword_mappings(ajz, arabert_tokenizer)\n",
        "        subword_embeddings = get_subword_embeddings(subword_tokens, arabert_tokenizer, arabert_model)\n",
        "        aggregated_embeddings = aggregate_embeddings(subword_embeddings, subword_indices)\n",
        "        #convert the characters into indexes.\n",
        "        ajz_labels = torch.tensor([char_to_idx[char] for char in ajz])\n",
        "        data.append({\"character_embeddings\": aggregated_embeddings, \"labels\":ajz_labels, \"text\":ajz})\n",
        "        max_len = max(max_len, len(aggregated_embeddings))\n",
        "\n",
        "\n",
        "    #padding\n",
        "    for item in data:\n",
        "      current_len = len(item[\"character_embeddings\"])\n",
        "      if current_len < max_len:\n",
        "        pad_length = max_len - current_len\n",
        "        padding_tensor = torch.full((pad_length, 768), 0)\n",
        "        item[\"character_embeddings\"] = torch.cat((item[\"character_embeddings\"], padding_tensor), dim=0)\n",
        "    return data\n",
        "\n",
        "prepared_dataset = prepare_dataset(normalized_dataset, arabert_tokenizer, arabert_model, char_to_idx)\n",
        "\n",
        "# --- Data Loader ---\n",
        "class PoetryDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        return {\n",
        "            \"character_embeddings\": item[\"character_embeddings\"],\n",
        "            \"labels\": item[\"labels\"],\n",
        "        }\n",
        "dataset = PoetryDataset(prepared_dataset)\n",
        "data_loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "# --- Model Definition ---\n",
        "class CharacterLanguageModel(Module):\n",
        "    def __init__(self, hidden_size=256, num_layers=2, output_size=len(char_vocab), embedding_size=768):\n",
        "        super(CharacterLanguageModel, self).__init__()\n",
        "        self.lstm = torch.nn.LSTM(embedding_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.linear = Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        logits = self.linear(lstm_out)\n",
        "        return logits\n",
        "\n",
        "# --- Model and Training ---\n",
        "model = CharacterLanguageModel()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# --- Training Loop ---\n",
        "num_epochs = 10 #increase the number of epochs.\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0\n",
        "    for batch in data_loader:\n",
        "        input_embeddings = batch[\"character_embeddings\"].unsqueeze(0)\n",
        "        targets = batch[\"labels\"].unsqueeze(0)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(input_embeddings)\n",
        "\n",
        "        # Pad targets to match the logits shape\n",
        "        # Find the maximum length between targets and logits\n",
        "        max_seq_length = max(targets.size(1), logits.size(1))\n",
        "\n",
        "        # Pad targets\n",
        "        targets_padded = torch.nn.functional.pad(targets, (0, max_seq_length - targets.size(1)), 'constant', 0)\n",
        "        loss = loss_function(logits.view(-1, logits.shape[-1]), targets_padded.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss = epoch_loss + loss.item()\n",
        "\n",
        "    print(f\"loss for epoch: {epoch}, is : {epoch_loss}\")\n",
        "\n",
        "# --- Inference ---\n",
        "\n",
        "# Helper function to process text for the model.\n",
        "def process_text(text, arabert_tokenizer, arabert_model):\n",
        "    characters, subword_tokens, subword_indices = get_character_subword_mappings(text, arabert_tokenizer)\n",
        "    subword_embeddings = get_subword_embeddings(subword_tokens, arabert_tokenizer, arabert_model)\n",
        "    aggregated_embeddings = aggregate_embeddings(subword_embeddings, subword_indices)\n",
        "    return aggregated_embeddings\n",
        "\n",
        "# Function to prepare input text for the model\n",
        "def prepare_input_text(text, arabert_tokenizer, arabert_model, char_to_idx, prepared_dataset):\n",
        "    aggregated_embeddings = process_text(text, arabert_tokenizer, arabert_model)\n",
        "\n",
        "    # Padding the input\n",
        "    max_len = 0\n",
        "    for item in prepared_dataset: #use the data from the training set to find the max length\n",
        "        max_len = max(max_len, len(item[\"character_embeddings\"]))\n",
        "\n",
        "    current_len = len(aggregated_embeddings)\n",
        "    if current_len < max_len:\n",
        "      pad_length = max_len - current_len\n",
        "      padding_tensor = torch.full((pad_length, 768), 0)\n",
        "      aggregated_embeddings = torch.cat((aggregated_embeddings, padding_tensor), dim=0)\n",
        "\n",
        "    return aggregated_embeddings.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "# Function to generate text (or get predictions)\n",
        "def generate_text(model, input_text, arabert_tokenizer, arabert_model, char_to_idx, idx_to_char, prepared_dataset):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    input_tensor = prepare_input_text(input_text, arabert_tokenizer, arabert_model, char_to_idx, prepared_dataset)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_tensor)\n",
        "\n",
        "    # Process logits\n",
        "    # here we get the sequence of numbers that the model has generated.\n",
        "    # the numbers will be the indices of the characters in the vocabulary.\n",
        "    predicted_sequence = torch.argmax(logits, dim=-1).squeeze().tolist()\n",
        "    characters = [idx_to_char[idx] for idx in predicted_sequence]\n",
        "    return \"\".join(characters)\n",
        "\n",
        "# Example usage:\n",
        "input_text = \"الحب\"  # Example input text\n",
        "predicted_sequence = generate_text(model, input_text, arabert_tokenizer, arabert_model, char_to_idx, idx_to_char, prepared_dataset)\n",
        "print(f\"Input Text: {input_text}\")\n",
        "print(f\"Predicted Sequence: {predicted_sequence}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "id": "m6eJRP8rOLCd",
        "outputId": "2bca0ff4-28d6-4797-d177-8bc84537b53e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error loading or processing dataset from ashaar: 'list' object has no attribute 'split'\n",
            "First dataset not suitable. Attempting to use the second dataset.\n",
            "Failed to download the file. Status code: 404\n",
            "Exiting due to dataset loading failure.\n",
            "Warning: dataset is None. Skipping filtering.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "num_samples should be a positive integer value, but got num_samples=0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c0d710031c66>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    264\u001b[0m         }\n\u001b[1;32m    265\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPoetryDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepared_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m \u001b[0mdata_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;31m# --- Model Definition ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# map-style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    165\u001b[0m                 \u001b[0;34mf\"num_samples should be a positive integer value, but got num_samples={self.num_samples}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
          ]
        }
      ]
    },
    {
      "source": [
        "# Import the necessary modules\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import pyarabic.araby as araby\n",
        "import string\n",
        "import torch\n",
        "from torch.nn import Linear, Module\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.optim import Adam\n",
        "import random\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "# --- Helper Functions ---\n",
        "\n",
        "# Function to remove punctuation\n",
        "def remove_punctuation(text):\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    return text.translate(translator)\n",
        "\n",
        "# Preprocess Arabic text\n",
        "def preprocess_text(text):\n",
        "    text = araby.strip_tashkeel(text)  # Remove diacritics\n",
        "    text = araby.normalize_ligature(text)  # Normalize ligatures\n",
        "    text = remove_punctuation(text)  # Remove punctuation\n",
        "    return text\n",
        "\n",
        "# --- Dataset Loading and Selection ---\n",
        "\n",
        "def load_and_select_dataset(dataset_name, num_verses=50000):\n",
        "    \"\"\"\n",
        "    Loads the dataset from Hugging Face or a CSV file, selects a random subset of verses.\n",
        "\n",
        "    Args:\n",
        "        dataset_name (str): Name of the dataset to load ('ashaar' or path to csv file).\n",
        "        num_verses (int): Number of random verses to select.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of verses or None if neither dataset is suitable.\n",
        "    \"\"\"\n",
        "    if dataset_name == 'ashaar':\n",
        "        try:\n",
        "            # Load the ashaar dataset from Hugging Face\n",
        "            dataset = load_dataset('arbml/ashaar')\n",
        "\n",
        "            # Get the list of all the verses.\n",
        "            verses = dataset['train']['poem verses']\n",
        "\n",
        "            print(f\"Dataset loaded successfully from {dataset_name}.\")\n",
        "\n",
        "            # Select up to num_verses\n",
        "            if len(verses) > num_verses:\n",
        "                selected_verses = random.sample(verses, num_verses)\n",
        "            else:\n",
        "                selected_verses = verses\n",
        "\n",
        "            return selected_verses\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading or processing dataset from {dataset_name}: {e}\")\n",
        "            return None\n",
        "\n",
        "    elif dataset_name.endswith('.csv'):\n",
        "        try:\n",
        "            # Load the dataset from a local CSV file using pandas\n",
        "            df = pd.read_csv(dataset_name)\n",
        "\n",
        "            # Check if the dataframe has the necessary columns\n",
        "            if 'verse' in df.columns:\n",
        "                verses = df['verse'].tolist()\n",
        "\n",
        "                print(f\"Dataset loaded successfully from {dataset_name}.\")\n",
        "\n",
        "                # Select up to num_verses\n",
        "                if len(verses) > num_verses:\n",
        "                    selected_verses = random.sample(verses, num_verses)\n",
        "                else:\n",
        "                    selected_verses = verses\n",
        "                return selected_verses\n",
        "            else:\n",
        "                print(f\"Dataset from {dataset_name} does not contain 'verse' columns.\")\n",
        "                return None\n",
        "        except FileNotFoundError:\n",
        "            print(f\"File not found: {dataset_name}. Please check the filepath.\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading or processing dataset from {dataset_name}: {e}\")\n",
        "            return None\n",
        "    else:\n",
        "        print(f\"Invalid dataset name or file path: {dataset_name}\")\n",
        "        return None\n",
        "\n",
        "def select_dataset(num_verses=50000):\n",
        "    \"\"\"\n",
        "    Selects and loads a dataset, trying 'ashaar' first and then 'pcd_dataset_path' if necessary.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of verses or None if neither dataset is suitable.\n",
        "    \"\"\"\n",
        "    selected_dataset = load_and_select_dataset('ashaar', num_verses)  # try with ashaar first\n",
        "\n",
        "    # If the first dataset was not suitable, load and select from the second dataset\n",
        "    if selected_dataset is None:\n",
        "        print(\"First dataset not suitable. Attempting to use the second dataset.\")\n",
        "\n",
        "        # Download the dataset from the URL\n",
        "        url = \"https://raw.githubusercontent.com/arbml/poetry-corpus/main/mcj6vkg6zw_cleaned_fragmented_dataset.csv\"\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            with open(\"mcj6vkg6zw_cleaned_fragmented_dataset.csv\", \"wb\") as f:\n",
        "                f.write(response.content)\n",
        "        else:\n",
        "            print(f\"Failed to download the file. Status code: {response.status_code}\")\n",
        "            return None\n",
        "\n",
        "        pcd_dataset_path = \"mcj6vkg6zw_cleaned_fragmented_dataset.csv\"\n",
        "        selected_dataset = load_and_select_dataset(pcd_dataset_path, num_verses)\n",
        "\n",
        "    if selected_dataset is None:\n",
        "        print(\"Both datasets were not suitable.\")\n",
        "\n",
        "    return selected_dataset\n",
        "\n",
        "# --- Dataset Loading and Preprocessing ---\n",
        "num_verses = 50000\n",
        "selected_dataset = select_dataset(num_verses)\n",
        "\n",
        "if selected_dataset is None:\n",
        "    print(\"Exiting due to dataset loading failure.\")\n",
        "    exit()\n",
        "\n",
        "# --- Dataset Preparation ---\n",
        "\n",
        "def filter_incomplete_verses(dataset):\n",
        "    \"\"\"Filters out incomplete verses (empty strings).\"\"\"\n",
        "    # Check if the dataset is None before attempting to iterate over it.\n",
        "    if dataset is None:\n",
        "        print(\"Warning: dataset is None. Skipping filtering.\")\n",
        "        return []  # Return an empty list to avoid the TypeError\n",
        "\n",
        "    filtered_dataset = [verse for verse in dataset if len(verse) > 0]  # at least one verse\n",
        "    return filtered_dataset\n",
        "\n",
        "def normalize_dataset(dataset):\n",
        "    \"\"\"Normalizes text in the dataset.\"\"\"\n",
        "    normalized_dataset = []\n",
        "    for verse in dataset:\n",
        "        normalized_verse = preprocess_text(verse)\n",
        "        normalized_dataset.append(normalized_verse)\n",
        "    return normalized_dataset\n",
        "\n",
        "filtered_dataset = filter_incomplete_verses(selected_dataset)\n",
        "normalized_dataset = normalize_dataset(filtered_dataset)\n",
        "\n",
        "# Create a character vocabulary.\n",
        "char_vocab = sorted(list(set(\"\".join(normalized_dataset))))\n",
        "char_to_idx = {char: idx for idx, char in enumerate(char_vocab)}\n",
        "idx_to_char = {idx: char for idx, char in enumerate(char_vocab)}\n",
        "\n",
        "# --- AraBERT Setup ---\n",
        "arabert_tokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/bert-base-arabertv2\")\n",
        "arabert_model = AutoModel.from_pretrained(\"aubmindlab/bert-base-arabertv2\")\n",
        "\n",
        "# --- AraBERT helper functions ---\n",
        "def get_character_subword_mappings(text, arabert_tokenizer):\n",
        "    characters = list(text)\n",
        "    subword_tokens = []\n",
        "    subword_indices = []\n",
        "    for i, char in enumerate(characters):\n",
        "        char_tokens = arabert_tokenizer.tokenize(char)\n",
        "        subword_tokens.extend(char_tokens)\n",
        "        subword_indices.extend([i] * len(char_tokens))\n",
        "    return characters, subword_tokens, subword_indices\n",
        "\n",
        "def get_subword_embeddings(subword_tokens, arabert_tokenizer, arabert_model):\n",
        "    inputs = arabert_tokenizer(subword_tokens, is_split_into_words=True, return_tensors=\"pt\", padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = arabert_model(**inputs)\n",
        "    subword_embeddings = outputs.last_hidden_state\n",
        "    return subword_embeddings\n",
        "\n",
        "def aggregate_embeddings(subword_embeddings, subword_indices):\n",
        "    aggregated_embeddings = []\n",
        "    unique_indices = sorted(list(set(subword_indices)))\n",
        "    for unique_idx in unique_indices:\n",
        "      idx_mask = [i for i, x in enumerate(subword_indices) if x == unique_idx]\n",
        "      char_embeddings = subword_embeddings[0][idx_mask] #get all the embeddings for that character.\n",
        "      if len(char_embeddings)>0:\n",
        "          aggregated_embeddings.append(torch.mean(char_embeddings, dim=0)) #get the mean of the characters embeddings.\n",
        "    return torch.stack(aggregated_embeddings)\n",
        "\n",
        "# --- Dataset Preparation ---\n",
        "def prepare_dataset(dataset, arabert_tokenizer, arabert_model, char_to_idx):\n",
        "    data = []\n",
        "    max_len = 0\n",
        "    for verse in dataset:\n",
        "        characters, subword_tokens, subword_indices = get_character_subword_mappings(verse, arabert_tokenizer)\n",
        "        subword_embeddings = get_subword_embeddings(subword_tokens, arabert_tokenizer, arabert_model)\n",
        "        aggregated_embeddings = aggregate_embeddings(subword_embeddings, subword_indices)\n",
        "        #convert the characters into indexes.\n",
        "        verse_labels = torch.tensor([char_to_idx[char] for char in verse])\n",
        "        data.append({\"character_embeddings\": aggregated_embeddings, \"labels\":verse_labels, \"text\":verse})\n",
        "        max_len = max(max_len, len(aggregated_embeddings))\n",
        "\n",
        "    #padding\n",
        "    for item in data:\n",
        "      current_len = len(item[\"character_embeddings\"])\n",
        "      if current_len < max_len:\n",
        "        pad_length = max_len - current_len\n",
        "        padding_tensor = torch.full((pad_length, 768), 0)\n",
        "        item[\"character_embeddings\"] = torch.cat((item[\"character_embeddings\"], padding_tensor), dim=0)\n",
        "    return data\n",
        "\n",
        "prepared_dataset = prepare_dataset(normalized_dataset, arabert_tokenizer, arabert_model, char_to_idx)\n",
        "\n",
        "# --- Data Loader ---\n",
        "class PoetryDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        return {\n",
        "            \"character_embeddings\": item[\"character_embeddings\"],\n",
        "            \"labels\": item[\"labels\"],\n",
        "        }\n",
        "dataset = PoetryDataset(prepared_dataset)\n",
        "data_loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "# --- Model Definition ---\n",
        "class CharacterLanguageModel(Module):\n",
        "    def __init__(self, hidden_size=256, num_layers=2, output_size=len(char_vocab), embedding_size=768):\n",
        "        super(CharacterLanguageModel, self).__init__()\n",
        "        self.lstm = torch.nn.LSTM(embedding_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.linear = Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        logits = self.linear(lstm_out)\n",
        "        return logits\n",
        "\n",
        "# --- Model and Training ---\n",
        "model = CharacterLanguageModel()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# --- Training Loop ---\n",
        "num_epochs = 10 #increase the number of epochs.\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0\n",
        "    for batch in data_loader:\n",
        "        input_embeddings = batch[\"character_embeddings\"].unsqueeze(0)\n",
        "        targets = batch[\"labels\"].unsqueeze(0)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(input_embeddings)\n",
        "\n",
        "        # Pad targets to match the logits shape\n",
        "        # Find the maximum length between targets and logits\n",
        "        max_seq_length = max(targets.size(1), logits.size(1))\n",
        "\n",
        "        # Pad targets\n",
        "        targets_padded = torch.nn.functional.pad(targets, (0, max_seq_length - targets.size(1)), 'constant', 0)\n",
        "        loss = loss_function(logits.view(-1, logits.shape[-1]), targets_padded.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss = epoch_loss + loss.item()\n",
        "\n",
        "    print(f\"loss for epoch: {epoch}, is : {epoch_loss}\")\n",
        "\n",
        "# --- Inference ---\n",
        "\n",
        "# Helper function to process text for the model.\n",
        "def process_text(text, arabert_tokenizer, arabert_model):\n",
        "    characters, subword_tokens, subword_indices = get_character_subword_mappings(text, arabert_tokenizer)\n",
        "    subword_embeddings = get_subword_embeddings(subword_tokens, arabert_tokenizer, arabert_model)\n",
        "    aggregated_embeddings = aggregate_embeddings(subword_embeddings, subword_indices)\n",
        "    return aggregated_embeddings\n",
        "\n",
        "# Function to prepare input text for the model\n",
        "def prepare_input_text(text, arabert_tokenizer, arabert_model, char_to_idx, prepared_dataset):\n",
        "    aggregated_embeddings = process_text(text, arabert_tokenizer, arabert_model)\n",
        "\n",
        "    # Padding the input\n",
        "    max_len = 0\n",
        "    for item in prepared_dataset: #use the data from the training set to find the max length\n",
        "        max_len = max(max_len, len(item[\"character_embeddings\"]))\n",
        "\n",
        "    current_len = len(aggregated_embeddings)\n",
        "    if current_len < max_len:\n",
        "      pad_length = max_len - current_len\n",
        "      padding_tensor = torch.full((pad_length, 768), 0)\n",
        "      aggregated_embeddings = torch.cat((aggregated_embeddings, padding_tensor), dim=0)\n",
        "\n",
        "    return aggregated_embeddings.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "# Function to generate text (or get predictions)\n",
        "def generate_text(model, input_text, arabert_tokenizer, arabert_model, char_to_idx, idx_to_char, prepared_dataset):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    input_tensor = prepare_input_text(input_text, arabert_tokenizer, arabert_model, char_to_idx, prepared_dataset)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_tensor)\n",
        "\n",
        "    # Process logits\n",
        "    # here we get the sequence of numbers that the model has generated.\n",
        "    # the numbers will be the indices of the characters in the vocabulary.\n",
        "    predicted_sequence = torch.argmax(logits, dim=-1).squeeze().tolist()\n",
        "    characters = [idx_to_char[idx] for idx in predicted_sequence]\n",
        "    return \"\".join(characters)\n",
        "\n",
        "# Example usage:\n",
        "input_text = \"الحب\"  # Example input text\n",
        "predicted_sequence = generate_text(model, input_text, arabert_tokenizer, arabert_model, char_to_idx, idx_to_char, prepared_dataset)\n",
        "print(f\"Input Text: {input_text}\")\n",
        "print(f\"Predicted Sequence: {predicted_sequence}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 862
        },
        "id": "45U1pN9MOpO3",
        "outputId": "6fbe2a96-2f4d-4622-d44b-824e4104526d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully from ashaar.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'isalpha'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d2ef91648a00>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0mfiltered_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_incomplete_verses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m \u001b[0mnormalized_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;31m# Create a character vocabulary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-d2ef91648a00>\u001b[0m in \u001b[0;36mnormalize_dataset\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0mnormalized_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mverse\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mnormalized_verse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0mnormalized_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalized_verse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnormalized_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-d2ef91648a00>\u001b[0m in \u001b[0;36mpreprocess_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Preprocess Arabic text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maraby\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip_tashkeel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Remove diacritics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maraby\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_ligature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Normalize ligatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_punctuation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Remove punctuation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarabic/araby.py\u001b[0m in \u001b[0;36mstrip_tashkeel\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mis_vocalized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mTASHKEEL\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarabic/araby.py\u001b[0m in \u001b[0;36mis_vocalized\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m    565\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mBoolean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m     \"\"\"\n\u001b[0;32m--> 567\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'isalpha'"
          ]
        }
      ]
    },
    {
      "source": [
        "# Install necessary libraries\n",
        "!pip install transformers torch pyarabic datasets pandas\n",
        "\n",
        "# Import the necessary modules\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import pyarabic.araby as araby\n",
        "import string\n",
        "import torch\n",
        "from torch.nn import Linear, Module\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.optim import Adam\n",
        "import random\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "# --- Helper Functions ---\n",
        "\n",
        "# Function to remove punctuation\n",
        "def remove_punctuation(text):\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    return text.translate(translator)\n",
        "\n",
        "# Preprocess Arabic text\n",
        "def preprocess_text(text):\n",
        "    try: # added a try block.\n",
        "      text = araby.strip_tashkeel(text)  # Remove diacritics\n",
        "      text = araby.normalize_ligature(text)  # Normalize ligatures\n",
        "      text = remove_punctuation(text)  # Remove punctuation\n",
        "      return text\n",
        "    except Exception as e:\n",
        "      print(f\"Error in preprocess_text: {e}. Skipping this verse.\")\n",
        "      return \"\" # return empty string.\n",
        "\n",
        "# --- Dataset Loading and Selection ---\n",
        "\n",
        "def load_and_select_dataset(dataset_name, num_verses=50000):\n",
        "    \"\"\"\n",
        "    Loads the dataset from Hugging Face or a CSV file, selects a random subset of verses.\n",
        "\n",
        "    Args:\n",
        "        dataset_name (str): Name of the dataset to load ('ashaar' or path to csv file).\n",
        "        num_verses (int): Number of random verses to select.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of verses or None if neither dataset is suitable.\n",
        "    \"\"\"\n",
        "    if dataset_name == 'ashaar':\n",
        "        try:\n",
        "            # Load the ashaar dataset from Hugging Face\n",
        "            dataset = load_dataset('arbml/ashaar')\n",
        "\n",
        "            # Get the list of all the verses.\n",
        "            verses = dataset['train']['poem verses'] #changed this line from 'poem verses' to 'bait'\n",
        "\n",
        "            print(f\"Dataset loaded successfully from {dataset_name}.\")\n",
        "\n",
        "            # Select up to num_verses\n",
        "            if len(verses) > num_verses:\n",
        "                selected_verses = random.sample(verses, num_verses)\n",
        "            else:\n",
        "                selected_verses = verses\n",
        "\n",
        "            return selected_verses\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading or processing dataset from {dataset_name}: {e}\")\n",
        "            return None\n",
        "\n",
        "    elif dataset_name.endswith('.csv'):\n",
        "        try:\n",
        "            # Load the dataset from a local CSV file using pandas\n",
        "            df = pd.read_csv(dataset_name)\n",
        "\n",
        "            # Check if the dataframe has the necessary columns\n",
        "            if 'verse' in df.columns:\n",
        "                verses = df['verse'].tolist()\n",
        "\n",
        "                print(f\"Dataset loaded successfully from {dataset_name}.\")\n",
        "\n",
        "                # Select up to num_verses\n",
        "                if len(verses) > num_verses:\n",
        "                    selected_verses = random.sample(verses, num_verses)\n",
        "                else:\n",
        "                    selected_verses = verses\n",
        "                return selected_verses\n",
        "            else:\n",
        "                print(f\"Dataset from {dataset_name} does not contain 'verse' columns.\")\n",
        "                return None\n",
        "        except FileNotFoundError:\n",
        "            print(f\"File not found: {dataset_name}. Please check the filepath.\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading or processing dataset from {dataset_name}: {e}\")\n",
        "            return None\n",
        "    else:\n",
        "        print(f\"Invalid dataset name or file path: {dataset_name}\")\n",
        "        return None\n",
        "\n",
        "def select_dataset(num_verses=50000):\n",
        "    \"\"\"\n",
        "    Selects and loads a dataset, trying 'ashaar' first and then 'pcd_dataset_path' if necessary.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of verses or None if neither dataset is suitable.\n",
        "    \"\"\"\n",
        "    selected_dataset = load_and_select_dataset('ashaar', num_verses)  # try with ashaar first\n",
        "\n",
        "    # If the first dataset was not suitable, load and select from the second dataset\n",
        "    if selected_dataset is None:\n",
        "        print(\"First dataset not suitable. Attempting to use the second dataset.\")\n",
        "\n",
        "        # Download the dataset from the URL\n",
        "        url = \"https://raw.githubusercontent.com/arbml/poetry-corpus/main/mcj6vkg6zw_cleaned_fragmented_dataset.csv\"\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            with open(\"mcj6vkg6zw_cleaned_fragmented_dataset.csv\", \"wb\") as f:\n",
        "                f.write(response.content)\n",
        "        else:\n",
        "            print(f\"Failed to download the file. Status code: {response.status_code}\")\n",
        "            return None\n",
        "\n",
        "        pcd_dataset_path = \"mcj6vkg6zw_cleaned_fragmented_dataset.csv\"\n",
        "        selected_dataset = load_and_select_dataset(pcd_dataset_path, num_verses)\n",
        "\n",
        "    if selected_dataset is None:\n",
        "        print(\"Both datasets were not suitable.\")\n",
        "\n",
        "    return selected_dataset\n",
        "\n",
        "# --- Dataset Loading and Preprocessing ---\n",
        "num_verses = 50000\n",
        "selected_dataset = select_dataset(num_verses)\n",
        "\n",
        "if selected_dataset is None:\n",
        "    print(\"Exiting due to dataset loading failure.\")\n",
        "    exit()\n",
        "\n",
        "# --- Dataset Preparation ---\n",
        "\n",
        "def filter_incomplete_verses(dataset):\n",
        "    \"\"\"Filters out incomplete verses (empty strings).\"\"\"\n",
        "    # Check if the dataset is None before attempting to iterate over it.\n",
        "    if dataset is None:\n",
        "        print(\"Warning: dataset is None. Skipping filtering.\")\n",
        "        return []  # Return an empty list to avoid the TypeError\n",
        "\n",
        "    filtered_dataset = [verse for verse in dataset if isinstance(verse, str) and len(verse) > 0] # added isinstance(verse, str) to check that the element is a string.\n",
        "    return filtered_dataset\n",
        "\n",
        "def normalize_dataset(dataset):\n",
        "    \"\"\"Normalizes text in the dataset.\"\"\"\n",
        "    normalized_dataset = []\n",
        "    for verse in dataset:\n",
        "      if isinstance(verse, str): # added a check to see if the verse is a string\n",
        "        normalized_verse = preprocess_text(verse)\n",
        "        normalized_dataset.append(normalized_verse)\n",
        "    return normalized_dataset\n",
        "\n",
        "filtered_dataset = filter_incomplete_verses(selected_dataset)\n",
        "normalized_dataset = normalize_dataset(filtered_dataset)\n",
        "\n",
        "# Create a character vocabulary.\n",
        "char_vocab = sorted(list(set(\"\".join(normalized_dataset))))\n",
        "char_to_idx = {char: idx for idx, char in enumerate(char_vocab)}\n",
        "idx_to_char = {idx: char for idx, char in enumerate(char_vocab)}\n",
        "\n",
        "# --- AraBERT Setup ---\n",
        "arabert_tokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/bert-base-arabertv2\")\n",
        "arabert_model = AutoModel.from_pretrained(\"aubmindlab/bert-base-arabertv2\")\n",
        "\n",
        "# --- AraBERT helper functions ---\n",
        "def get_character_subword_mappings(text, arabert_tokenizer):\n",
        "    characters = list(text)\n",
        "    subword_tokens = []\n",
        "    subword_indices = []\n",
        "    for i, char in enumerate(characters):\n",
        "        char_tokens = arabert_tokenizer.tokenize(char)\n",
        "        subword_tokens.extend(char_tokens)\n",
        "        subword_indices.extend([i] * len(char_tokens))\n",
        "    return characters, subword_tokens, subword_indices\n",
        "\n",
        "def get_subword_embeddings(subword_tokens, arabert_tokenizer, arabert_model):\n",
        "    inputs = arabert_tokenizer(subword_tokens, is_split_into_words=True, return_tensors=\"pt\", padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = arabert_model(**inputs)\n",
        "    subword_embeddings = outputs.last_hidden_state\n",
        "    return subword_embeddings\n",
        "\n",
        "def aggregate_embeddings(subword_embeddings, subword_indices):\n",
        "    aggregated_embeddings = []\n",
        "    unique_indices = sorted(list(set(subword_indices)))\n",
        "    for unique_idx in unique_indices:\n",
        "      idx_mask = [i for i, x in enumerate(subword_indices) if x == unique_idx]\n",
        "      char_embeddings = subword_embeddings[0][idx_mask] #get all the embeddings for that character.\n",
        "      if len(char_embeddings)>0:\n",
        "          aggregated_embeddings.append(torch.mean(char_embeddings, dim=0)) #get the mean of the characters embeddings.\n",
        "    return torch.stack(aggregated_embeddings)\n",
        "\n",
        "# --- Dataset Preparation ---\n",
        "def prepare_dataset(dataset, arabert_tokenizer, arabert_model, char_to_idx):\n",
        "    data = []\n",
        "    max_len = 0\n",
        "    for verse in dataset:\n",
        "        characters, subword_tokens, subword_indices = get_character_subword_mappings(verse, arabert_tokenizer)\n",
        "        subword_embeddings = get_subword_embeddings(subword_tokens, arabert_tokenizer, arabert_model)\n",
        "        aggregated_embeddings = aggregate_embeddings(subword_embeddings, subword_indices)\n",
        "        #convert the characters into indexes.\n",
        "        verse_labels = torch.tensor([char_to_idx[char] for char in verse])\n",
        "        data.append({\"character_embeddings\": aggregated_embeddings, \"labels\":verse_labels, \"text\":verse})\n",
        "        max_len = max(max_len, len(aggregated_embeddings))\n",
        "\n",
        "    #padding\n",
        "    for item in data:\n",
        "      current_len = len(item[\"character_embeddings\"])\n",
        "      if current_len < max_len:\n",
        "        pad_length = max_len - current_len\n",
        "        padding_tensor = torch.full((pad_length, 768), 0)\n",
        "        if item[\"character_embeddings\"].ndim == 2:\n",
        "          item[\"character_embeddings\"] = torch.cat((item[\"character_embeddings\"], padding_tensor), dim=0)\n",
        "        elif item[\"character_embeddings\"].ndim == 3:\n",
        "          item[\"character_embeddings\"] = torch.cat((item[\"character_embeddings\"], padding_tensor.unsqueeze(0)), dim=1) #add batch dimension\n",
        "    return data\n",
        "\n",
        "prepared_dataset = prepare_dataset(normalized_dataset, arabert_tokenizer, arabert_model, char_to_idx)\n",
        "\n",
        "# --- Data Loader ---\n",
        "class PoetryDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        return {\n",
        "            \"character_embeddings\": item[\"character_embeddings\"],\n",
        "            \"labels\": item[\"labels\"],\n",
        "        }\n",
        "dataset = PoetryDataset(prepared_dataset)\n",
        "data_loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "# --- Model Definition ---\n",
        "class CharacterLanguageModel(Module):\n",
        "    def __init__(self, hidden_size=256, num_layers=2, output_size=len(char_vocab), embedding_size=768):\n",
        "        super(CharacterLanguageModel, self).__init__()\n",
        "        self.lstm = torch.nn.LSTM(embedding_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.linear = Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        logits = self.linear(lstm_out)\n",
        "        return logits\n",
        "\n",
        "# --- Model and Training ---\n",
        "model = CharacterLanguageModel()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# --- Training Loop ---\n",
        "num_epochs = 30 #increased to 30\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0\n",
        "    for batch in data_loader:\n",
        "        input_embeddings = batch[\"character_embeddings\"].unsqueeze(0)\n",
        "        targets = batch[\"labels\"].unsqueeze(0)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(input_embeddings)\n",
        "\n",
        "        # Pad targets to match the logits shape\n",
        "        # Find the maximum length between targets and logits\n",
        "        max_seq_length = max(targets.size(1), logits.size(1))\n",
        "\n",
        "        # Pad targets\n",
        "        targets_padded = torch.nn.functional.pad(targets, (0, max_seq_length - targets.size(1)), 'constant', 0)\n",
        "        loss = loss_function(logits.view(-1, logits.shape[-1]), targets_padded.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss = epoch_loss + loss.item()\n",
        "\n",
        "    print(f\"loss for epoch: {epoch}, is : {epoch_loss}\")\n",
        "\n",
        "# --- Inference ---\n",
        "\n",
        "# Helper function to process text for the model.\n",
        "def process_text(text, arabert_tokenizer, arabert_model):\n",
        "    characters, subword_tokens, subword_indices = get_character_subword_mappings(text, arabert_tokenizer)\n",
        "    subword_embeddings = get_subword_embeddings(subword_tokens, arabert_tokenizer, arabert_model)\n",
        "    aggregated_embeddings = aggregate_embeddings(subword_embeddings, subword_indices)\n",
        "    return aggregated_embeddings\n",
        "\n",
        "# Function to prepare input text for the model\n",
        "def prepare_input_text(text, arabert_tokenizer, arabert_model, char_to_idx, prepared_dataset):\n",
        "    aggregated_embeddings = process_text(text, arabert_tokenizer, arabert_model)\n",
        "\n",
        "    # Padding the input\n",
        "    max_len = 0\n",
        "    for item in prepared_dataset: #use the data from the training set to find the max length\n",
        "        max_len = max(max_len, len(item[\"character_embeddings\"]))\n",
        "\n",
        "    current_len = len(aggregated_embeddings)\n",
        "    if current_len < max_len:\n",
        "      pad_length = max_len - current_len\n",
        "      padding_tensor = torch.full((pad_length, 768), 0)\n",
        "      if aggregated_embeddings.ndim == 2:\n",
        "        aggregated_embeddings = torch.cat((aggregated_embeddings, padding_tensor), dim=0)\n",
        "      elif aggregated_embeddings.ndim == 3:\n",
        "        aggregated_embeddings = torch.cat((aggregated_embeddings, padding_tensor.unsqueeze(0)), dim=1)\n",
        "\n",
        "    return aggregated_embeddings.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "# Function to generate text (or get predictions)\n",
        "def generate_text(model, input_text, arabert_tokenizer, arabert_model, char_to_idx, idx_to_char, prepared_dataset):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    input_tensor = prepare_input_text(input_text, arabert_tokenizer, arabert_model, char_to_idx, prepared_dataset)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_tensor)\n",
        "\n",
        "    # Process logits\n",
        "    # here we get the sequence of numbers that the model has generated.\n",
        "    # the numbers will be the indices of the characters in the vocabulary.\n",
        "    predicted_sequence = torch.argmax(logits, dim=-1).squeeze().tolist()\n",
        "    characters = [idx_to_char[idx] for idx in predicted_sequence]\n",
        "    return \"\".join(characters)\n",
        "\n",
        "# Example usage:\n",
        "input_text = \"الحب\"  # Example input text\n",
        "predicted_sequence = generate_text(model, input_text, arabert_tokenizer, arabert_model, char_to_idx, idx_to_char, prepared_dataset)\n",
        "print(f\"Input Text: {input_text}\")\n",
        "print(f\"Predicted Sequence: {predicted_sequence}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "W_ha0qUjPrkd",
        "outputId": "166ba76f-f78c-4995-b611-cc6560903f5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: pyarabic in /usr/local/lib/python3.11/dist-packages (0.6.15)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from pyarabic) (1.17.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully from ashaar.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "num_samples should be a positive integer value, but got num_samples=0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-61964ed6da24>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    240\u001b[0m         }\n\u001b[1;32m    241\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPoetryDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepared_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m \u001b[0mdata_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;31m# --- Model Definition ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# map-style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    165\u001b[0m                 \u001b[0;34mf\"num_samples should be a positive integer value, but got num_samples={self.num_samples}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q_1cuX74NCkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Install necessary libraries\n",
        "!pip install transformers torch pyarabic datasets pandas\n",
        "\n",
        "# Import the necessary modules\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import pyarabic.araby as araby\n",
        "import string\n",
        "import torch\n",
        "from torch.nn import Linear, Module\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.optim import Adam\n",
        "import random\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "# --- Helper Functions ---\n",
        "\n",
        "# Function to remove punctuation\n",
        "def remove_punctuation(text):\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    return text.translate(translator)\n",
        "\n",
        "# Preprocess Arabic text\n",
        "def preprocess_text(text):\n",
        "    try: # added a try block.\n",
        "      text = araby.strip_tashkeel(text)  # Remove diacritics\n",
        "      text = araby.normalize_ligature(text)  # Normalize ligatures\n",
        "      text = remove_punctuation(text)  # Remove punctuation\n",
        "      return text\n",
        "    except Exception as e:\n",
        "      print(f\"Error in preprocess_text: {e}. Skipping this verse.\")\n",
        "      return \"\" # return empty string.\n",
        "\n",
        "# --- Dataset Loading and Selection ---\n",
        "\n",
        "def load_and_select_dataset(dataset_name, num_verses=50000):\n",
        "    \"\"\"\n",
        "    Loads the dataset from Hugging Face or a CSV file, selects a random subset of verses.\n",
        "\n",
        "    Args:\n",
        "        dataset_name (str): Name of the dataset to load ('ashaar' or path to csv file).\n",
        "        num_verses (int): Number of random verses to select.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of verses or None if neither dataset is suitable.\n",
        "    \"\"\"\n",
        "    if dataset_name == 'ashaar':\n",
        "        try:\n",
        "            # Load the ashaar dataset from Hugging Face\n",
        "            dataset = load_dataset('arbml/ashaar')\n",
        "\n",
        "            # Get the list of all the verses.\n",
        "            verses = dataset['train']['bait'] #changed this line from 'poem verses' to 'bait'\n",
        "\n",
        "            print(f\"Dataset loaded successfully from {dataset_name}.\")\n",
        "\n",
        "            # Select up to num_verses\n",
        "            if len(verses) > num_verses:\n",
        "                selected_verses = random.sample(verses, num_verses)\n",
        "            else:\n",
        "                selected_verses = verses\n",
        "\n",
        "            return selected_verses\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading or processing dataset from {dataset_name}: {e}\")\n",
        "            return None\n",
        "\n",
        "    elif dataset_name.endswith('.csv'):\n",
        "        try:\n",
        "            # Load the dataset from a local CSV file using pandas\n",
        "            df = pd.read_csv(dataset_name)\n",
        "\n",
        "            # Check if the dataframe has the necessary columns\n",
        "            if 'verse' in df.columns:\n",
        "                verses = df['verse'].tolist()\n",
        "\n",
        "                print(f\"Dataset loaded successfully from {dataset_name}.\")\n",
        "\n",
        "                # Select up to num_verses\n",
        "                if len(verses) > num_verses:\n",
        "                    selected_verses = random.sample(verses, num_verses)\n",
        "                else:\n",
        "                    selected_verses = verses\n",
        "                return selected_verses\n",
        "            else:\n",
        "                print(f\"Dataset from {dataset_name} does not contain 'verse' columns.\")\n",
        "                return None\n",
        "        except FileNotFoundError:\n",
        "            print(f\"File not found: {dataset_name}. Please check the filepath.\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading or processing dataset from {dataset_name}: {e}\")\n",
        "            return None\n",
        "    else:\n",
        "        print(f\"Invalid dataset name or file path: {dataset_name}\")\n",
        "        return None\n",
        "\n",
        "def select_dataset(num_verses=50000):\n",
        "    \"\"\"\n",
        "    Selects and loads a dataset, trying 'ashaar' first and then 'pcd_dataset_path' if necessary.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of verses or None if neither dataset is suitable.\n",
        "    \"\"\"\n",
        "    selected_dataset = load_and_select_dataset('ashaar', num_verses)  # try with ashaar first\n",
        "\n",
        "    # If the first dataset was not suitable, load and select from the second dataset\n",
        "    if selected_dataset is None:\n",
        "        print(\"First dataset not suitable. Attempting to use the second dataset.\")\n",
        "\n",
        "        # Download the dataset from the URL\n",
        "        url = \"https://raw.githubusercontent.com/arbml/poetry-corpus/main/mcj6vkg6zw_cleaned_fragmented_dataset.csv\"\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            with open(\"mcj6vkg6zw_cleaned_fragmented_dataset.csv\", \"wb\") as f:\n",
        "                f.write(response.content)\n",
        "        else:\n",
        "            print(f\"Failed to download the file. Status code: {response.status_code}\")\n",
        "            return None\n",
        "\n",
        "        pcd_dataset_path = \"mcj6vkg6zw_cleaned_fragmented_dataset.csv\"\n",
        "        selected_dataset = load_and_select_dataset(pcd_dataset_path, num_verses)\n",
        "\n",
        "    if selected_dataset is None:\n",
        "        print(\"Both datasets were not suitable.\")\n",
        "\n",
        "    return selected_dataset\n",
        "\n",
        "# --- Dataset Loading and Preprocessing ---\n",
        "num_verses = 5000\n",
        "selected_dataset = select_dataset(num_verses)\n",
        "\n",
        "if selected_dataset is None:\n",
        "    print(\"Exiting due to dataset loading failure.\")\n",
        "    exit()\n",
        "\n",
        "print(f\"Initial number of verses: {len(selected_dataset)}\") # print the initial length.\n",
        "\n",
        "# --- Dataset Preparation ---\n",
        "\n",
        "def filter_incomplete_verses(dataset):\n",
        "    \"\"\"Filters out incomplete verses (empty strings).\"\"\"\n",
        "    # Check if the dataset is None before attempting to iterate over it.\n",
        "    if dataset is None:\n",
        "        print(\"Warning: dataset is None. Skipping filtering.\")\n",
        "        return []  # Return an empty list to avoid the TypeError\n",
        "\n",
        "    filtered_dataset = [verse for verse in dataset if isinstance(verse, str)] # changed this line.\n",
        "    return filtered_dataset\n",
        "\n",
        "def normalize_dataset(dataset):\n",
        "    \"\"\"Normalizes text in the dataset.\"\"\"\n",
        "    normalized_dataset = []\n",
        "    for verse in dataset:\n",
        "      if isinstance(verse, str): # added a check to see if the verse is a string\n",
        "        normalized_verse = preprocess_text(verse)\n",
        "        if normalized_verse != \"\": # only add if the verse is not an empty string.\n",
        "          normalized_dataset.append(normalized_verse)\n",
        "    return normalized_dataset\n",
        "\n",
        "filtered_dataset = filter_incomplete_verses(selected_dataset)\n",
        "normalized_dataset = normalize_dataset(filtered_dataset)\n",
        "\n",
        "print(f\"Number of verses after filtering and normalizing: {len(normalized_dataset)}\") # print the length after the filtering.\n",
        "\n",
        "if len(normalized_dataset) == 0: #if the dataset is empty.\n",
        "    print(\"Error: The dataset is empty after filtering and normalizing. Exiting.\")\n",
        "    exit()\n",
        "\n",
        "# Create a character vocabulary.\n",
        "char_vocab = sorted(list(set(\"\".join(normalized_dataset))))\n",
        "char_to_idx = {char: idx for idx, char in enumerate(char_vocab)}\n",
        "idx_to_char = {idx: char for idx, char in enumerate(char_vocab)}\n",
        "\n",
        "# --- AraBERT Setup ---\n",
        "arabert_tokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/bert-base-arabertv2\")\n",
        "arabert_model = AutoModel.from_pretrained(\"aubmindlab/bert-base-arabertv2\")\n",
        "\n",
        "# --- AraBERT helper functions ---\n",
        "def get_character_subword_mappings(text, arabert_tokenizer):\n",
        "    characters = list(text)\n",
        "    subword_tokens = []\n",
        "    subword_indices = []\n",
        "    for i, char in enumerate(characters):\n",
        "        char_tokens = arabert_tokenizer.tokenize(char)\n",
        "        subword_tokens.extend(char_tokens)\n",
        "        subword_indices.extend([i] * len(char_tokens))\n",
        "    return characters, subword_tokens, subword_indices\n",
        "\n",
        "def get_subword_embeddings(subword_tokens, arabert_tokenizer, arabert_model):\n",
        "    inputs = arabert_tokenizer(subword_tokens, is_split_into_words=True, return_tensors=\"pt\", padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = arabert_model(**inputs)\n",
        "    subword_embeddings = outputs.last_hidden_state\n",
        "    return subword_embeddings\n",
        "\n",
        "def aggregate_embeddings(subword_embeddings, subword_indices):\n",
        "    aggregated_embeddings = []\n",
        "    unique_indices = sorted(list(set(subword_indices)))\n",
        "    for unique_idx in unique_indices:\n",
        "      idx_mask = [i for i, x in enumerate(subword_indices) if x == unique_idx]\n",
        "      char_embeddings = subword_embeddings[0][idx_mask] #get all the embeddings for that character.\n",
        "      if len(char_embeddings)>0:\n",
        "          aggregated_embeddings.append(torch.mean(char_embeddings, dim=0)) #get the mean of the characters embeddings.\n",
        "    return torch.stack(aggregated_embeddings)\n",
        "\n",
        "# --- Dataset Preparation ---\n",
        "def prepare_dataset(dataset, arabert_tokenizer, arabert_model, char_to_idx):\n",
        "    data = []\n",
        "    max_len = 0\n",
        "    for verse in dataset:\n",
        "        characters, subword_tokens, subword_indices = get_character_subword_mappings(verse, arabert_tokenizer)\n",
        "        subword_embeddings = get_subword_embeddings(subword_tokens, arabert_tokenizer, arabert_model)\n",
        "        aggregated_embeddings = aggregate_embeddings(subword_embeddings, subword_indices)\n",
        "        #convert the characters into indexes.\n",
        "        verse_labels = torch.tensor([char_to_idx[char] for char in verse])\n",
        "        data.append({\"character_embeddings\": aggregated_embeddings, \"labels\":verse_labels, \"text\":verse})\n",
        "        max_len = max(max_len, len(aggregated_embeddings))\n",
        "\n",
        "    #padding\n",
        "    for item in data:\n",
        "      current_len = len(item[\"character_embeddings\"])\n",
        "      if current_len < max_len:\n",
        "        pad_length = max_len - current_len\n",
        "        padding_tensor = torch.full((pad_length, 768), 0)\n",
        "        if item[\"character_embeddings\"].ndim == 2:\n",
        "          item[\"character_embeddings\"] = torch.cat((item[\"character_embeddings\"], padding_tensor), dim=0)\n",
        "        elif item[\"character_embeddings\"].ndim == 3:\n",
        "          item[\"character_embeddings\"] = torch.cat((item[\"character_embeddings\"], padding_tensor.unsqueeze(0)), dim=1) #add batch dimension\n",
        "    return data\n",
        "\n",
        "prepared_dataset = prepare_dataset(normalized_dataset, arabert_tokenizer, arabert_model, char_to_idx)\n",
        "\n",
        "# --- Data Loader ---\n",
        "class PoetryDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        return {\n",
        "            \"character_embeddings\": item[\"character_embeddings\"],\n",
        "            \"labels\": item[\"labels\"],\n",
        "        }\n",
        "dataset = PoetryDataset(prepared_dataset)\n",
        "data_loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "# --- Model Definition ---\n",
        "class CharacterLanguageModel(Module):\n",
        "    def __init__(self, hidden_size=256, num_layers=2, output_size=len(char_vocab), embedding_size=768):\n",
        "        super(CharacterLanguageModel, self).__init__()\n",
        "        self.lstm = torch.nn.LSTM(embedding_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.linear = Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        logits = self.linear(lstm_out)\n",
        "        return logits\n",
        "\n",
        "# --- Model and Training ---\n",
        "model = CharacterLanguageModel()\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# --- Training Loop ---\n",
        "num_epochs = 30 #increased to 30\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0\n",
        "    for batch in data_loader:\n",
        "        input_embeddings = batch[\"character_embeddings\"].unsqueeze(0)\n",
        "        targets = batch[\"labels\"].unsqueeze(0)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(input_embeddings)\n",
        "\n",
        "        # Pad targets to match the logits shape\n",
        "        # Find the maximum length between targets and logits\n",
        "        max_seq_length = max(targets.size(1), logits.size(1))\n",
        "\n",
        "        # Pad targets\n",
        "        targets_padded = torch.nn.functional.pad(targets, (0, max_seq_length - targets.size(1)), 'constant', 0)\n",
        "        loss = loss_function(logits.view(-1, logits.shape[-1]), targets_padded.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss = epoch_loss + loss.item()\n",
        "\n",
        "    print(f\"loss for epoch: {epoch}, is : {epoch_loss}\")\n",
        "\n",
        "# --- Inference ---\n",
        "\n",
        "# Helper function to process text for the model.\n",
        "def process_text(text, arabert_tokenizer, arabert_model):\n",
        "    characters, subword_tokens, subword_indices = get_character_subword_mappings(text, arabert_tokenizer)\n",
        "    subword_embeddings = get_subword_embeddings(subword_tokens, arabert_tokenizer, arabert_model)\n",
        "    aggregated_embeddings = aggregate_embeddings(subword_embeddings, subword_indices)\n",
        "    return aggregated_embeddings\n",
        "\n",
        "# Function to prepare input text for the model\n",
        "def prepare_input_text(text, arabert_tokenizer, arabert_model, char_to_idx, prepared_dataset):\n",
        "    aggregated_embeddings = process_text(text, arabert_tokenizer, arabert_model)\n",
        "\n",
        "    # Padding the input\n",
        "    max_len = 0\n",
        "    for item in prepared_dataset: #use the data from the training set to find the max length\n",
        "        max_len = max(max_len, len(item[\"character_embeddings\"]))\n",
        "\n",
        "    current_len = len(aggregated_embeddings)\n",
        "    if current_len < max_len:\n",
        "      pad_length = max_len - current_len\n",
        "      padding_tensor = torch.full((pad_length, 768), 0)\n",
        "      if aggregated_embeddings.ndim == 2:\n",
        "        aggregated_embeddings = torch.cat((aggregated_embeddings, padding_tensor), dim=0)\n",
        "      elif aggregated_embeddings.ndim == 3:\n",
        "        aggregated_embeddings = torch.cat((aggregated_embeddings, padding_tensor.unsqueeze(0)), dim=1)\n",
        "\n",
        "    return aggregated_embeddings.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "# Function to generate text (or get predictions)\n",
        "def generate_text(model, input_text, arabert_tokenizer, arabert_model, char_to_idx, idx_to_char, prepared_dataset):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    input_tensor = prepare_input_text(input_text, arabert_tokenizer, arabert_model, char_to_idx, prepared_dataset)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_tensor)\n",
        "\n",
        "    # Process logits\n",
        "    # here we get the sequence of numbers that the model has generated.\n",
        "    # the numbers will be the indices of the characters in the vocabulary.\n",
        "    predicted_sequence = torch.argmax(logits, dim=-1).squeeze().tolist()\n",
        "    characters = [idx_to_char[idx] for idx in predicted_sequence]\n",
        "    return \"\".join(characters)\n",
        "\n",
        "# Example usage:\n",
        "input_text = \"الحب\"  # Example input text\n",
        "predicted_sequence = generate_text(model, input_text, arabert_tokenizer, arabert_model, char_to_idx, idx_to_char, prepared_dataset)\n",
        "print(f\"Input Text: {input_text}\")\n",
        "print(f\"Predicted Sequence: {predicted_sequence}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "meTjvv5_RXJU",
        "outputId": "4a3977f4-d63f-4f7f-a4c1-4094e1b16e99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: pyarabic in /usr/local/lib/python3.11/dist-packages (0.6.15)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from pyarabic) (1.17.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Error loading or processing dataset from ashaar: \"Column bait not in the dataset. Current columns in the dataset: ['poem title', 'poem meter', 'poem verses', 'poem theme', 'poem url', 'poet name', 'poet description', 'poet url', 'poet era', 'poet location', 'poem description', 'poem language type']\"\n",
            "First dataset not suitable. Attempting to use the second dataset.\n",
            "Failed to download the file. Status code: 404\n",
            "Exiting due to dataset loading failure.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "object of type 'NoneType' has no len()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-39c42a1e7323>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Initial number of verses: {len(selected_dataset)}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# print the initial length.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;31m# --- Dataset Preparation ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NLJuu8U4RfgM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}